# FinLoRA: Finetuning Qauntized Financial Large Language Models using Low-Rank Adaptation


### Motivation
Large Language Models (LLMs) have shown remarkable performance, but pre-training and fine-tuning LLMs can be computationally expensive. The low-rank and quantization techniques show success. Our project aims to provide a thorough evaluation of these techniques with a focus on financial applications.

### Key Methodology
The paper FinGPT-HPC [1] explores low-rank structure and quantization techniques in pretraining and finetuning LLMs and evaluates its performance on both general tasks and financial tasks, and it shows significant speedup and lower GPU memory consumption. This ICDCS conference paper will be extended into a journal paper by employing more comprehensive testing.


### Expected Outcomes 
This project would produce comprehensive understanding in using multiple novel low-rank [5] and quantization structures in both pretraining and finetuning stages, and its performance for both general tasks and financial tasks.
The first milestone is an extended evaluation using newer LLMs, low-rank techniques, and datasets.
The second milestone involves extending this conference paper [1] into a longer journal paper by inserting new methodologies and results.

### References
[1] Xiao-Yang Liu, Jie Zhang, Guoxuan Wang, Weiqing Tong, Anwar Walid. FinGPT-HPC: Efficient Pretraining and Finetuning Large Language Models for Financial Applications with High-Performance Computing. IEEE ICDCS 2024. 

[2] Mao, Y., Ge, Y., Fan, Y., Xu, W., Mi, Y., Hu, Z. and Gao, Y., 2024. A Survey on LoRA of Large Language Models. arXiv preprint arXiv:2407.11046.

[3] Vlad Fomenko, Han Yu, Jongho Lee, Stanley Hsieh, Weizhu Chen. A Note on LoRA, 2024. https://arxiv.org/abs/2404.05086 
  

