

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>LoRA Foundations and Methods &mdash; FinLoRA Documentation 1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=ca35e0e8" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Fine-Tuning Tutorials" href="../tutorials/tutorials.html" />
    <link rel="prev" title="XBRL Data Analysis" href="../tasks/xbrl_analysis_tasks.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            FinLoRA Documentation
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intro/overview.html">Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tasks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tasks/general_financial_tasks.html">General Financial Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tasks/xbrl_reporting_tasks.html">XBRL Data Reporting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tasks/xbrl_analysis_tasks.html">XBRL Data Analysis</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">LoRA Methods</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">LoRA Foundations and Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#what-is-lora">What is LoRA?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#foundations-of-lora">Foundations of LoRA</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#ranks">Ranks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#full-fine-tuning">Full Fine-Tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fine-tuning-with-adapters-parameter-efficient-fine-tuningpeft">Fine-Tuning With Adapters (Parameter-Efficient Fine-Tuning—PEFT)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#lora-methods">LoRA Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#low-rank-adaptation-lora">Low-Rank Adaptation (LoRA)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#quantized-lora-qlora">Quantized LoRA (QLoRA)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#weight-decomposed-low-rank-adaptation-dora">Weight-Decomposed Low-Rank Adaptation (DoRA)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#rank-stabilized-lora-rslora">Rank-Stabilized LoRA (rsLoRA)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#lora-with-federated-learning">LoRA with Federated Learning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/tutorials.html">Fine-Tuning Tutorials</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Benchmark Results</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../benchmark_results/results.html">Benchmark Results</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">FinLoRA Documentation</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">LoRA Foundations and Methods</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/Open-Finance-Lab/FinLoRA/blob/main/docs/source/lora_methods/lora_methods.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="lora-foundations-and-methods">
<h1>LoRA Foundations and Methods<a class="headerlink" href="#lora-foundations-and-methods" title="Link to this heading"></a></h1>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#what-is-lora" id="id11">What is LoRA?</a></p></li>
<li><p><a class="reference internal" href="#foundations-of-lora" id="id12">Foundations of LoRA</a></p>
<ul>
<li><p><a class="reference internal" href="#ranks" id="id13">Ranks</a></p></li>
<li><p><a class="reference internal" href="#full-fine-tuning" id="id14">Full Fine-Tuning</a></p></li>
<li><p><a class="reference internal" href="#fine-tuning-with-adapters-parameter-efficient-fine-tuningpeft" id="id15">Fine-Tuning With Adapters (Parameter-Efficient Fine-Tuning—PEFT)</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#lora-methods" id="id16">LoRA Methods</a></p>
<ul>
<li><p><a class="reference internal" href="#low-rank-adaptation-lora" id="id17">Low-Rank Adaptation (LoRA)</a></p></li>
<li><p><a class="reference internal" href="#quantized-lora-qlora" id="id18">Quantized LoRA (QLoRA)</a></p></li>
<li><p><a class="reference internal" href="#weight-decomposed-low-rank-adaptation-dora" id="id19">Weight-Decomposed Low-Rank Adaptation (DoRA)</a></p></li>
<li><p><a class="reference internal" href="#rank-stabilized-lora-rslora" id="id20">Rank-Stabilized LoRA (rsLoRA)</a></p></li>
<li><p><a class="reference internal" href="#lora-with-federated-learning" id="id21">LoRA with Federated Learning</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#references" id="id22">References</a></p></li>
</ul>
</nav>
<section id="what-is-lora">
<h2><a class="toc-backref" href="#id11" role="doc-backlink">What is LoRA?</a><a class="headerlink" href="#what-is-lora" title="Link to this heading"></a></h2>
<p>LoRA is a technique to efficiently update the parameters of pre-trained language models when fine-tuning on new tasks.</p>
</section>
<section id="foundations-of-lora">
<h2><a class="toc-backref" href="#id12" role="doc-backlink">Foundations of LoRA</a><a class="headerlink" href="#foundations-of-lora" title="Link to this heading"></a></h2>
<p>In this subsection, we introduce two fundamental concepts needed to understand LoRA—ranks and fine-tuning.</p>
<section id="ranks">
<h3><a class="toc-backref" href="#id13" role="doc-backlink">Ranks</a><a class="headerlink" href="#ranks" title="Link to this heading"></a></h3>
<p>Rank is the number of linearly independent rows or columns in a matrix.
Linearly independent columns, for example, are columns whose entries cannot be written as an integer-weighted sum of earlier columns.</p>
<div class="math notranslate nohighlight">
\[\begin{split}W =
\begin{bmatrix}
 1 &amp; 7 &amp; 2 &amp; 8 &amp; 5\\
 2 &amp; 10 &amp; 4 &amp; 12 &amp; 10\\
 3 &amp; 15 &amp; 12 &amp; 18 &amp; 27\\
 4 &amp; 12 &amp; 16 &amp; 16 &amp; 36
\end{bmatrix},
\qquad
\text{Dimensions: }4 \times 5\;(\text{rows}\times\text{columns})\end{split}\]</div>
<p>In this matrix there are <strong>two</strong> linearly independent columns, so
<span class="math notranslate nohighlight">\(\operatorname{rank}(W)=2\)</span>.</p>
<ul class="simple">
<li><p>Column 1 is independent (nothing precedes it).</p></li>
<li><p>Column 2 cannot be written as a multiple of Column 1, so it is also independent.</p></li>
<li><p>Columns 3–5 are dependent:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[C_3 = 2C_1,\qquad
C_4 = C_1 + C_2,\qquad
C_5 = C_1 + 2C_2.\]</div>
<p>Re-expressing those dependencies in vector form:</p>
<div class="math notranslate nohighlight">
\[\begin{split}W \;=\;
\underbrace{\begin{bmatrix}
 1 &amp; 7\\
 2 &amp; 10\\
 3 &amp; 15\\
 4 &amp; 12
\end{bmatrix}}_{B\in\mathbb{R}^{4\times2}}
\;
\underbrace{\begin{bmatrix}
 1 &amp; 0 &amp; 2 &amp; 1 &amp; 1\\
 0 &amp; 1 &amp; 0 &amp; 1 &amp; 2
\end{bmatrix}}_{A\in\mathbb{R}^{2\times5}}.\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\text{Dimensions}(W)     &amp;= d\times k = 4\times5,\\
\text{Dimensions}(B)     &amp;= d\times r = 4\times2,\\
\text{Dimensions}(A)     &amp;= r\times k = 2\times5,\\
\text{Dimensions}(BA)    &amp;= d\times k = \text{Dimensions}(W).
\end{aligned}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\text{Parameters}(W) &amp;= 4\times5 = 20,\\
\text{Parameters}(B) &amp;= 4\times2 = 8,\\
\text{Parameters}(A) &amp;= 2\times5 = 10,\\
\text{Parameters}(BA)  &amp;= 8 + 10 = 18.
\end{aligned}\end{split}\]</div>
<p>Thus storing <span class="math notranslate nohighlight">\(B\)</span> and <span class="math notranslate nohighlight">\(A\)</span> uses fewer parameters than storing <span class="math notranslate nohighlight">\(W\)</span> directly—a key idea behind <em>low-rank</em> adaptation (LoRA).</p>
</section>
<section id="full-fine-tuning">
<h3><a class="toc-backref" href="#id14" role="doc-backlink">Full Fine-Tuning</a><a class="headerlink" href="#full-fine-tuning" title="Link to this heading"></a></h3>
<p>Consider a pre-trained model M with 500 million parameters. Suppose we pre-trained M with two tasks. Task 1 is Masked Language Modeling (MLM), where we mask some words in a sentence, and the task is to predict the sentence with the masked tokens filled in. Task 2 is Next Sentence Prediction (NSP), where the task is to predict if, given 2 sentences, whether or not sentence A comes before sentence B.</p>
<p>If we want to fine-tune the pre-trained model M on a new task, Named Entity Recognition (NER), where the task is to annotate one entity (location/person/organization) per sentence in a financial task.</p>
<p>When we perform full fine-tuning on model M, all parameters are updated based on the gradients we compute during backpropagation. In backpropagation, we compute the loss (the difference between the predicted output and the target output) and propagate the loss backward through the model. As we propagate the loss backward, we compute the gradient of the loss with respect to each parameter. The optimizer uses these gradients to update the model’s parameters.</p>
<p>If we want to fine-tune model M on another task Financial Phrase Bank (FPB), where the task is to annotate sentences from financial news and reports with sentiment, we still need to update all 500 million parameters. This is costly and can lead to overfitting and the model forgetting pre-training tasks.</p>
</section>
<section id="fine-tuning-with-adapters-parameter-efficient-fine-tuningpeft">
<h3><a class="toc-backref" href="#id15" role="doc-backlink">Fine-Tuning With Adapters (Parameter-Efficient Fine-Tuning—PEFT)</a><a class="headerlink" href="#fine-tuning-with-adapters-parameter-efficient-fine-tuningpeft" title="Link to this heading"></a></h3>
<p>Parameter Efficient Fine-Tuning (PEFT) adds small adapter layers per transformer block as shown below. Let’s consider a scenario in which we use PEFT to fine-tune the pre-trained model M and add two adapter layers per transformer layer.</p>
<p>Now, when we fine-tune M on NER, only the adapter parameters are updated. This only consists of a tiny fraction of the original parameters. The rest of the model’s parameters are frozen. This means that, during backpropagation, the gradients of loss pass through them, but the parameters aren’t updated. While we do have to swap the adapters and store the updated parameters separately for FPB, the number of parameters required to fine-tune on FPB is much smaller than full fine-tuning.</p>
</section>
</section>
<section id="lora-methods">
<h2><a class="toc-backref" href="#id16" role="doc-backlink">LoRA Methods</a><a class="headerlink" href="#lora-methods" title="Link to this heading"></a></h2>
<p>In this subsection, we introduce the five LoRA methods we use in our paper. We choose LoRA <a class="footnote-reference brackets" href="#id6" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> and QLoRA <a class="footnote-reference brackets" href="#id7" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a> due to their standard use in fine-tuning. We choose DoRA <a class="footnote-reference brackets" href="#id8" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a> and rsLoRA <a class="footnote-reference brackets" href="#id9" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a> due to their performance enhancements: DoRA proposes fine-grained updates for achieving accuracy through LoRA, and rsLoRA proposes a scaling factor to achieve gradient stability. Lastly, we choose LoRA with federated learning for its practical ability to allow financial institutions to collaborate in fine-tuning models while using private, confidential data.</p>
<section id="low-rank-adaptation-lora">
<h3><a class="toc-backref" href="#id17" role="doc-backlink">Low-Rank Adaptation (LoRA)</a><a class="headerlink" href="#low-rank-adaptation-lora" title="Link to this heading"></a></h3>
<p>LoRA adds a scaled low-rank update <span class="math notranslate nohighlight">\(\Delta \boldsymbol{W} = \gamma_r\boldsymbol{B}\boldsymbol{A}\)</span>—where <span class="math notranslate nohighlight">\(\gamma_r\)</span> is a scaling factor (<span class="math notranslate nohighlight">\(\gamma_r=\frac{\alpha}{r}\)</span> with <span class="math notranslate nohighlight">\(\alpha\)</span> &gt; 0 and rank <span class="math notranslate nohighlight">\(r\)</span> &gt; 0), <span class="math notranslate nohighlight">\(\boldsymbol{B} \in \mathbb{R}^{d \times r}\)</span>, and <span class="math notranslate nohighlight">\(\boldsymbol{A} \in \mathbb{R}^{r \times k}\)</span>—to the frozen pre-trained weight matrix <span class="math notranslate nohighlight">\(\boldsymbol{W}_0 \in \mathbb{R}^{d \times k}\)</span>.</p>
<p>For each multi-head attention layer, we have query, key, and value weight matrices, which we can factorize as follows:</p>
<div class="math notranslate nohighlight">
\[W_Q^{(n)} = B_Q^{(n)}A_Q^{(n)},\quad
W_K^{(n)} = B_K^{(n)}A_K^{(n)},\quad
W_V^{(n)} = B_V^{(n)}A_V^{(n)}.\]</div>
<p>During fine-tuning, the weight matrices are updated as follows with the scaled low-rank update:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
W_{Q,\text{new}}^{(n)} &amp;= W_{Q,\text{old}}^{(n)} + \gamma_rB_Q^{(n)}A_Q^{(n)},\\
W_{K,\text{new}}^{(n)} &amp;= W_{K,\text{old}}^{(n)} + \gamma_rB_K^{(n)}A_K^{(n)},\\
W_{V,\text{new}}^{(n)} &amp;= W_{V,\text{old}}^{(n)} + \gamma_rB_V^{(n)}A_V^{(n)}.
\end{aligned}\end{split}\]</div>
<p>Because the update is in-place, no extra layers are added, and inference latency is unchanged.</p>
</section>
<section id="quantized-lora-qlora">
<h3><a class="toc-backref" href="#id18" role="doc-backlink">Quantized LoRA (QLoRA)</a><a class="headerlink" href="#quantized-lora-qlora" title="Link to this heading"></a></h3>
<p>When fine-tuning, LoRA requires a large amount of GPU memory. To solve this issue, we can use QLoRA.
QLoRA drastically reduces memory usage and lets you fine-tune on a single GPU.</p>
<p>In QLoRA, we quantize the weights of the adapter layers, reducing both parameter count and memory usage. Quantization is a technique that reduces the precision of the weights to reduce the number of bits used to store them. It consists of two parts: Rounding to the nearest integer and truncating to remove the decimal portion of a floating point number. QLoRA specifically uses 4-bit NormalFloat (NF4), an optimal data type for normally distributed weights, quantization. Pre-trained weights are usually normally distributed and centered around 0, which is why NF4 is ideal for quantization.</p>
<p>If we quantize from Float16 to Int4, we can represent 16 different values (bins) because Int4 has 4 bits and <span class="math notranslate nohighlight">\(2^{4}=16\)</span>. Inputs are usually normalized from -1 to 1. Very close together values, however, will be mapped to the same bin. This means that the precision is lost if we want to convert back to Float16. However, we can use blockwise quantization, where we divide the input range into blocks and quantize each block separately. QLoRA uses a 64 blocksize for better precision.</p>
<p>Since regular quantization relies on the bins being equally probable, QLoRA uses NormalFloat where the bins are weighted by the normal distribution. The spacing between bins is therefore closer together near 0 and further apart further away from 0.</p>
<p>Each block in QLoRA has a quantization constant. QLoRA employs double quantization, where it quantizes the quantization constants themselves to further save space.</p>
<p>The last part of QLoRA is paged optimizers. Paged optimizers reduce GPU memory spikes by switching pages to CPU memory when GPU RAM becomes full when processing long sequences, and the pages are not needed for the current computation of the forward/backward pass.</p>
<p>The forward pass for QLoRA is <span class="math notranslate nohighlight">\(\boldsymbol{y} = p_{16}(\boldsymbol{W}_0^{\text{NF4}}) \boldsymbol{x} + \gamma_r \boldsymbol{B} \boldsymbol{A} \boldsymbol{x}\)</span>.</p>
</section>
<section id="weight-decomposed-low-rank-adaptation-dora">
<h3><a class="toc-backref" href="#id19" role="doc-backlink">Weight-Decomposed Low-Rank Adaptation (DoRA)</a><a class="headerlink" href="#weight-decomposed-low-rank-adaptation-dora" title="Link to this heading"></a></h3>
<p>LoRA makes simple changes to the model weights, so it sometimes doesn’t capture the full complexity of the data and its relationships. DoRA solves this issue of capturing data complexity.</p>
<p>DoRA decomposes the weight matrix into a <em>magnitude vector</em> and a <em>direction matrix</em>.
The magnitude vector consists of the lengths of the columns in the weight matrix and is computed by taking each column’s <span class="math notranslate nohighlight">\(\ell_2\)</span> norm.
The direction matrix <span class="math notranslate nohighlight">\(\boldsymbol V\)</span> is the collection of the original columns. Its unit-column form <span class="math notranslate nohighlight">\(\widehat{\boldsymbol V}=\boldsymbol V/\lVert\boldsymbol V\rVert_c\)</span> is obtained by dividing each column by its <span class="math notranslate nohighlight">\(\ell_2\)</span> norm.</p>
<p>The magnitude vector <span class="math notranslate nohighlight">\(\boldsymbol{m}\)</span> is of size <span class="math notranslate nohighlight">\(1 \times k\)</span>, where <span class="math notranslate nohighlight">\(k\)</span> is the number of columns. The direction matrix <span class="math notranslate nohighlight">\(\boldsymbol{V}\)</span> is of size <span class="math notranslate nohighlight">\(d \times k\)</span>, where <span class="math notranslate nohighlight">\(d\)</span> is the number of rows in a weight matrix.</p>
<p>The decomposition can be written as:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{W}_0 \;=\; \boldsymbol{m}\,\frac{\boldsymbol{V}}{\lVert \boldsymbol{V} \rVert_c}\;=\;\lVert \boldsymbol{W}_0 \rVert_c\,\frac{\boldsymbol{W}_0}{\lVert \boldsymbol{W}_0 \rVert_c},\]</div>
<p>where <span class="math notranslate nohighlight">\(\lVert \cdot \rVert_c\)</span> denotes the column-wise <span class="math notranslate nohighlight">\(\ell_2\)</span> norm (i.e., the norm is taken independently for each column) and <span class="math notranslate nohighlight">\(\boldsymbol{W}_0\)</span> is the frozen pretrained weight.</p>
<p>Here is an example of the decomposition:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\boldsymbol{W}_0 =
\begin{bmatrix}
1 &amp; 7 &amp; 2 &amp; 8 &amp; 5 \\
2 &amp; 10 &amp; 4 &amp; 12 &amp; 10 \\
3 &amp; 15 &amp; 12 &amp; 18 &amp; 27 \\
4 &amp; 12 &amp; 16 &amp; 16 &amp; 36
\end{bmatrix}, \qquad
\boldsymbol{W}_0 \in \mathbb{R}^{4 \times 5}.\end{split}\]</div>
<p>For column <span class="math notranslate nohighlight">\(j\)</span></p>
<div class="math notranslate nohighlight">
\[\lVert \boldsymbol{w}_j \rVert_2 = \sqrt{\sum_{i=1}^{4} W_{ij}^2}.\]</div>
<p>These norms form a <span class="math notranslate nohighlight">\(1 \times 5\)</span> magnitude vector:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{m} = \left[ 5.4772,\; 22.7596,\; 20.4939,\; 28.0713,\; 46.3681 \right].\]</div>
<p>The unit-column direction matrix is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\widehat{\boldsymbol{V}} =
\begin{bmatrix}
0.182574 &amp; 0.307562 &amp; 0.097590 &amp; 0.284988 &amp; 0.107833 \\
0.365148 &amp; 0.439375 &amp; 0.195180 &amp; 0.427482 &amp; 0.215666 \\
0.547723 &amp; 0.659062 &amp; 0.585540 &amp; 0.641223 &amp; 0.582297 \\
0.730297 &amp; 0.527250 &amp; 0.780720 &amp; 0.569976 &amp; 0.776396
\end{bmatrix}.\end{split}\]</div>
<p>Every column of <span class="math notranslate nohighlight">\(\widehat{\boldsymbol{V}}\)</span> now has unit length:</p>
<div class="math notranslate nohighlight">
\[\lVert \boldsymbol{v}_j \rVert_2 = 1, \qquad \text{for all } j.\]</div>
<p>These are updated separately. The magnitude vector <span class="math notranslate nohighlight">\(\boldsymbol{m}\)</span> is trained directly, while the direction matrix <span class="math notranslate nohighlight">\(\boldsymbol{V}\)</span> is fine-tuned using LoRA: <span class="math notranslate nohighlight">\(\Delta\boldsymbol{V} = \boldsymbol{B}\boldsymbol{A}\)</span> with <span class="math notranslate nohighlight">\(\boldsymbol{B}\!\in\!\mathbb{R}^{d\times r}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{A}\!\in\!\mathbb{R}^{r\times k}\)</span>.</p>
<p>After the updates, the new weight matrix is</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{W}' = \boldsymbol{m}\,\frac{\boldsymbol{V} + \Delta \boldsymbol{V}}{\lVert \boldsymbol{V} + \Delta \boldsymbol{V} \rVert_c}
     = \boldsymbol{m}\,\frac{\boldsymbol{W}_0 + \boldsymbol{B}\boldsymbol{A}}{\lVert \boldsymbol{W}_0 + \boldsymbol{B}\boldsymbol{A} \rVert_c}.\]</div>
</section>
<section id="rank-stabilized-lora-rslora">
<h3><a class="toc-backref" href="#id20" role="doc-backlink">Rank-Stabilized LoRA (rsLoRA)</a><a class="headerlink" href="#rank-stabilized-lora-rslora" title="Link to this heading"></a></h3>
<p>LoRA scales the weight matrix update <span class="math notranslate nohighlight">\(\boldsymbol{BA}\)</span> by <span class="math notranslate nohighlight">\(\frac{\alpha}{r}\)</span>, which can cause gradients to explode or diminish as the rank <span class="math notranslate nohighlight">\(r\)</span> increases. In contrast, rsLoRA uses a scaling factor <span class="math notranslate nohighlight">\(\frac{\alpha}{\sqrt{r}}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol W'=\boldsymbol W_0+\frac{\alpha}{\sqrt{r}}\boldsymbol B\boldsymbol A.\]</div>
<p>This scaling results in gradient-scale stability at higher ranks, enabling the rank to be higher to capture more details in long-context tasks like XBRL extraction. rsLoRA also results in lower perplexity—the model assigns higher probabilities to correct words—than LoRA at higher ranks.</p>
</section>
<section id="lora-with-federated-learning">
<h3><a class="toc-backref" href="#id21" role="doc-backlink">LoRA with Federated Learning</a><a class="headerlink" href="#lora-with-federated-learning" title="Link to this heading"></a></h3>
<p>In the finance sector, multiple banks may want to work together on a model to predict credit risk and whether a borrower will default on a loan. Each bank may have a different dataset, but they cannot share their data due to compliance reasons and privacy concerns. Federated learning solves this issue by fine-tuning a model on local data and aggregating updates during backpropagation to a centralized model via a server.</p>
<p>Differentially Private Low-Rank Adaptation (DP-LoRA) <a class="footnote-reference brackets" href="#id10" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a> is a method to use federated learning with LoRA.</p>
<p>DP-LoRA first uses a server to send the current global LoRA weights (the A and B matrices from earlier) to all clients.</p>
<p>Every client does the following: 1) Gets a minibatch of its private data 2) Computes the gradient for only its local A and B weights clipped with an <span class="math notranslate nohighlight">\(\ell_2\)</span> norm (square root of the sum of the squares of elements in the vector) 3) Adds Gaussian noise to the gradients 4) Updates the A and B LoRA matrices 5) Sends the updated A and B matrices to the server.</p>
<p>By adding noise, DP-LoRA prevents the centralized model from inferring the private data later on. This would allow the banks in the credit risk example to work on a model together.</p>
<p>As in normal federated learning, the server then aggregates the weights from all clients in a weighted average and sends the updated weights to all clients.</p>
</section>
</section>
<section id="references">
<h2><a class="toc-backref" href="#id22" role="doc-backlink">References</a><a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id6" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., … &amp; Chen, W. (2022). Lora: Low-rank adaptation of large language models. ICLR, 1(2), 3.</p>
</aside>
<aside class="footnote brackets" id="id7" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>Dettmers, T., Pagnoni, A., Holtzman, A., &amp; Zettlemoyer, L. (2023). Qlora: Efficient finetuning of quantized llms. Advances in neural information processing systems, 36, 10088-10115.</p>
</aside>
<aside class="footnote brackets" id="id8" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">3</a><span class="fn-bracket">]</span></span>
<p>Liu, S. Y., Wang, C. Y., Yin, H., Molchanov, P., Wang, Y. C. F., Cheng, K. T., &amp; Chen, M. H. (2024, July). Dora: Weight-decomposed low-rank adaptation. In Forty-first International Conference on Machine Learning.</p>
</aside>
<aside class="footnote brackets" id="id9" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">4</a><span class="fn-bracket">]</span></span>
<p>Kalajdzievski, D. (2023). Rank-stabilized scaling factor for LoRA adaptation.</p>
</aside>
<aside class="footnote brackets" id="id10" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">5</a><span class="fn-bracket">]</span></span>
<p>Liu, X. Y., Zhu, R., Zha, D., Gao, J., Zhong, S., White, M., &amp; Qiu, M. (2025). Differentially private low-rank adaptation of large language model using federated learning. ACM Transactions on Management Information Systems, 16(2), 1-24.</p>
</aside>
</aside>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../tasks/xbrl_analysis_tasks.html" class="btn btn-neutral float-left" title="XBRL Data Analysis" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../tutorials/tutorials.html" class="btn btn-neutral float-right" title="Fine-Tuning Tutorials" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, FinLoRA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>