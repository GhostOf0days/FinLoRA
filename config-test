
-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 2:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

world size:                                                             4       
data parallel size:                                                     4       
model parallel size:                                                    1       
batch size per GPU:                                                     8       
params per GPU:                                                         8.03 B  
params of model = params per GPU * mp_size:                             8.03 B  
fwd MACs per GPU:                                                       5.61 TMACs
fwd flops per GPU:                                                      11.21 T 
fwd flops of model = fwd flops per GPU * mp_size:                       11.21 T 
fwd latency:                                                            185.11 ms
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    60.57 TFLOPS
bwd latency:                                                            299.69 ms
bwd FLOPS per GPU = 2 * fwd flops per GPU / bwd latency:                74.82 TFLOPS
fwd+bwd FLOPS per GPU = 3 * fwd flops per GPU / (fwd+bwd latency):      69.38 TFLOPS
step latency:                                                           6.07 ms 
iter latency:                                                           490.87 ms
FLOPS per GPU = 3 * fwd flops per GPU / iter latency:                   68.52 TFLOPS
samples/second:                                                         65.19   

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'PeftModelForCausalLM': '8.03 B'}
    MACs        - {'PeftModelForCausalLM': '5.61 TMACs'}
    fwd latency - {'PeftModelForCausalLM': '180.82 ms'}
depth 1:
    params      - {'LoraModel': '8.03 B'}
    MACs        - {'LoraModel': '5.61 TMACs'}
    fwd latency - {'LoraModel': '180.82 ms'}
depth 2:
    params      - {'LlamaForCausalLM': '8.03 B'}
    MACs        - {'LlamaForCausalLM': '5.61 TMACs'}
    fwd latency - {'LlamaForCausalLM': '180.82 ms'}
depth 3:
    params      - {'LlamaModel': '7.51 B'}
    MACs        - {'LlamaModel': '5.22 TMACs'}
    fwd latency - {'LlamaModel': '169.42 ms'}
depth 4:
    params      - {'ModuleList': '6.98 B'}
    MACs        - {'ModuleList': '5.22 TMACs'}
    fwd latency - {'ModuleList': '161.21 ms'}
depth 5:
    params      - {'LlamaDecoderLayer': '6.98 B'}
    MACs        - {'LlamaDecoderLayer': '5.22 TMACs'}
    fwd latency - {'LlamaDecoderLayer': '161.21 ms'}
depth 6:
    params      - {'LlamaMLP': '5.64 B'}
    MACs        - {'LlamaMLP': '4.1 TMACs'}
    fwd latency - {'LlamaMLP': '77.92 ms'}
depth 7:
    params      - {'Linear': '6.98 B'}
    MACs        - {'Linear': '5.08 TMACs'}
    fwd latency - {'Linear': '121.24 ms'}
depth 8:
    params      - {'Linear': '805.31 M'}
    MACs        - {'Linear': '586.26 GMACs'}
    fwd latency - {'Linear': '15.36 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

PeftModelForCausalLM(
  8.03 B = 100% Params, 5.61 TMACs = 100% MACs, 180.82 ms = 100% latency, 62.01 TFLOPS
  (base_model): LoraModel(
    8.03 B = 100% Params, 5.61 TMACs = 100% MACs, 180.82 ms = 100% latency, 62.01 TFLOPS
    (model): LlamaForCausalLM(
      8.03 B = 100% Params, 5.61 TMACs = 100% MACs, 180.82 ms = 100% latency, 62.01 TFLOPS
      (model): LlamaModel(
        7.51 B = 93.46% Params, 5.22 TMACs = 93.18% MACs, 169.42 ms = 93.7% latency, 61.66 TFLOPS
        (embed_tokens): Embedding(525.34 M = 6.54% Params, 0 MACs = 0% MACs, 173.57 us = 0.1% latency, 0 FLOPS, 128257, 4096)
        (layers): ModuleList(
          (0): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 5.37 ms = 2.97% latency, 60.84 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.3 ms = 1.27% latency, 30.45 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 740.53 us = 0.41% latency, 33.12 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 351.91 us = 0.19% latency, 69.42 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 75.1 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 75.1 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 85.35 us = 0.05% latency, 558.97 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 85.35 us = 0.05% latency, 558.97 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 65.57 us = 0.04% latency, 727.68 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 65.57 us = 0.04% latency, 727.68 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 418.66 us = 0.23% latency, 14.73 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 113.96 us = 0.06% latency, 53.59 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 53.41 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 53.41 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.41 us = 0.03% latency, 816.78 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.41 us = 0.03% latency, 816.78 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 67.23 us = 0.04% latency, 177.4 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 67.23 us = 0.04% latency, 177.4 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 373.36 us = 0.21% latency, 16.52 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 97.75 us = 0.05% latency, 62.47 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.98 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.98 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.03 us = 0.03% latency, 851.54 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.03 us = 0.03% latency, 851.54 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.64 us = 0.03% latency, 245.23 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.64 us = 0.03% latency, 245.23 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 275.37 us = 0.15% latency, 88.71 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.47 ms = 1.37% latency, 103.86 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 736.71 us = 0.41% latency, 116.05 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 717.4 us = 0.4% latency, 119.18 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 734.09 us = 0.41% latency, 116.47 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 98.94 us = 0.05% latency, 105.48 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 235.32 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 198.6 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (1): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 5.01 ms = 2.77% latency, 65.16 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.05 ms = 1.13% latency, 34.18 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 574.35 us = 0.32% latency, 42.7 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 272.27 us = 0.15% latency, 89.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.45 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.45 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.65 us = 0.03% latency, 813.46 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.65 us = 0.03% latency, 813.46 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 50.31 us = 0.03% latency, 948.39 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 50.31 us = 0.03% latency, 948.39 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 376.94 us = 0.21% latency, 16.36 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 98.71 us = 0.05% latency, 61.87 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.69 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.69 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.03 us = 0.03% latency, 851.54 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.03 us = 0.03% latency, 851.54 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 50.07 us = 0.03% latency, 238.23 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 50.07 us = 0.03% latency, 238.23 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 369.55 us = 0.2% latency, 16.69 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 95.61 us = 0.05% latency, 63.88 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.5 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.5 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.31 us = 0.03% latency, 862.55 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.31 us = 0.03% latency, 862.55 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.64 us = 0.03% latency, 245.23 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.64 us = 0.03% latency, 245.23 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 271.32 us = 0.15% latency, 90.03 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.42 ms = 1.34% latency, 106.15 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 717.64 us = 0.4% latency, 119.14 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 710.25 us = 0.39% latency, 120.38 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 727.89 us = 0.4% latency, 117.46 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 81.54 us = 0.05% latency, 128 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 201.7 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 196.7 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (2): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 4.98 ms = 2.76% latency, 65.52 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.02 ms = 1.12% latency, 34.56 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 577.45 us = 0.32% latency, 42.47 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 272.99 us = 0.15% latency, 89.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 53.17 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 53.17 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 59.13 us = 0.03% latency, 806.9 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 59.13 us = 0.03% latency, 806.9 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 51.02 us = 0.03% latency, 935.1 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 51.02 us = 0.03% latency, 935.1 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 374.56 us = 0.21% latency, 16.46 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 98.47 us = 0.05% latency, 62.02 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.74 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.74 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.27 us = 0.03% latency, 847.93 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.27 us = 0.03% latency, 847.93 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 50.07 us = 0.03% latency, 238.23 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 50.07 us = 0.03% latency, 238.23 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 369.55 us = 0.2% latency, 16.69 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 95.84 us = 0.05% latency, 63.72 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 50.78 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 50.78 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.55 us = 0.03% latency, 858.85 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.55 us = 0.03% latency, 858.85 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.83 us = 0.03% latency, 239.37 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.83 us = 0.03% latency, 239.37 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 270.84 us = 0.15% latency, 90.19 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.41 ms = 1.33% latency, 106.34 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 716.92 us = 0.4% latency, 119.25 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 710.49 us = 0.39% latency, 120.34 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 726.7 us = 0.4% latency, 117.65 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 81.54 us = 0.05% latency, 128 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 200.75 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 197.17 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (3): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 4.99 ms = 2.76% latency, 65.48 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.03 ms = 1.12% latency, 34.46 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 575.07 us = 0.32% latency, 42.64 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 271.56 us = 0.15% latency, 89.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.69 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.69 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 59.37 us = 0.03% latency, 803.66 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 59.37 us = 0.03% latency, 803.66 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 51.02 us = 0.03% latency, 935.1 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 51.02 us = 0.03% latency, 935.1 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 378.37 us = 0.21% latency, 16.3 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 98.47 us = 0.05% latency, 62.02 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.93 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.93 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.51 us = 0.03% latency, 844.35 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.51 us = 0.03% latency, 844.35 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 50.31 us = 0.03% latency, 237.1 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 50.31 us = 0.03% latency, 237.1 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 370.74 us = 0.21% latency, 16.63 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 96.32 us = 0.05% latency, 63.4 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.26 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.26 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.27 us = 0.03% latency, 847.93 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.27 us = 0.03% latency, 847.93 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.35 us = 0.03% latency, 241.68 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.35 us = 0.03% latency, 241.68 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 270.84 us = 0.15% latency, 90.19 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.41 ms = 1.33% latency, 106.31 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 716.69 us = 0.4% latency, 119.29 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 713.59 us = 0.39% latency, 119.81 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 722.89 us = 0.4% latency, 118.27 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 81.54 us = 0.05% latency, 128 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 200.27 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 196.22 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (4): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 4.97 ms = 2.75% latency, 65.74 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.01 ms = 1.11% latency, 34.76 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 572.44 us = 0.32% latency, 42.84 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 270.61 us = 0.15% latency, 90.27 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 53.17 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 53.17 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.17 us = 0.03% latency, 820.13 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.17 us = 0.03% latency, 820.13 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 51.02 us = 0.03% latency, 935.1 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 51.02 us = 0.03% latency, 935.1 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 370.74 us = 0.21% latency, 16.63 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 97.75 us = 0.05% latency, 62.47 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.21 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.21 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.07 us = 0.03% latency, 866.28 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.07 us = 0.03% latency, 866.28 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.64 us = 0.03% latency, 245.23 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.64 us = 0.03% latency, 245.23 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 365.26 us = 0.2% latency, 16.88 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 95.84 us = 0.05% latency, 63.72 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.5 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.5 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.12 us = 0.03% latency, 881.55 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.12 us = 0.03% latency, 881.55 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 47.68 us = 0.03% latency, 250.14 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 47.68 us = 0.03% latency, 250.14 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 269.89 us = 0.15% latency, 90.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.41 ms = 1.33% latency, 106.59 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 714.06 us = 0.39% latency, 119.73 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 708.58 us = 0.39% latency, 120.66 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 723.84 us = 0.4% latency, 118.12 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 81.78 us = 0.05% latency, 127.62 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 200.99 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 197.89 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (5): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 4.98 ms = 2.75% latency, 65.61 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.03 ms = 1.12% latency, 34.55 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 572.68 us = 0.32% latency, 42.82 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 270.61 us = 0.15% latency, 90.27 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 53.17 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 53.17 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 59.13 us = 0.03% latency, 806.9 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 59.13 us = 0.03% latency, 806.9 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 50.31 us = 0.03% latency, 948.39 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 50.31 us = 0.03% latency, 948.39 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 383.85 us = 0.21% latency, 16.06 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 98.23 us = 0.05% latency, 62.17 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.26 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.26 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 57.46 us = 0.03% latency, 830.34 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 57.46 us = 0.03% latency, 830.34 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.83 us = 0.03% latency, 239.37 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.83 us = 0.03% latency, 239.37 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 366.21 us = 0.2% latency, 16.84 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 95.61 us = 0.05% latency, 63.88 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.5 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.5 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.6 us = 0.03% latency, 873.85 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.6 us = 0.03% latency, 873.85 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 47.92 us = 0.03% latency, 248.89 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 47.92 us = 0.03% latency, 248.89 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 269.41 us = 0.15% latency, 90.67 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.41 ms = 1.33% latency, 106.54 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 716.21 us = 0.4% latency, 119.37 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 711.2 us = 0.39% latency, 120.21 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 724.08 us = 0.4% latency, 118.08 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 81.54 us = 0.05% latency, 128 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 201.46 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 196.7 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (6): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 5 ms = 2.76% latency, 65.33 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.02 ms = 1.12% latency, 34.66 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 573.64 us = 0.32% latency, 42.75 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 271.56 us = 0.15% latency, 89.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.21 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.21 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 59.37 us = 0.03% latency, 803.66 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 59.37 us = 0.03% latency, 803.66 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 51.5 us = 0.03% latency, 926.44 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 51.5 us = 0.03% latency, 926.44 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 375.99 us = 0.21% latency, 16.4 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 98.23 us = 0.05% latency, 62.17 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.98 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.98 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.74 us = 0.03% latency, 840.8 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.74 us = 0.03% latency, 840.8 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.88 us = 0.03% latency, 244.04 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.88 us = 0.03% latency, 244.04 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 365.73 us = 0.2% latency, 16.86 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 95.61 us = 0.05% latency, 63.88 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.5 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.5 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.84 us = 0.03% latency, 870.05 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.84 us = 0.03% latency, 870.05 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.88 us = 0.03% latency, 244.04 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.88 us = 0.03% latency, 244.04 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 271.08 us = 0.15% latency, 90.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.43 ms = 1.35% latency, 105.46 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 724.79 us = 0.4% latency, 117.96 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 717.4 us = 0.4% latency, 119.18 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 731.47 us = 0.4% latency, 116.88 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 81.54 us = 0.05% latency, 128 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 201.46 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 198.13 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (7): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 5.02 ms = 2.78% latency, 65.06 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.04 ms = 1.13% latency, 34.3 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 579.12 us = 0.32% latency, 42.35 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 277.28 us = 0.15% latency, 88.1 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 53.41 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 53.41 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 57.94 us = 0.03% latency, 823.5 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 57.94 us = 0.03% latency, 823.5 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 49.83 us = 0.03% latency, 957.47 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 49.83 us = 0.03% latency, 957.47 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 374.56 us = 0.21% latency, 16.46 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 99.9 us = 0.06% latency, 61.13 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.5 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.5 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.07 us = 0.03% latency, 866.28 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.07 us = 0.03% latency, 866.28 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.64 us = 0.03% latency, 245.23 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.64 us = 0.03% latency, 245.23 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 373.84 us = 0.21% latency, 16.5 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 97.51 us = 0.05% latency, 62.63 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.45 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.45 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.07 us = 0.03% latency, 866.28 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.07 us = 0.03% latency, 866.28 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.35 us = 0.03% latency, 241.68 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.35 us = 0.03% latency, 241.68 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 276.8 us = 0.15% latency, 88.25 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.43 ms = 1.34% latency, 105.52 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 722.89 us = 0.4% latency, 118.27 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 717.4 us = 0.4% latency, 119.18 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 729.32 us = 0.4% latency, 117.23 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 83.45 us = 0.05% latency, 125.07 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 201.94 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 198.84 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (8): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 5.02 ms = 2.77% latency, 65.09 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.04 ms = 1.13% latency, 34.28 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 577.45 us = 0.32% latency, 42.47 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 277.52 us = 0.15% latency, 88.02 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.45 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.45 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.41 us = 0.03% latency, 816.78 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.41 us = 0.03% latency, 816.78 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 49.11 us = 0.03% latency, 971.41 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 49.11 us = 0.03% latency, 971.41 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 373.84 us = 0.21% latency, 16.5 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 99.66 us = 0.06% latency, 61.28 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.02 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.02 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.03 us = 0.03% latency, 851.54 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.03 us = 0.03% latency, 851.54 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.16 us = 0.03% latency, 247.66 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.16 us = 0.03% latency, 247.66 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 377.42 us = 0.21% latency, 16.34 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 97.27 us = 0.05% latency, 62.78 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.26 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.26 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.6 us = 0.03% latency, 873.85 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.6 us = 0.03% latency, 873.85 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 57.22 us = 0.03% latency, 208.45 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 57.22 us = 0.03% latency, 208.45 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 276.8 us = 0.15% latency, 88.25 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.43 ms = 1.34% latency, 105.65 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 722.65 us = 0.4% latency, 118.31 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 715.73 us = 0.4% latency, 119.45 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 732.42 us = 0.41% latency, 116.73 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 82.02 us = 0.05% latency, 127.25 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 201.94 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 199.08 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (9): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 5.01 ms = 2.77% latency, 65.1 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.04 ms = 1.13% latency, 34.33 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 583.41 us = 0.32% latency, 42.03 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 277.76 us = 0.15% latency, 87.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 54.36 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 54.36 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.89 us = 0.03% latency, 810.17 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.89 us = 0.03% latency, 810.17 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 50.07 us = 0.03% latency, 952.91 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 50.07 us = 0.03% latency, 952.91 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 374.32 us = 0.21% latency, 16.47 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 99.66 us = 0.06% latency, 61.28 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 53.17 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 53.17 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.36 us = 0.03% latency, 877.68 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.36 us = 0.03% latency, 877.68 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.83 us = 0.03% latency, 239.37 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.83 us = 0.03% latency, 239.37 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 368.83 us = 0.2% latency, 16.72 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 97.04 us = 0.05% latency, 62.93 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.45 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.45 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 53.88 us = 0.03% latency, 885.45 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 53.88 us = 0.03% latency, 885.45 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 47.92 us = 0.03% latency, 248.89 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 47.92 us = 0.03% latency, 248.89 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 277.28 us = 0.15% latency, 88.1 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.43 ms = 1.34% latency, 105.51 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 722.41 us = 0.4% latency, 118.35 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 716.21 us = 0.4% latency, 119.37 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 733.85 us = 0.41% latency, 116.5 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 81.54 us = 0.05% latency, 128 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 201.7 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 197.89 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (10): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 5 ms = 2.77% latency, 65.24 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.03 ms = 1.12% latency, 34.48 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 576.97 us = 0.32% latency, 42.5 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 277.76 us = 0.15% latency, 87.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.93 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.93 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 57.94 us = 0.03% latency, 823.5 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 57.94 us = 0.03% latency, 823.5 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 49.11 us = 0.03% latency, 971.41 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 49.11 us = 0.03% latency, 971.41 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 372.65 us = 0.21% latency, 16.55 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 99.42 us = 0.05% latency, 61.42 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.21 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.21 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.6 us = 0.03% latency, 873.85 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.6 us = 0.03% latency, 873.85 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.59 us = 0.03% latency, 240.52 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.59 us = 0.03% latency, 240.52 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 365.02 us = 0.2% latency, 16.89 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 97.75 us = 0.05% latency, 62.47 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.26 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.26 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.12 us = 0.03% latency, 881.55 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.12 us = 0.03% latency, 881.55 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 47.92 us = 0.03% latency, 248.89 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 47.92 us = 0.03% latency, 248.89 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 277.04 us = 0.15% latency, 88.17 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.43 ms = 1.34% latency, 105.6 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 720.02 us = 0.4% latency, 118.74 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 716.92 us = 0.4% latency, 119.25 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 730.04 us = 0.4% latency, 117.11 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 83.21 us = 0.05% latency, 125.43 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 201.7 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 198.13 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (11): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 5.02 ms = 2.78% latency, 65.06 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.05 ms = 1.13% latency, 34.19 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 578.17 us = 0.32% latency, 42.42 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 277.28 us = 0.15% latency, 88.1 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 53.41 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 53.41 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 57.94 us = 0.03% latency, 823.5 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 57.94 us = 0.03% latency, 823.5 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 50.78 us = 0.03% latency, 939.49 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 50.78 us = 0.03% latency, 939.49 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 376.94 us = 0.21% latency, 16.36 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 100.14 us = 0.06% latency, 60.99 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.45 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.45 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.03 us = 0.03% latency, 851.54 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.03 us = 0.03% latency, 851.54 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.35 us = 0.03% latency, 241.68 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.35 us = 0.03% latency, 241.68 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 370.98 us = 0.21% latency, 16.62 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 98.23 us = 0.05% latency, 62.17 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.98 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.98 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.36 us = 0.03% latency, 877.68 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.36 us = 0.03% latency, 877.68 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.64 us = 0.03% latency, 245.23 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.64 us = 0.03% latency, 245.23 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 284.91 us = 0.16% latency, 85.74 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.42 ms = 1.34% latency, 105.79 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 720.74 us = 0.4% latency, 118.62 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 715.26 us = 0.4% latency, 119.53 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 731.71 us = 0.4% latency, 116.85 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 81.3 us = 0.04% latency, 128.37 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 202.18 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 199.08 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (12): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 5 ms = 2.77% latency, 65.26 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.03 ms = 1.12% latency, 34.5 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 579.6 us = 0.32% latency, 42.31 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 276.57 us = 0.15% latency, 88.32 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.45 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.45 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.41 us = 0.03% latency, 816.78 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.41 us = 0.03% latency, 816.78 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 50.31 us = 0.03% latency, 948.39 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 50.31 us = 0.03% latency, 948.39 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 375.27 us = 0.21% latency, 16.43 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 101.09 us = 0.06% latency, 60.41 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.21 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.21 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.79 us = 0.03% latency, 855.18 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.79 us = 0.03% latency, 855.18 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.11 us = 0.03% latency, 242.85 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.11 us = 0.03% latency, 242.85 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 367.4 us = 0.2% latency, 16.78 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 97.75 us = 0.05% latency, 62.47 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.5 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.5 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.6 us = 0.03% latency, 873.85 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.6 us = 0.03% latency, 873.85 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 47.68 us = 0.03% latency, 250.14 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 47.68 us = 0.03% latency, 250.14 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 276.09 us = 0.15% latency, 88.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.43 ms = 1.34% latency, 105.67 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 719.79 us = 0.4% latency, 118.78 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 717.16 us = 0.4% latency, 119.22 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 732.18 us = 0.4% latency, 116.77 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 81.54 us = 0.05% latency, 128 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 202.18 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 198.13 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (13): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 5 ms = 2.77% latency, 65.24 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.03 ms = 1.12% latency, 34.42 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 577.45 us = 0.32% latency, 42.47 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 277.04 us = 0.15% latency, 88.17 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 53.41 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 53.41 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.65 us = 0.03% latency, 813.46 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.65 us = 0.03% latency, 813.46 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 49.59 us = 0.03% latency, 962.07 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 49.59 us = 0.03% latency, 962.07 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 372.41 us = 0.21% latency, 16.56 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 99.66 us = 0.06% latency, 61.28 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.21 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.21 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.84 us = 0.03% latency, 870.05 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.84 us = 0.03% latency, 870.05 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.16 us = 0.03% latency, 247.66 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.16 us = 0.03% latency, 247.66 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 370.98 us = 0.21% latency, 16.62 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 97.99 us = 0.05% latency, 62.32 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.21 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.21 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.79 us = 0.03% latency, 855.18 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.79 us = 0.03% latency, 855.18 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.35 us = 0.03% latency, 241.68 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.35 us = 0.03% latency, 241.68 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 277.28 us = 0.15% latency, 88.1 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.42 ms = 1.34% latency, 105.87 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 718.83 us = 0.4% latency, 118.94 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 712.63 us = 0.39% latency, 119.97 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 732.18 us = 0.4% latency, 116.77 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 82.02 us = 0.05% latency, 127.25 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 200.75 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 198.6 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (14): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 5.02 ms = 2.78% latency, 65 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.04 ms = 1.13% latency, 34.3 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 583.41 us = 0.32% latency, 42.03 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 277.52 us = 0.15% latency, 88.02 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 53.64 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 53.64 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 59.6 us = 0.03% latency, 800.44 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 59.6 us = 0.03% latency, 800.44 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 50.31 us = 0.03% latency, 948.39 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 50.31 us = 0.03% latency, 948.39 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 376.22 us = 0.21% latency, 16.39 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 100.14 us = 0.06% latency, 60.99 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.45 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.45 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.31 us = 0.03% latency, 862.55 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.31 us = 0.03% latency, 862.55 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.83 us = 0.03% latency, 239.37 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.83 us = 0.03% latency, 239.37 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 366.45 us = 0.2% latency, 16.83 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 97.99 us = 0.05% latency, 62.32 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.98 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.98 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 53.88 us = 0.03% latency, 885.45 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 53.88 us = 0.03% latency, 885.45 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 47.92 us = 0.03% latency, 248.89 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 47.92 us = 0.03% latency, 248.89 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 278.95 us = 0.15% latency, 87.57 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.43 ms = 1.35% latency, 105.38 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 722.17 us = 0.4% latency, 118.39 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 719.55 us = 0.4% latency, 118.82 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 732.9 us = 0.41% latency, 116.66 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 82.25 us = 0.05% latency, 126.88 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 201.94 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 198.6 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (15): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 5.02 ms = 2.78% latency, 65.05 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.05 ms = 1.13% latency, 34.2 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 584.13 us = 0.32% latency, 41.98 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 278.95 us = 0.15% latency, 87.57 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 53.41 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 53.41 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 59.37 us = 0.03% latency, 803.66 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 59.37 us = 0.03% latency, 803.66 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 50.54 us = 0.03% latency, 943.92 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 50.54 us = 0.03% latency, 943.92 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 372.65 us = 0.21% latency, 16.55 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 100.85 us = 0.06% latency, 60.55 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.69 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.69 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.07 us = 0.03% latency, 866.28 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.07 us = 0.03% latency, 866.28 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 47.68 us = 0.03% latency, 250.14 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 47.68 us = 0.03% latency, 250.14 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 377.42 us = 0.21% latency, 16.34 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 107.05 us = 0.06% latency, 57.05 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.02 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.02 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.6 us = 0.03% latency, 873.85 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.6 us = 0.03% latency, 873.85 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.64 us = 0.03% latency, 245.23 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.64 us = 0.03% latency, 245.23 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 277.76 us = 0.15% latency, 87.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.43 ms = 1.34% latency, 105.75 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 721.45 us = 0.4% latency, 118.51 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 715.26 us = 0.4% latency, 119.53 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 732.42 us = 0.41% latency, 116.73 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 81.54 us = 0.05% latency, 128 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 201.23 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 199.56 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (16): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 5.03 ms = 2.78% latency, 64.85 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.05 ms = 1.14% latency, 34.08 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 582.46 us = 0.32% latency, 42.1 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 279.66 us = 0.15% latency, 87.35 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 53.64 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 53.64 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.17 us = 0.03% latency, 820.13 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.17 us = 0.03% latency, 820.13 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 49.35 us = 0.03% latency, 966.72 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 49.35 us = 0.03% latency, 966.72 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 377.42 us = 0.21% latency, 16.34 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 100.85 us = 0.06% latency, 60.55 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.69 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.69 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.51 us = 0.03% latency, 844.35 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.51 us = 0.03% latency, 844.35 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.88 us = 0.03% latency, 244.04 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.88 us = 0.03% latency, 244.04 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 372.41 us = 0.21% latency, 16.56 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 98.71 us = 0.05% latency, 61.87 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.21 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.21 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.51 us = 0.03% latency, 844.35 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.51 us = 0.03% latency, 844.35 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.16 us = 0.03% latency, 247.66 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.16 us = 0.03% latency, 247.66 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 278.95 us = 0.15% latency, 87.57 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.43 ms = 1.35% latency, 105.38 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 721.45 us = 0.4% latency, 118.51 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 717.88 us = 0.4% latency, 119.1 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 735.76 us = 0.41% latency, 116.2 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 81.78 us = 0.05% latency, 127.62 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 202.42 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 198.6 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (17): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 5.04 ms = 2.79% latency, 64.83 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.05 ms = 1.13% latency, 34.18 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 581.26 us = 0.32% latency, 42.19 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 280.38 us = 0.16% latency, 87.12 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 53.41 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 53.41 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.17 us = 0.03% latency, 820.13 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.17 us = 0.03% latency, 820.13 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 49.83 us = 0.03% latency, 957.47 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 49.83 us = 0.03% latency, 957.47 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 378.61 us = 0.21% latency, 16.29 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 101.33 us = 0.06% latency, 60.27 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.69 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.69 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.55 us = 0.03% latency, 858.85 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.55 us = 0.03% latency, 858.85 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.59 us = 0.03% latency, 240.52 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.59 us = 0.03% latency, 240.52 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 372.17 us = 0.21% latency, 16.57 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 98.94 us = 0.05% latency, 61.72 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.26 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.26 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.79 us = 0.03% latency, 855.18 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.79 us = 0.03% latency, 855.18 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.88 us = 0.03% latency, 244.04 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.88 us = 0.03% latency, 244.04 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 279.66 us = 0.15% latency, 87.35 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.44 ms = 1.35% latency, 105.08 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 725.27 us = 0.4% latency, 117.88 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 716.45 us = 0.4% latency, 119.33 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 737.19 us = 0.41% latency, 115.98 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 84.4 us = 0.05% latency, 123.66 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 201.94 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 198.36 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (18): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 5.05 ms = 2.79% latency, 64.68 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.06 ms = 1.14% latency, 33.98 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 582.46 us = 0.32% latency, 42.1 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 279.9 us = 0.15% latency, 87.27 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 53.41 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 53.41 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.17 us = 0.03% latency, 820.13 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.17 us = 0.03% latency, 820.13 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 50.78 us = 0.03% latency, 939.49 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 50.78 us = 0.03% latency, 939.49 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 378.61 us = 0.21% latency, 16.29 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 102.04 us = 0.06% latency, 59.85 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 53.64 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 53.64 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.03 us = 0.03% latency, 851.54 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.03 us = 0.03% latency, 851.54 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.83 us = 0.03% latency, 239.37 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.83 us = 0.03% latency, 239.37 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 371.22 us = 0.21% latency, 16.61 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 99.9 us = 0.06% latency, 61.13 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.26 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.26 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.84 us = 0.03% latency, 870.05 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.84 us = 0.03% latency, 870.05 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 47.68 us = 0.03% latency, 250.14 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 47.68 us = 0.03% latency, 250.14 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 278.47 us = 0.15% latency, 87.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.44 ms = 1.35% latency, 105.03 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 724.55 us = 0.4% latency, 118 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 718.12 us = 0.4% latency, 119.06 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 740.05 us = 0.41% latency, 115.53 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 82.02 us = 0.05% latency, 127.25 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 202.18 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 197.41 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (19): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 5.07 ms = 2.8% latency, 64.42 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.07 ms = 1.14% latency, 33.83 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 589.85 us = 0.33% latency, 41.58 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 281.33 us = 0.16% latency, 86.83 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 54.6 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 54.6 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 59.84 us = 0.03% latency, 797.26 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 59.84 us = 0.03% latency, 797.26 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 51.02 us = 0.03% latency, 935.1 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 51.02 us = 0.03% latency, 935.1 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 381.71 us = 0.21% latency, 16.16 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 101.09 us = 0.06% latency, 60.41 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.69 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.69 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.17 us = 0.03% latency, 820.13 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.17 us = 0.03% latency, 820.13 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 50.07 us = 0.03% latency, 238.23 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 50.07 us = 0.03% latency, 238.23 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 374.79 us = 0.21% latency, 16.45 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 99.42 us = 0.05% latency, 61.42 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.74 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.74 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.55 us = 0.03% latency, 858.85 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.55 us = 0.03% latency, 858.85 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.88 us = 0.03% latency, 244.04 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.88 us = 0.03% latency, 244.04 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 278.71 us = 0.15% latency, 87.64 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.45 ms = 1.36% latency, 104.68 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 724.32 us = 0.4% latency, 118.04 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 722.89 us = 0.4% latency, 118.27 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 740.53 us = 0.41% latency, 115.45 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 82.97 us = 0.05% latency, 125.79 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 201.46 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 198.84 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (20): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 5.05 ms = 2.79% latency, 64.62 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.04 ms = 1.13% latency, 34.24 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 580.31 us = 0.32% latency, 42.26 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 279.19 us = 0.15% latency, 87.5 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.93 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.93 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.17 us = 0.03% latency, 820.13 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.17 us = 0.03% latency, 820.13 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 49.35 us = 0.03% latency, 966.72 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 49.35 us = 0.03% latency, 966.72 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 375.51 us = 0.21% latency, 16.42 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 101.09 us = 0.06% latency, 60.41 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.69 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.69 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.31 us = 0.03% latency, 862.55 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.31 us = 0.03% latency, 862.55 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.64 us = 0.03% latency, 245.23 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.64 us = 0.03% latency, 245.23 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 370.03 us = 0.2% latency, 16.67 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 98.47 us = 0.05% latency, 62.02 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.74 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.74 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.07 us = 0.03% latency, 866.28 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.07 us = 0.03% latency, 866.28 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.4 us = 0.03% latency, 246.44 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.4 us = 0.03% latency, 246.44 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 278.95 us = 0.15% latency, 87.57 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.46 ms = 1.36% latency, 104.28 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 728.37 us = 0.4% latency, 117.38 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 725.75 us = 0.4% latency, 117.81 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 741.48 us = 0.41% latency, 115.31 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 82.97 us = 0.05% latency, 125.79 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 202.66 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 198.13 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (21): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 5.05 ms = 2.79% latency, 64.63 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.06 ms = 1.14% latency, 33.97 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 584.13 us = 0.32% latency, 41.98 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 280.14 us = 0.15% latency, 87.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.69 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.69 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 59.6 us = 0.03% latency, 800.44 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 59.6 us = 0.03% latency, 800.44 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 50.54 us = 0.03% latency, 943.92 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 50.54 us = 0.03% latency, 943.92 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 378.85 us = 0.21% latency, 16.28 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 100.61 us = 0.06% latency, 60.7 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.21 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.21 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.51 us = 0.03% latency, 844.35 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.51 us = 0.03% latency, 844.35 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.83 us = 0.03% latency, 239.37 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.83 us = 0.03% latency, 239.37 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 370.26 us = 0.2% latency, 16.65 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 98.47 us = 0.05% latency, 62.02 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.98 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.98 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.36 us = 0.03% latency, 877.68 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.36 us = 0.03% latency, 877.68 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.88 us = 0.03% latency, 244.04 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.88 us = 0.03% latency, 244.04 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 287.77 us = 0.16% latency, 84.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.44 ms = 1.35% latency, 104.97 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 727.18 us = 0.4% latency, 117.57 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 722.17 us = 0.4% latency, 118.39 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 736.24 us = 0.41% latency, 116.13 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 82.02 us = 0.05% latency, 127.25 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 201.7 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 199.56 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (22): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 5.06 ms = 2.8% latency, 64.54 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.07 ms = 1.14% latency, 33.86 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 592.71 us = 0.33% latency, 41.37 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 278.71 us = 0.15% latency, 87.64 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 53.41 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 53.41 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 60.32 us = 0.03% latency, 790.95 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 60.32 us = 0.03% latency, 790.95 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 51.02 us = 0.03% latency, 935.1 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 51.02 us = 0.03% latency, 935.1 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 382.9 us = 0.21% latency, 16.1 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 101.8 us = 0.06% latency, 59.99 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.93 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.93 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.98 us = 0.03% latency, 837.29 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.98 us = 0.03% latency, 837.29 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 50.07 us = 0.03% latency, 238.23 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 50.07 us = 0.03% latency, 238.23 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 374.32 us = 0.21% latency, 16.47 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 98.47 us = 0.05% latency, 62.02 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.21 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.21 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.55 us = 0.03% latency, 858.85 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.55 us = 0.03% latency, 858.85 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.35 us = 0.03% latency, 241.68 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.35 us = 0.03% latency, 241.68 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 277.52 us = 0.15% latency, 88.02 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.44 ms = 1.35% latency, 104.99 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 726.7 us = 0.4% latency, 117.65 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 718.83 us = 0.4% latency, 118.94 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 736.95 us = 0.41% latency, 116.01 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 82.49 us = 0.05% latency, 126.52 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 202.66 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 198.84 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (23): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 5.03 ms = 2.78% latency, 64.9 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.04 ms = 1.13% latency, 34.28 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 579.83 us = 0.32% latency, 42.29 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 278.23 us = 0.15% latency, 87.8 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.93 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.93 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.89 us = 0.03% latency, 810.17 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.89 us = 0.03% latency, 810.17 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 49.35 us = 0.03% latency, 966.72 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 49.35 us = 0.03% latency, 966.72 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 379.56 us = 0.21% latency, 16.25 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 100.37 us = 0.06% latency, 60.84 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.45 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.45 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 57.22 us = 0.03% latency, 833.8 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 57.22 us = 0.03% latency, 833.8 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.11 us = 0.03% latency, 242.85 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.11 us = 0.03% latency, 242.85 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 368.6 us = 0.2% latency, 16.73 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 97.99 us = 0.05% latency, 62.32 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.74 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.74 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.07 us = 0.03% latency, 866.28 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.07 us = 0.03% latency, 866.28 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.64 us = 0.03% latency, 245.23 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.64 us = 0.03% latency, 245.23 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 277.04 us = 0.15% latency, 88.17 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.44 ms = 1.35% latency, 105.1 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 724.32 us = 0.4% latency, 118.04 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 720.5 us = 0.4% latency, 118.66 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 736.71 us = 0.41% latency, 116.05 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 81.78 us = 0.05% latency, 127.62 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 202.89 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 198.36 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (24): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 5.03 ms = 2.78% latency, 64.86 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.04 ms = 1.13% latency, 34.25 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 580.79 us = 0.32% latency, 42.22 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 278 us = 0.15% latency, 87.87 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 53.64 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 53.64 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 59.13 us = 0.03% latency, 806.9 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 59.13 us = 0.03% latency, 806.9 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 49.59 us = 0.03% latency, 962.07 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 49.59 us = 0.03% latency, 962.07 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 377.66 us = 0.21% latency, 16.33 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 100.37 us = 0.06% latency, 60.84 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.45 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.45 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.79 us = 0.03% latency, 855.18 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.79 us = 0.03% latency, 855.18 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.83 us = 0.03% latency, 239.37 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.83 us = 0.03% latency, 239.37 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 370.03 us = 0.2% latency, 16.67 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 97.99 us = 0.05% latency, 62.32 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.5 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.5 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.03 us = 0.03% latency, 851.54 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.03 us = 0.03% latency, 851.54 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.64 us = 0.03% latency, 245.23 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.64 us = 0.03% latency, 245.23 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 278 us = 0.15% latency, 87.87 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.44 ms = 1.35% latency, 105.08 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 726.22 us = 0.4% latency, 117.73 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 719.31 us = 0.4% latency, 118.86 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 736 us = 0.41% latency, 116.16 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 81.78 us = 0.05% latency, 127.62 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 202.18 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 199.79 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (25): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 5.05 ms = 2.79% latency, 64.65 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.06 ms = 1.14% latency, 34.01 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 582.7 us = 0.32% latency, 42.09 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 278 us = 0.15% latency, 87.87 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 53.41 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 53.41 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.89 us = 0.03% latency, 810.17 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.89 us = 0.03% latency, 810.17 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 51.02 us = 0.03% latency, 935.1 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 51.02 us = 0.03% latency, 935.1 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 385.76 us = 0.21% latency, 15.99 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 100.85 us = 0.06% latency, 60.55 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.21 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.21 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.79 us = 0.03% latency, 855.18 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.79 us = 0.03% latency, 855.18 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 57.94 us = 0.03% latency, 205.88 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 57.94 us = 0.03% latency, 205.88 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 370.26 us = 0.2% latency, 16.65 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 98.94 us = 0.05% latency, 61.72 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.74 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.74 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.84 us = 0.03% latency, 870.05 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.84 us = 0.03% latency, 870.05 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.4 us = 0.03% latency, 246.44 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.4 us = 0.03% latency, 246.44 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 277.76 us = 0.15% latency, 87.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.44 ms = 1.35% latency, 104.91 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 724.79 us = 0.4% latency, 117.96 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 720.98 us = 0.4% latency, 118.58 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 739.34 us = 0.41% latency, 115.64 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 82.02 us = 0.05% latency, 127.25 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 201.7 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 198.84 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (26): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 5.04 ms = 2.79% latency, 64.79 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.04 ms = 1.13% latency, 34.24 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 581.5 us = 0.32% latency, 42.17 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 278.23 us = 0.15% latency, 87.8 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.93 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.93 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.89 us = 0.03% latency, 810.17 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.89 us = 0.03% latency, 810.17 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 50.54 us = 0.03% latency, 943.92 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 50.54 us = 0.03% latency, 943.92 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 377.18 us = 0.21% latency, 16.35 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 100.37 us = 0.06% latency, 60.84 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.98 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.98 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.27 us = 0.03% latency, 847.93 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.27 us = 0.03% latency, 847.93 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.4 us = 0.03% latency, 246.44 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.4 us = 0.03% latency, 246.44 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 369.07 us = 0.2% latency, 16.71 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 97.99 us = 0.05% latency, 62.32 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.74 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.74 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.31 us = 0.03% latency, 862.55 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.31 us = 0.03% latency, 862.55 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.16 us = 0.03% latency, 247.66 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 48.16 us = 0.03% latency, 247.66 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 276.57 us = 0.15% latency, 88.32 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.45 ms = 1.35% latency, 104.84 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 725.51 us = 0.4% latency, 117.84 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 720.5 us = 0.4% latency, 118.66 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 739.57 us = 0.41% latency, 115.6 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 82.02 us = 0.05% latency, 127.25 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 203.13 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 198.6 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (27): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 5.03 ms = 2.78% latency, 64.88 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.04 ms = 1.13% latency, 34.3 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 580.07 us = 0.32% latency, 42.28 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 278.71 us = 0.15% latency, 87.64 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 53.64 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 53.64 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 57.94 us = 0.03% latency, 823.5 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 57.94 us = 0.03% latency, 823.5 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 49.83 us = 0.03% latency, 957.47 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 49.83 us = 0.03% latency, 957.47 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 375.75 us = 0.21% latency, 16.41 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 100.37 us = 0.06% latency, 60.84 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.21 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.21 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.27 us = 0.03% latency, 847.93 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 56.27 us = 0.03% latency, 847.93 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.11 us = 0.03% latency, 242.85 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.11 us = 0.03% latency, 242.85 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 371.22 us = 0.21% latency, 16.61 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 97.75 us = 0.05% latency, 62.47 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.5 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.5 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.55 us = 0.03% latency, 858.85 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.55 us = 0.03% latency, 858.85 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.11 us = 0.03% latency, 242.85 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.11 us = 0.03% latency, 242.85 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 276.8 us = 0.15% latency, 88.25 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.45 ms = 1.35% latency, 104.84 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 725.98 us = 0.4% latency, 117.77 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 721.22 us = 0.4% latency, 118.55 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 738.14 us = 0.41% latency, 115.83 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 83.92 us = 0.05% latency, 124.36 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 201.7 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 197.89 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (28): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 5.12 ms = 2.83% latency, 63.77 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.13 ms = 1.18% latency, 32.86 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 580.31 us = 0.32% latency, 42.26 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 278.47 us = 0.15% latency, 87.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 53.64 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 53.64 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.89 us = 0.03% latency, 810.17 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.89 us = 0.03% latency, 810.17 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 50.07 us = 0.03% latency, 952.91 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 50.07 us = 0.03% latency, 952.91 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 377.42 us = 0.21% latency, 16.34 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 100.85 us = 0.06% latency, 60.55 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.93 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.93 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.31 us = 0.03% latency, 862.55 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.31 us = 0.03% latency, 862.55 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 50.54 us = 0.03% latency, 235.98 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 50.54 us = 0.03% latency, 235.98 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 453.47 us = 0.25% latency, 13.6 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 157.12 us = 0.09% latency, 38.87 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 59.37 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 59.37 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 59.84 us = 0.03% latency, 797.26 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 59.84 us = 0.03% latency, 797.26 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 47.92 us = 0.03% latency, 248.89 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 47.92 us = 0.03% latency, 248.89 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 277.28 us = 0.15% latency, 88.1 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.44 ms = 1.35% latency, 105.05 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 726.22 us = 0.4% latency, 117.73 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 719.55 us = 0.4% latency, 118.82 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 737.67 us = 0.41% latency, 115.9 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 82.02 us = 0.05% latency, 127.25 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 202.66 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 199.32 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (29): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 5.06 ms = 2.8% latency, 64.48 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.05 ms = 1.13% latency, 34.14 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 584.84 us = 0.32% latency, 41.93 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 279.19 us = 0.15% latency, 87.5 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 53.41 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 53.41 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.17 us = 0.03% latency, 820.13 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.17 us = 0.03% latency, 820.13 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 49.35 us = 0.03% latency, 966.72 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 49.35 us = 0.03% latency, 966.72 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 376.22 us = 0.21% latency, 16.39 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 100.85 us = 0.06% latency, 60.55 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.74 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.74 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.55 us = 0.03% latency, 858.85 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.55 us = 0.03% latency, 858.85 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 50.07 us = 0.03% latency, 238.23 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 50.07 us = 0.03% latency, 238.23 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 369.55 us = 0.2% latency, 16.69 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 97.51 us = 0.05% latency, 62.63 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.21 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.21 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.12 us = 0.03% latency, 881.55 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.12 us = 0.03% latency, 881.55 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 47.68 us = 0.03% latency, 250.14 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 47.68 us = 0.03% latency, 250.14 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 277.76 us = 0.15% latency, 87.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.45 ms = 1.35% latency, 104.74 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 726.7 us = 0.4% latency, 117.65 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 720.98 us = 0.4% latency, 118.58 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 741.48 us = 0.41% latency, 115.31 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 82.73 us = 0.05% latency, 126.15 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 208.85 us = 0.12% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 198.36 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (30): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 5.05 ms = 2.79% latency, 64.62 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.06 ms = 1.14% latency, 34.02 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 590.09 us = 0.33% latency, 41.56 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 278.71 us = 0.15% latency, 87.64 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 54.12 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 54.12 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 61.04 us = 0.03% latency, 781.68 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 61.04 us = 0.03% latency, 781.68 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 51.26 us = 0.03% latency, 930.75 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 51.26 us = 0.03% latency, 930.75 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 376.46 us = 0.21% latency, 16.38 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 100.85 us = 0.06% latency, 60.55 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 52.21 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 52.21 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.55 us = 0.03% latency, 858.85 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.55 us = 0.03% latency, 858.85 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 47.68 us = 0.03% latency, 250.14 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 47.68 us = 0.03% latency, 250.14 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 371.93 us = 0.21% latency, 16.58 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 99.18 us = 0.05% latency, 61.57 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.5 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.5 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.36 us = 0.03% latency, 877.68 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 54.36 us = 0.03% latency, 877.68 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 47.92 us = 0.03% latency, 248.89 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 47.92 us = 0.03% latency, 248.89 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 279.66 us = 0.15% latency, 87.35 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.45 ms = 1.35% latency, 104.87 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 726.46 us = 0.4% latency, 117.69 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 720.26 us = 0.4% latency, 118.7 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 739.1 us = 0.41% latency, 115.68 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 82.97 us = 0.05% latency, 125.79 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 202.18 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 198.6 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (31): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 163.23 GMACs = 2.91% MACs, 5.04 ms = 2.79% latency, 64.72 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 34.98 GMACs = 0.62% MACs, 2.05 ms = 1.13% latency, 34.1 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 12.26 GMACs = 0.22% MACs, 578.88 us = 0.32% latency, 42.36 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 278.23 us = 0.15% latency, 87.8 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 53.41 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 53.41 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.41 us = 0.03% latency, 816.78 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 58.41 us = 0.03% latency, 816.78 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 49.83 us = 0.03% latency, 957.47 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 49.83 us = 0.03% latency, 957.47 GFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 376.22 us = 0.21% latency, 16.39 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 100.14 us = 0.06% latency, 60.99 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.98 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.98 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.55 us = 0.03% latency, 858.85 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.55 us = 0.03% latency, 858.85 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.11 us = 0.03% latency, 242.85 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 49.11 us = 0.03% latency, 242.85 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 3.08 GMACs = 0.06% MACs, 371.93 us = 0.21% latency, 16.58 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 3.05 GMACs = 0.05% MACs, 98.47 us = 0.05% latency, 62.02 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 51.98 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 51.98 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.31 us = 0.03% latency, 862.55 GFLOPS
                  (default): Linear(32.77 K = 0% Params, 23.86 MMACs = 0% MACs, 55.31 us = 0.03% latency, 862.55 GFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 47.45 us = 0.03% latency, 251.4 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 5.96 MMACs = 0% MACs, 47.45 us = 0.03% latency, 251.4 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 12.21 GMACs = 0.22% MACs, 277.76 us = 0.15% latency, 87.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 128.25 GMACs = 2.29% MACs, 2.44 ms = 1.35% latency, 105.01 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 726.94 us = 0.4% latency, 117.61 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 720.26 us = 0.4% latency, 118.7 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 42.75 GMACs = 0.76% MACs, 735.76 us = 0.41% latency, 116.2 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 81.54 us = 0.05% latency, 128 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 202.89 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 198.84 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 200.27 us = 0.11% latency, 0 FLOPS, (4096,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 46.59 KMACs = 0% MACs, 788.93 us = 0.44% latency, 118.11 MFLOPS)
      )
      (lm_head): Linear(525.34 M = 6.54% Params, 382.45 GMACs = 6.82% MACs, 11.4 ms = 6.3% latency, 67.11 TFLOPS, in_features=4096, out_features=128257, bias=False)
    )
  )
)
------------------------------------------------------------------------------
