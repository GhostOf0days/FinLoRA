
-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 2:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

world size:                                                             1       
data parallel size:                                                     1       
model parallel size:                                                    1       
batch size per GPU:                                                     4       
params per GPU:                                                         6.74 B  
params of model = params per GPU * mp_size:                             6.74 B  
fwd MACs per GPU:                                                       2.33 TMACs
fwd flops per GPU:                                                      4.67 T  
fwd flops of model = fwd flops per GPU * mp_size:                       4.67 T  
fwd latency:                                                            191 ms  
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    24.45 TFLOPS
bwd latency:                                                            265.88 ms
bwd FLOPS per GPU = 2 * fwd flops per GPU / bwd latency:                35.13 TFLOPS
fwd+bwd FLOPS per GPU = 3 * fwd flops per GPU / (fwd+bwd latency):      30.66 TFLOPS
step latency:                                                           66.36 ms
iter latency:                                                           523.24 ms
FLOPS per GPU = 3 * fwd flops per GPU / iter latency:                   26.78 TFLOPS
samples/second:                                                         7.64    

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'PeftModelForCausalLM': '6.74 B'}
    MACs        - {'PeftModelForCausalLM': '2.33 TMACs'}
    fwd latency - {'PeftModelForCausalLM': '189.87 ms'}
depth 1:
    params      - {'LoraModel': '6.74 B'}
    MACs        - {'LoraModel': '2.33 TMACs'}
    fwd latency - {'LoraModel': '189.87 ms'}
depth 2:
    params      - {'LlamaForCausalLM': '6.74 B'}
    MACs        - {'LlamaForCausalLM': '2.33 TMACs'}
    fwd latency - {'LlamaForCausalLM': '189.87 ms'}
depth 3:
    params      - {'LlamaModel': '6.61 B'}
    MACs        - {'LlamaModel': '2.29 TMACs'}
    fwd latency - {'LlamaModel': '188.21 ms'}
depth 4:
    params      - {'ModuleList': '6.48 B'}
    MACs        - {'ModuleList': '2.29 TMACs'}
    fwd latency - {'ModuleList': '180.08 ms'}
depth 5:
    params      - {'LlamaDecoderLayer': '6.48 B'}
    MACs        - {'LlamaDecoderLayer': '2.29 TMACs'}
    fwd latency - {'LlamaDecoderLayer': '180.08 ms'}
depth 6:
    params      - {'LlamaMLP': '4.33 B'}
    MACs        - {'LlamaMLP': '1.52 TMACs'}
    fwd latency - {'LlamaAttention': '100.35 ms'}
depth 7:
    params      - {'Linear': '6.48 B'}
    MACs        - {'Linear': '2.28 TMACs'}
    fwd latency - {'Linear': '124.14 ms'}
depth 8:
    params      - {'Linear': '1.61 B'}
    MACs        - {'Linear': '566.94 GMACs'}
    fwd latency - {'ModuleDict': '23.95 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

PeftModelForCausalLM(
  6.74 B = 100% Params, 2.33 TMACs = 100% MACs, 189.87 ms = 100% latency, 24.6 TFLOPS
  (base_model): LoraModel(
    6.74 B = 100% Params, 2.33 TMACs = 100% MACs, 189.87 ms = 100% latency, 24.6 TFLOPS
    (model): LlamaForCausalLM(
      6.74 B = 100% Params, 2.33 TMACs = 100% MACs, 189.87 ms = 100% latency, 24.6 TFLOPS
      (model): LlamaModel(
        6.61 B = 98.06% Params, 2.29 TMACs = 98.02% MACs, 188.21 ms = 99.12% latency, 24.32 TFLOPS
        (embed_tokens): Embedding(131.07 M = 1.94% Params, 0 MACs = 0% MACs, 115.16 us = 0.06% latency, 0 FLOPS, 32000, 4096, padding_idx=0)
        (layers): ModuleList(
          (0): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 6.23 ms = 3.28% latency, 22.97 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.63 ms = 1.91% latency, 13.18 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 798.94 us = 0.42% latency, 14.81 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 285.86 us = 0.15% latency, 41.32 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 88.45 us = 0.05% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 88.45 us = 0.05% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 100.14 us = 0.05% latency, 115.19 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 100.14 us = 0.05% latency, 115.19 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 102.04 us = 0.05% latency, 113.03 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 102.04 us = 0.05% latency, 113.03 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 677.11 us = 0.36% latency, 17.48 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 227.69 us = 0.12% latency, 51.87 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.24 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.24 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.36 us = 0.05% latency, 127.65 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.36 us = 0.05% latency, 127.65 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.73 us = 0.04% latency, 139.42 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.73 us = 0.04% latency, 139.42 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 763.65 us = 0.4% latency, 15.5 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 224.11 us = 0.12% latency, 52.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 86.07 us = 0.05% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 86.07 us = 0.05% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 111.58 us = 0.06% latency, 103.37 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 111.58 us = 0.06% latency, 103.37 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 99.42 us = 0.05% latency, 116.02 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 99.42 us = 0.05% latency, 116.02 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 241.04 us = 0.13% latency, 49 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 87.74 us = 0.05% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.94 ms = 1.02% latency, 49.15 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 593.19 us = 0.31% latency, 53.51 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 586.27 us = 0.31% latency, 54.14 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 478.27 us = 0.25% latency, 66.37 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 74.15 us = 0.04% latency, 52.26 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 251.29 us = 0.13% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 197.41 us = 0.1% latency, 0 FLOPS)
          )
          (1): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.66 ms = 2.98% latency, 25.26 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.12 ms = 1.64% latency, 15.33 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 686.41 us = 0.36% latency, 17.24 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 226.5 us = 0.12% latency, 52.15 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.91 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.91 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.31 us = 0.05% latency, 126.31 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.31 us = 0.05% latency, 126.31 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.92 us = 0.04% latency, 137.44 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.92 us = 0.04% latency, 137.44 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 673.77 us = 0.35% latency, 17.56 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 225.31 us = 0.12% latency, 52.42 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.88 us = 0.05% latency, 128.32 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.88 us = 0.05% latency, 128.32 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.97 us = 0.04% latency, 139.02 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.97 us = 0.04% latency, 139.02 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 663.28 us = 0.35% latency, 17.84 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 219.11 us = 0.12% latency, 53.91 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 69.86 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 69.86 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.17 us = 0.05% latency, 129.35 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.17 us = 0.05% latency, 129.35 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.97 us = 0.04% latency, 139.02 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.97 us = 0.04% latency, 139.02 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 225.07 us = 0.12% latency, 52.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 67.23 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.93 ms = 1.02% latency, 49.35 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 591.52 us = 0.31% latency, 53.66 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 583.41 us = 0.31% latency, 54.41 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 483.51 us = 0.25% latency, 65.65 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 67.23 us = 0.04% latency, 57.63 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 197.41 us = 0.1% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 203.61 us = 0.11% latency, 0 FLOPS)
          )
          (2): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.83 ms = 3.07% latency, 24.54 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.27 ms = 1.72% latency, 14.63 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 792.03 us = 0.42% latency, 14.94 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 239.37 us = 0.13% latency, 49.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 90.12 us = 0.05% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 90.12 us = 0.05% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 109.43 us = 0.06% latency, 105.4 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 109.43 us = 0.06% latency, 105.4 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 101.33 us = 0.05% latency, 113.83 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 101.33 us = 0.05% latency, 113.83 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 695.94 us = 0.37% latency, 17 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 234.37 us = 0.12% latency, 50.4 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 97.99 us = 0.05% latency, 117.71 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 97.99 us = 0.05% latency, 117.71 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.92 us = 0.04% latency, 137.44 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.92 us = 0.04% latency, 137.44 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 672.58 us = 0.35% latency, 17.6 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 223.88 us = 0.12% latency, 52.76 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.33 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.33 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.12 us = 0.05% latency, 127.99 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.12 us = 0.05% latency, 127.99 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.97 us = 0.04% latency, 139.02 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.97 us = 0.04% latency, 139.02 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 227.21 us = 0.12% latency, 51.98 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 67.47 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.92 ms = 1.01% latency, 49.61 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 588.18 us = 0.31% latency, 53.97 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 582.7 us = 0.31% latency, 54.48 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 480.89 us = 0.25% latency, 66.01 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 68.19 us = 0.04% latency, 56.83 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 236.27 us = 0.12% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 188.83 us = 0.1% latency, 0 FLOPS)
          )
          (3): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.64 ms = 2.97% latency, 25.36 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.13 ms = 1.65% latency, 15.26 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 686.65 us = 0.36% latency, 17.23 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 229.36 us = 0.12% latency, 51.5 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.43 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.43 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.6 us = 0.05% latency, 127.31 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.6 us = 0.05% latency, 127.31 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.68 us = 0.04% latency, 137.83 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.68 us = 0.04% latency, 137.83 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 672.34 us = 0.35% latency, 17.6 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 223.88 us = 0.12% latency, 52.76 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.1 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.1 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.6 us = 0.05% latency, 127.31 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.6 us = 0.05% latency, 127.31 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.45 us = 0.04% latency, 138.22 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.45 us = 0.04% latency, 138.22 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 674.01 us = 0.35% latency, 17.56 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 226.26 us = 0.12% latency, 52.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 71.05 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 71.05 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.65 us = 0.05% latency, 128.67 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.65 us = 0.05% latency, 128.67 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.45 us = 0.04% latency, 138.22 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.45 us = 0.04% latency, 138.22 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 225.31 us = 0.12% latency, 52.42 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 68.9 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.91 ms = 1.01% latency, 49.89 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 585.08 us = 0.31% latency, 54.25 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 579.6 us = 0.31% latency, 54.77 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 480.65 us = 0.25% latency, 66.04 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 67.47 us = 0.04% latency, 57.43 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 198.6 us = 0.1% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 199.79 us = 0.11% latency, 0 FLOPS)
          )
          (4): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.65 ms = 2.97% latency, 25.34 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.15 ms = 1.66% latency, 15.19 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 687.6 us = 0.36% latency, 17.21 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 228.88 us = 0.12% latency, 51.6 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.03 us = 0.05% latency, 125.33 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.03 us = 0.05% latency, 125.33 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.64 us = 0.04% latency, 136.28 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.64 us = 0.04% latency, 136.28 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 681.4 us = 0.36% latency, 17.37 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 228.17 us = 0.12% latency, 51.77 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 71.76 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 71.76 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.55 us = 0.05% latency, 125.99 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.55 us = 0.05% latency, 125.99 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.88 us = 0.04% latency, 135.89 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.88 us = 0.04% latency, 135.89 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 679.97 us = 0.36% latency, 17.4 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 222.68 us = 0.12% latency, 53.04 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.24 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.24 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.27 us = 0.05% latency, 125.01 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.27 us = 0.05% latency, 125.01 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.64 us = 0.04% latency, 136.28 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.64 us = 0.04% latency, 136.28 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 226.02 us = 0.12% latency, 52.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 67.95 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.91 ms = 1.01% latency, 49.85 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 583.41 us = 0.31% latency, 54.41 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 579.36 us = 0.31% latency, 54.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 483.04 us = 0.25% latency, 65.71 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 67 us = 0.04% latency, 57.84 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 196.93 us = 0.1% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 189.54 us = 0.1% latency, 0 FLOPS)
          )
          (5): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.61 ms = 2.95% latency, 25.51 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.12 ms = 1.64% latency, 15.32 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 687.6 us = 0.36% latency, 17.21 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 225.07 us = 0.12% latency, 52.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.67 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.67 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.27 us = 0.05% latency, 125.01 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.27 us = 0.05% latency, 125.01 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.64 us = 0.04% latency, 136.28 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.64 us = 0.04% latency, 136.28 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 676.63 us = 0.36% latency, 17.49 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 224.11 us = 0.12% latency, 52.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 71.76 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 71.76 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.31 us = 0.05% latency, 126.31 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.31 us = 0.05% latency, 126.31 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.64 us = 0.04% latency, 136.28 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.64 us = 0.04% latency, 136.28 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 679.49 us = 0.36% latency, 17.42 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 225.07 us = 0.12% latency, 52.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 71.53 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 71.53 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.27 us = 0.05% latency, 125.01 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.27 us = 0.05% latency, 125.01 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.16 us = 0.04% latency, 137.05 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.16 us = 0.04% latency, 137.05 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 214.1 us = 0.11% latency, 55.17 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 68.19 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.9 ms = 1% latency, 50.2 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 578.17 us = 0.3% latency, 54.9 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 574.59 us = 0.3% latency, 55.24 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 472.55 us = 0.25% latency, 67.17 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 69.86 us = 0.04% latency, 55.47 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 197.65 us = 0.1% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 189.07 us = 0.1% latency, 0 FLOPS)
          )
          (6): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.57 ms = 2.94% latency, 25.67 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.11 ms = 1.64% latency, 15.35 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 679.25 us = 0.36% latency, 17.42 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 216.48 us = 0.11% latency, 54.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 74.39 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 74.39 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.51 us = 0.05% latency, 124.69 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.51 us = 0.05% latency, 124.69 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 85.12 us = 0.04% latency, 135.51 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 85.12 us = 0.04% latency, 135.51 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 664.47 us = 0.35% latency, 17.81 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 212.91 us = 0.11% latency, 55.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.81 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.81 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.36 us = 0.05% latency, 127.65 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.36 us = 0.05% latency, 127.65 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.92 us = 0.04% latency, 137.44 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.92 us = 0.04% latency, 137.44 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 687.12 us = 0.36% latency, 17.22 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 213.15 us = 0.11% latency, 55.41 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 92.27 us = 0.05% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 92.27 us = 0.05% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.74 us = 0.05% latency, 124.37 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.74 us = 0.05% latency, 124.37 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.45 us = 0.04% latency, 138.22 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.45 us = 0.04% latency, 138.22 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 213.86 us = 0.11% latency, 55.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 68.9 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.87 ms = 0.98% latency, 50.99 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 567.67 us = 0.3% latency, 55.92 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 563.86 us = 0.3% latency, 56.29 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 471.35 us = 0.25% latency, 67.34 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 68.66 us = 0.04% latency, 56.43 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 199.08 us = 0.1% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 190.02 us = 0.1% latency, 0 FLOPS)
          )
          (7): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.57 ms = 2.93% latency, 25.7 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.07 ms = 1.62% latency, 15.56 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 671.63 us = 0.35% latency, 17.62 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 215.77 us = 0.11% latency, 54.74 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 71.76 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 71.76 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.03 us = 0.05% latency, 125.33 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.03 us = 0.05% latency, 125.33 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.92 us = 0.04% latency, 137.44 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.92 us = 0.04% latency, 137.44 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 661.13 us = 0.35% latency, 17.9 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 212.43 us = 0.11% latency, 55.6 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.36 us = 0.05% latency, 127.65 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.36 us = 0.05% latency, 127.65 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.45 us = 0.04% latency, 138.22 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.45 us = 0.04% latency, 138.22 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 658.99 us = 0.35% latency, 17.96 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 213.15 us = 0.11% latency, 55.41 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.1 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.1 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.88 us = 0.05% latency, 128.32 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.88 us = 0.05% latency, 128.32 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.25 us = 0.04% latency, 140.23 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.25 us = 0.04% latency, 140.23 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 214.1 us = 0.11% latency, 55.17 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 67.71 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.89 ms = 1% latency, 50.39 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 581.5 us = 0.31% latency, 54.59 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 566.96 us = 0.3% latency, 55.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 472.78 us = 0.25% latency, 67.14 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 69.38 us = 0.04% latency, 55.85 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 207.19 us = 0.11% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 192.88 us = 0.1% latency, 0 FLOPS)
          )
          (8): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.55 ms = 2.92% latency, 25.77 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.09 ms = 1.63% latency, 15.46 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 674.01 us = 0.35% latency, 17.56 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 216.72 us = 0.11% latency, 54.5 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.91 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.91 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.31 us = 0.05% latency, 126.31 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.31 us = 0.05% latency, 126.31 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.16 us = 0.04% latency, 137.05 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.16 us = 0.04% latency, 137.05 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 661.37 us = 0.35% latency, 17.89 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 213.38 us = 0.11% latency, 55.35 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 71.05 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 71.05 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.12 us = 0.05% latency, 127.99 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.12 us = 0.05% latency, 127.99 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.49 us = 0.04% latency, 139.82 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.49 us = 0.04% latency, 139.82 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 662.57 us = 0.35% latency, 17.86 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 212.43 us = 0.11% latency, 55.6 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 71.05 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 71.05 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.6 us = 0.05% latency, 127.31 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.6 us = 0.05% latency, 127.31 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.68 us = 0.04% latency, 137.83 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.68 us = 0.04% latency, 137.83 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 214.34 us = 0.11% latency, 55.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 75.82 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.87 ms = 0.98% latency, 50.99 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 566.24 us = 0.3% latency, 56.06 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 561.95 us = 0.3% latency, 56.49 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 477.55 us = 0.25% latency, 66.47 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 67 us = 0.04% latency, 57.84 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 198.36 us = 0.1% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 191.69 us = 0.1% latency, 0 FLOPS)
          )
          (9): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.65 ms = 2.98% latency, 25.31 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.14 ms = 1.65% latency, 15.25 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 689.74 us = 0.36% latency, 17.16 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 222.21 us = 0.12% latency, 53.15 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 82.02 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 82.02 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.27 us = 0.05% latency, 125.01 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.27 us = 0.05% latency, 125.01 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.92 us = 0.04% latency, 137.44 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.92 us = 0.04% latency, 137.44 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 687.12 us = 0.36% latency, 17.22 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 217.91 us = 0.11% latency, 54.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 71.76 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 71.76 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.36 us = 0.05% latency, 127.65 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.36 us = 0.05% latency, 127.65 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.92 us = 0.04% latency, 137.44 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.92 us = 0.04% latency, 137.44 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 666.62 us = 0.35% latency, 17.75 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 217.2 us = 0.11% latency, 54.38 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 71.76 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 71.76 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.6 us = 0.05% latency, 127.31 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.6 us = 0.05% latency, 127.31 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.92 us = 0.04% latency, 137.44 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.92 us = 0.04% latency, 137.44 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 218.87 us = 0.12% latency, 53.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 68.66 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.89 ms = 1% latency, 50.29 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 578.17 us = 0.3% latency, 54.9 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 569.11 us = 0.3% latency, 55.78 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 479.7 us = 0.25% latency, 66.17 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 67.71 us = 0.04% latency, 57.23 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 230.31 us = 0.12% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 190.73 us = 0.1% latency, 0 FLOPS)
          )
          (10): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.59 ms = 2.94% latency, 25.58 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.11 ms = 1.64% latency, 15.38 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 682.12 us = 0.36% latency, 17.35 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 221.73 us = 0.12% latency, 53.27 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 74.39 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 74.39 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.31 us = 0.05% latency, 126.31 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.31 us = 0.05% latency, 126.31 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.4 us = 0.04% latency, 136.66 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.4 us = 0.04% latency, 136.66 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 666.86 us = 0.35% latency, 17.75 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 218.15 us = 0.11% latency, 54.14 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 71.53 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 71.53 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.88 us = 0.05% latency, 128.32 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.88 us = 0.05% latency, 128.32 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.73 us = 0.04% latency, 139.42 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.73 us = 0.04% latency, 139.42 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 663.76 us = 0.35% latency, 17.83 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 217.68 us = 0.11% latency, 54.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.12 us = 0.05% latency, 127.99 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.12 us = 0.05% latency, 127.99 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.21 us = 0.04% latency, 138.62 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.21 us = 0.04% latency, 138.62 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 219.11 us = 0.12% latency, 53.91 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 68.43 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.89 ms = 1% latency, 50.39 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 578.88 us = 0.3% latency, 54.83 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 568.87 us = 0.3% latency, 55.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 478.98 us = 0.25% latency, 66.27 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 68.19 us = 0.04% latency, 56.83 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 197.17 us = 0.1% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 190.97 us = 0.1% latency, 0 FLOPS)
          )
          (11): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.63 ms = 2.96% latency, 25.41 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.13 ms = 1.65% latency, 15.27 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 682.35 us = 0.36% latency, 17.34 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 220.54 us = 0.12% latency, 53.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 74.15 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 74.15 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.27 us = 0.05% latency, 125.01 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.27 us = 0.05% latency, 125.01 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 85.35 us = 0.04% latency, 135.14 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 85.35 us = 0.04% latency, 135.14 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 671.39 us = 0.35% latency, 17.63 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 219.35 us = 0.12% latency, 53.85 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.08 us = 0.05% latency, 126.65 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.08 us = 0.05% latency, 126.65 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.68 us = 0.04% latency, 137.83 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.68 us = 0.04% latency, 137.83 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 667.57 us = 0.35% latency, 17.73 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 218.15 us = 0.11% latency, 54.14 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 71.05 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 71.05 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.12 us = 0.05% latency, 127.99 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.12 us = 0.05% latency, 127.99 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.64 us = 0.04% latency, 136.28 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.64 us = 0.04% latency, 136.28 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 219.58 us = 0.12% latency, 53.79 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 67.95 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.9 ms = 1% latency, 49.99 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 576.26 us = 0.3% latency, 55.08 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 567.67 us = 0.3% latency, 55.92 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 480.65 us = 0.25% latency, 66.04 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 67.47 us = 0.04% latency, 57.43 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 197.17 us = 0.1% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 191.45 us = 0.1% latency, 0 FLOPS)
          )
          (12): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.61 ms = 2.95% latency, 25.51 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.13 ms = 1.65% latency, 15.3 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 677.35 us = 0.36% latency, 17.47 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 221.49 us = 0.12% latency, 53.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.79 us = 0.05% latency, 125.66 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.79 us = 0.05% latency, 125.66 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.16 us = 0.04% latency, 137.05 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.16 us = 0.04% latency, 137.05 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 663.04 us = 0.35% latency, 17.85 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 216.96 us = 0.11% latency, 54.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.41 us = 0.05% latency, 129.01 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.41 us = 0.05% latency, 129.01 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.49 us = 0.04% latency, 139.82 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.49 us = 0.04% latency, 139.82 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 662.09 us = 0.35% latency, 17.87 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 217.91 us = 0.11% latency, 54.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.33 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.33 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.65 us = 0.05% latency, 128.67 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.65 us = 0.05% latency, 128.67 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.49 us = 0.04% latency, 139.82 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.49 us = 0.04% latency, 139.82 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 225.31 us = 0.12% latency, 52.42 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 67.71 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.88 ms = 0.99% latency, 50.67 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 572.68 us = 0.3% latency, 55.43 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 567.67 us = 0.3% latency, 55.92 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 475.17 us = 0.25% latency, 66.8 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 67.47 us = 0.04% latency, 57.43 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 197.41 us = 0.1% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 201.23 us = 0.11% latency, 0 FLOPS)
          )
          (13): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.58 ms = 2.94% latency, 25.66 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.11 ms = 1.64% latency, 15.4 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 685.69 us = 0.36% latency, 17.26 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 218.87 us = 0.12% latency, 53.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 83.45 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 83.45 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.55 us = 0.05% latency, 125.99 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.55 us = 0.05% latency, 125.99 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.16 us = 0.04% latency, 137.05 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.16 us = 0.04% latency, 137.05 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 674.49 us = 0.36% latency, 17.55 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 216.25 us = 0.11% latency, 54.62 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 78.92 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 78.92 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.79 us = 0.05% latency, 125.66 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.79 us = 0.05% latency, 125.66 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.25 us = 0.04% latency, 140.23 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.25 us = 0.04% latency, 140.23 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 658.27 us = 0.35% latency, 17.98 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 214.58 us = 0.11% latency, 55.04 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.1 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.1 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.65 us = 0.05% latency, 128.67 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.65 us = 0.05% latency, 128.67 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.21 us = 0.04% latency, 138.62 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.21 us = 0.04% latency, 138.62 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 216.25 us = 0.11% latency, 54.62 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 68.66 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.88 ms = 0.99% latency, 50.68 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 570.06 us = 0.3% latency, 55.68 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 566.01 us = 0.3% latency, 56.08 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 475.17 us = 0.25% latency, 66.8 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 67.23 us = 0.04% latency, 57.63 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 196.7 us = 0.1% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 190.02 us = 0.1% latency, 0 FLOPS)
          )
          (14): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.6 ms = 2.95% latency, 25.57 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.12 ms = 1.64% latency, 15.34 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 688.55 us = 0.36% latency, 17.19 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 219.82 us = 0.12% latency, 53.73 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 74.15 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 74.15 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 93.94 us = 0.05% latency, 122.79 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 93.94 us = 0.05% latency, 122.79 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.64 us = 0.04% latency, 136.28 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.64 us = 0.04% latency, 136.28 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 662.8 us = 0.35% latency, 17.85 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 214.1 us = 0.11% latency, 55.17 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 71.05 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 71.05 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.12 us = 0.05% latency, 127.99 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.12 us = 0.05% latency, 127.99 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.21 us = 0.04% latency, 138.62 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.21 us = 0.04% latency, 138.62 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 665.43 us = 0.35% latency, 17.78 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 215.29 us = 0.11% latency, 54.86 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.81 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.81 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.84 us = 0.05% latency, 126.98 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.84 us = 0.05% latency, 126.98 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.68 us = 0.04% latency, 137.83 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.68 us = 0.04% latency, 137.83 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 215.77 us = 0.11% latency, 54.74 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 68.43 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.88 ms = 0.99% latency, 50.75 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 571.49 us = 0.3% latency, 55.54 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 566.72 us = 0.3% latency, 56.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 475.41 us = 0.25% latency, 66.77 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 68.19 us = 0.04% latency, 56.83 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 198.13 us = 0.1% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 200.75 us = 0.11% latency, 0 FLOPS)
          )
          (15): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.59 ms = 2.94% latency, 25.59 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.11 ms = 1.64% latency, 15.4 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 674.01 us = 0.35% latency, 17.56 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 216.96 us = 0.11% latency, 54.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.19 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.19 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.03 us = 0.05% latency, 125.33 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.03 us = 0.05% latency, 125.33 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.4 us = 0.04% latency, 136.66 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.4 us = 0.04% latency, 136.66 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 664.23 us = 0.35% latency, 17.82 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 215.77 us = 0.11% latency, 54.74 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 69.86 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 69.86 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.6 us = 0.05% latency, 127.31 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.6 us = 0.05% latency, 127.31 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.97 us = 0.04% latency, 139.02 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.97 us = 0.04% latency, 139.02 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 684.02 us = 0.36% latency, 17.3 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 234.37 us = 0.12% latency, 50.4 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.81 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.81 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.65 us = 0.05% latency, 128.67 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.65 us = 0.05% latency, 128.67 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.21 us = 0.04% latency, 138.62 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.21 us = 0.04% latency, 138.62 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 217.44 us = 0.11% latency, 54.32 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 68.19 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.89 ms = 1% latency, 50.39 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 570.54 us = 0.3% latency, 55.64 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 569.11 us = 0.3% latency, 55.78 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 480.18 us = 0.25% latency, 66.11 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 68.19 us = 0.04% latency, 56.83 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 195.26 us = 0.1% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 191.21 us = 0.1% latency, 0 FLOPS)
          )
          (16): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.6 ms = 2.95% latency, 25.57 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.1 ms = 1.63% latency, 15.41 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 682.12 us = 0.36% latency, 17.35 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 222.92 us = 0.12% latency, 52.98 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 74.39 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 74.39 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.55 us = 0.05% latency, 125.99 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.55 us = 0.05% latency, 125.99 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.21 us = 0.04% latency, 138.62 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.21 us = 0.04% latency, 138.62 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 666.38 us = 0.35% latency, 17.76 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 218.15 us = 0.11% latency, 54.14 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.81 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.81 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.6 us = 0.05% latency, 127.31 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.6 us = 0.05% latency, 127.31 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.45 us = 0.04% latency, 138.22 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.45 us = 0.04% latency, 138.22 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 666.14 us = 0.35% latency, 17.77 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 217.2 us = 0.11% latency, 54.38 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.12 us = 0.05% latency, 127.99 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.12 us = 0.05% latency, 127.99 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.16 us = 0.04% latency, 137.05 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.16 us = 0.04% latency, 137.05 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 218.87 us = 0.12% latency, 53.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 68.66 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.89 ms = 0.99% latency, 50.46 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 576.5 us = 0.3% latency, 55.06 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 569.34 us = 0.3% latency, 55.75 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 478.27 us = 0.25% latency, 66.37 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 68.19 us = 0.04% latency, 56.83 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 209.81 us = 0.11% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 192.17 us = 0.1% latency, 0 FLOPS)
          )
          (17): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.61 ms = 2.95% latency, 25.52 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.13 ms = 1.65% latency, 15.26 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 679.73 us = 0.36% latency, 17.41 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 221.49 us = 0.12% latency, 53.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.03 us = 0.05% latency, 125.33 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.03 us = 0.05% latency, 125.33 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.4 us = 0.04% latency, 136.66 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.4 us = 0.04% latency, 136.66 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 669.24 us = 0.35% latency, 17.68 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 217.68 us = 0.11% latency, 54.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 69.86 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 69.86 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.6 us = 0.05% latency, 127.31 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.6 us = 0.05% latency, 127.31 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.88 us = 0.04% latency, 135.89 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.88 us = 0.04% latency, 135.89 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 687.36 us = 0.36% latency, 17.22 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 216.72 us = 0.11% latency, 54.5 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 110.15 us = 0.06% latency, 104.72 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 110.15 us = 0.06% latency, 104.72 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.64 us = 0.04% latency, 136.28 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.64 us = 0.04% latency, 136.28 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 219.58 us = 0.12% latency, 53.79 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 68.19 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.88 ms = 0.99% latency, 50.63 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 573.64 us = 0.3% latency, 55.34 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 567.91 us = 0.3% latency, 55.89 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 477.55 us = 0.25% latency, 66.47 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 67.95 us = 0.04% latency, 57.03 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 195.74 us = 0.1% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 193.12 us = 0.1% latency, 0 FLOPS)
          )
          (18): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.59 ms = 2.94% latency, 25.6 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.11 ms = 1.64% latency, 15.38 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 683.07 us = 0.36% latency, 17.33 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 221.97 us = 0.12% latency, 53.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.91 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.91 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.51 us = 0.05% latency, 124.69 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.51 us = 0.05% latency, 124.69 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.4 us = 0.04% latency, 136.66 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.4 us = 0.04% latency, 136.66 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 666.14 us = 0.35% latency, 17.77 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 217.44 us = 0.11% latency, 54.32 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.12 us = 0.05% latency, 127.99 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.12 us = 0.05% latency, 127.99 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.16 us = 0.04% latency, 137.05 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.16 us = 0.04% latency, 137.05 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 665.43 us = 0.35% latency, 17.78 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 217.44 us = 0.11% latency, 54.32 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.1 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.1 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.12 us = 0.05% latency, 127.99 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.12 us = 0.05% latency, 127.99 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.45 us = 0.04% latency, 138.22 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.45 us = 0.04% latency, 138.22 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 219.82 us = 0.12% latency, 53.73 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 66.76 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.89 ms = 1% latency, 50.31 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 573.16 us = 0.3% latency, 55.38 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 571.25 us = 0.3% latency, 55.57 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 478.03 us = 0.25% latency, 66.4 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 73.91 us = 0.04% latency, 52.43 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 195.74 us = 0.1% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 190.02 us = 0.1% latency, 0 FLOPS)
          )
          (19): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.58 ms = 2.94% latency, 25.62 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.11 ms = 1.64% latency, 15.36 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 678.3 us = 0.36% latency, 17.45 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 220.78 us = 0.12% latency, 53.5 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.84 us = 0.05% latency, 126.98 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.84 us = 0.05% latency, 126.98 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.92 us = 0.04% latency, 137.44 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.92 us = 0.04% latency, 137.44 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 667.1 us = 0.35% latency, 17.74 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 217.68 us = 0.11% latency, 54.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.33 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.33 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.08 us = 0.05% latency, 126.65 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.08 us = 0.05% latency, 126.65 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.16 us = 0.04% latency, 137.05 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.16 us = 0.04% latency, 137.05 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 665.66 us = 0.35% latency, 17.78 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 217.91 us = 0.11% latency, 54.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.88 us = 0.05% latency, 128.32 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.88 us = 0.05% latency, 128.32 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.45 us = 0.04% latency, 138.22 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.45 us = 0.04% latency, 138.22 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 218.87 us = 0.12% latency, 53.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 84.16 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.88 ms = 0.99% latency, 50.53 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 572.44 us = 0.3% latency, 55.45 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 570.77 us = 0.3% latency, 55.61 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 478.27 us = 0.25% latency, 66.37 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 67.23 us = 0.04% latency, 57.63 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 194.79 us = 0.1% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 191.21 us = 0.1% latency, 0 FLOPS)
          )
          (20): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.64 ms = 2.97% latency, 25.37 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.12 ms = 1.64% latency, 15.33 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 685.93 us = 0.36% latency, 17.25 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 221.49 us = 0.12% latency, 53.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.43 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.43 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 99.42 us = 0.05% latency, 116.02 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 99.42 us = 0.05% latency, 116.02 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.16 us = 0.04% latency, 137.05 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.16 us = 0.04% latency, 137.05 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 676.63 us = 0.36% latency, 17.49 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 229.12 us = 0.12% latency, 51.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.88 us = 0.05% latency, 128.32 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.88 us = 0.05% latency, 128.32 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.97 us = 0.04% latency, 139.02 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.97 us = 0.04% latency, 139.02 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 664.71 us = 0.35% latency, 17.8 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 217.44 us = 0.11% latency, 54.32 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.81 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.81 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.41 us = 0.05% latency, 129.01 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.41 us = 0.05% latency, 129.01 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.68 us = 0.04% latency, 137.83 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.68 us = 0.04% latency, 137.83 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 219.58 us = 0.12% latency, 53.79 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 68.66 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.91 ms = 1% latency, 49.97 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 573.64 us = 0.3% latency, 55.34 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 569.11 us = 0.3% latency, 55.78 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 481.13 us = 0.25% latency, 65.98 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 67.23 us = 0.04% latency, 57.63 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 194.07 us = 0.1% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 191.69 us = 0.1% latency, 0 FLOPS)
          )
          (21): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.58 ms = 2.94% latency, 25.62 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.11 ms = 1.64% latency, 15.4 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 681.4 us = 0.36% latency, 17.37 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 221.97 us = 0.12% latency, 53.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.48 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.48 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 93.22 us = 0.05% latency, 123.73 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 93.22 us = 0.05% latency, 123.73 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 85.59 us = 0.05% latency, 134.76 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 85.59 us = 0.05% latency, 134.76 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 666.14 us = 0.35% latency, 17.77 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 217.68 us = 0.11% latency, 54.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.08 us = 0.05% latency, 126.65 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.08 us = 0.05% latency, 126.65 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.45 us = 0.04% latency, 138.22 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.45 us = 0.04% latency, 138.22 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 666.62 us = 0.35% latency, 17.75 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 217.68 us = 0.11% latency, 54.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.81 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.81 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.6 us = 0.05% latency, 127.31 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.6 us = 0.05% latency, 127.31 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.68 us = 0.04% latency, 137.83 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.68 us = 0.04% latency, 137.83 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 218.39 us = 0.12% latency, 54.08 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 67.71 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.89 ms = 0.99% latency, 50.48 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 574.83 us = 0.3% latency, 55.22 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 568.63 us = 0.3% latency, 55.82 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 478.98 us = 0.25% latency, 66.27 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 68.43 us = 0.04% latency, 56.63 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 198.6 us = 0.1% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 189.78 us = 0.1% latency, 0 FLOPS)
          )
          (22): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.61 ms = 2.95% latency, 25.52 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.12 ms = 1.64% latency, 15.32 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 680.69 us = 0.36% latency, 17.39 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 222.21 us = 0.12% latency, 53.15 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.19 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.19 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.79 us = 0.05% latency, 125.66 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.79 us = 0.05% latency, 125.66 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.16 us = 0.04% latency, 137.05 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.16 us = 0.04% latency, 137.05 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 688.31 us = 0.36% latency, 17.19 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 237.46 us = 0.13% latency, 49.74 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.84 us = 0.05% latency, 126.98 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.84 us = 0.05% latency, 126.98 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.16 us = 0.04% latency, 137.05 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.16 us = 0.04% latency, 137.05 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 664.47 us = 0.35% latency, 17.81 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 217.91 us = 0.11% latency, 54.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 71.29 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 71.29 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.88 us = 0.05% latency, 128.32 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.88 us = 0.05% latency, 128.32 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.97 us = 0.04% latency, 139.02 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.97 us = 0.04% latency, 139.02 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 219.11 us = 0.12% latency, 53.91 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 68.19 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.89 ms = 0.99% latency, 50.42 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 572.92 us = 0.3% latency, 55.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 568.15 us = 0.3% latency, 55.87 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 479.46 us = 0.25% latency, 66.2 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 67.71 us = 0.04% latency, 57.23 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 205.99 us = 0.11% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 189.3 us = 0.1% latency, 0 FLOPS)
          )
          (23): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.57 ms = 2.93% latency, 25.67 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.1 ms = 1.63% latency, 15.44 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 680.45 us = 0.36% latency, 17.39 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 220.3 us = 0.12% latency, 53.61 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.91 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.91 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.03 us = 0.05% latency, 125.33 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.03 us = 0.05% latency, 125.33 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.16 us = 0.04% latency, 137.05 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.16 us = 0.04% latency, 137.05 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 669.72 us = 0.35% latency, 17.67 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 218.87 us = 0.12% latency, 53.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.33 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.33 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.31 us = 0.05% latency, 126.31 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.31 us = 0.05% latency, 126.31 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.64 us = 0.04% latency, 136.28 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.64 us = 0.04% latency, 136.28 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 664.47 us = 0.35% latency, 17.81 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 218.39 us = 0.12% latency, 54.08 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 71.05 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 71.05 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.88 us = 0.05% latency, 128.32 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.88 us = 0.05% latency, 128.32 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.97 us = 0.04% latency, 139.02 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.97 us = 0.04% latency, 139.02 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 219.11 us = 0.12% latency, 53.91 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 67.95 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.89 ms = 0.99% latency, 50.48 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 575.54 us = 0.3% latency, 55.15 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 568.63 us = 0.3% latency, 55.82 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 478.74 us = 0.25% latency, 66.3 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 67.47 us = 0.04% latency, 57.43 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 195.5 us = 0.1% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 191.93 us = 0.1% latency, 0 FLOPS)
          )
          (24): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.58 ms = 2.94% latency, 25.65 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.1 ms = 1.63% latency, 15.44 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 675.92 us = 0.36% latency, 17.51 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 220.54 us = 0.12% latency, 53.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.08 us = 0.05% latency, 126.65 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.08 us = 0.05% latency, 126.65 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.92 us = 0.04% latency, 137.44 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.92 us = 0.04% latency, 137.44 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 676.63 us = 0.36% latency, 17.49 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 218.15 us = 0.11% latency, 54.14 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 71.05 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 71.05 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 98.47 us = 0.05% latency, 117.14 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 98.47 us = 0.05% latency, 117.14 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.64 us = 0.04% latency, 136.28 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.64 us = 0.04% latency, 136.28 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 665.43 us = 0.35% latency, 17.78 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 218.15 us = 0.11% latency, 54.14 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.1 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.1 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.36 us = 0.05% latency, 127.65 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.36 us = 0.05% latency, 127.65 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.92 us = 0.04% latency, 137.44 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.92 us = 0.04% latency, 137.44 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 218.87 us = 0.12% latency, 53.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 67.47 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.89 ms = 0.99% latency, 50.42 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 571.97 us = 0.3% latency, 55.5 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 570.77 us = 0.3% latency, 55.61 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 478.98 us = 0.25% latency, 66.27 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 67.71 us = 0.04% latency, 57.23 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 196.46 us = 0.1% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 190.26 us = 0.1% latency, 0 FLOPS)
          )
          (25): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.58 ms = 2.94% latency, 25.62 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.09 ms = 1.63% latency, 15.46 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 678.54 us = 0.36% latency, 17.44 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 220.54 us = 0.12% latency, 53.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.19 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.19 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.74 us = 0.05% latency, 124.37 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.74 us = 0.05% latency, 124.37 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.21 us = 0.04% latency, 138.62 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.21 us = 0.04% latency, 138.62 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 672.1 us = 0.35% latency, 17.61 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 217.68 us = 0.11% latency, 54.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 80.35 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 80.35 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.88 us = 0.05% latency, 128.32 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.88 us = 0.05% latency, 128.32 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.97 us = 0.04% latency, 139.02 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.97 us = 0.04% latency, 139.02 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 663.52 us = 0.35% latency, 17.84 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 217.68 us = 0.11% latency, 54.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.88 us = 0.05% latency, 128.32 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.88 us = 0.05% latency, 128.32 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.49 us = 0.04% latency, 139.82 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.49 us = 0.04% latency, 139.82 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 219.35 us = 0.12% latency, 53.85 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 67.23 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.9 ms = 1% latency, 50.07 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 575.3 us = 0.3% latency, 55.18 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 568.87 us = 0.3% latency, 55.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 477.55 us = 0.25% latency, 66.47 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 67 us = 0.04% latency, 57.84 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 196.46 us = 0.1% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 189.78 us = 0.1% latency, 0 FLOPS)
          )
          (26): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.56 ms = 2.93% latency, 25.72 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.09 ms = 1.63% latency, 15.47 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 678.3 us = 0.36% latency, 17.45 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 220.78 us = 0.12% latency, 53.5 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.48 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.48 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.74 us = 0.05% latency, 124.37 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.74 us = 0.05% latency, 124.37 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.68 us = 0.04% latency, 137.83 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.68 us = 0.04% latency, 137.83 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 667.1 us = 0.35% latency, 17.74 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 218.15 us = 0.11% latency, 54.14 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 69.62 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 69.62 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.36 us = 0.05% latency, 127.65 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.36 us = 0.05% latency, 127.65 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.21 us = 0.04% latency, 138.62 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.21 us = 0.04% latency, 138.62 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 666.86 us = 0.35% latency, 17.75 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 218.15 us = 0.11% latency, 54.14 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.81 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.81 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.6 us = 0.05% latency, 127.31 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.6 us = 0.05% latency, 127.31 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.73 us = 0.04% latency, 139.42 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.73 us = 0.04% latency, 139.42 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 219.35 us = 0.12% latency, 53.85 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 67.71 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.88 ms = 0.99% latency, 50.54 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 575.54 us = 0.3% latency, 55.15 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 567.2 us = 0.3% latency, 55.96 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 478.98 us = 0.25% latency, 66.27 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 67.95 us = 0.04% latency, 57.03 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 195.26 us = 0.1% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 189.78 us = 0.1% latency, 0 FLOPS)
          )
          (27): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.6 ms = 2.95% latency, 25.53 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.1 ms = 1.63% latency, 15.41 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 679.02 us = 0.36% latency, 17.43 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 221.49 us = 0.12% latency, 53.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.43 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.43 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.03 us = 0.05% latency, 125.33 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.03 us = 0.05% latency, 125.33 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.92 us = 0.04% latency, 137.44 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.92 us = 0.04% latency, 137.44 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 678.3 us = 0.36% latency, 17.45 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 218.63 us = 0.12% latency, 54.02 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 78.92 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 78.92 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.08 us = 0.05% latency, 126.65 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.08 us = 0.05% latency, 126.65 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.92 us = 0.04% latency, 137.44 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.92 us = 0.04% latency, 137.44 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 664.47 us = 0.35% latency, 17.81 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 218.39 us = 0.12% latency, 54.08 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.33 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.33 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.36 us = 0.05% latency, 127.65 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.36 us = 0.05% latency, 127.65 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.49 us = 0.04% latency, 139.82 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.49 us = 0.04% latency, 139.82 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 218.87 us = 0.12% latency, 53.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 67.71 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.89 ms = 1% latency, 50.32 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 573.87 us = 0.3% latency, 55.31 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 573.87 us = 0.3% latency, 55.31 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 478.27 us = 0.25% latency, 66.37 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 67.95 us = 0.04% latency, 57.03 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 216.96 us = 0.11% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 188.59 us = 0.1% latency, 0 FLOPS)
          )
          (28): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.57 ms = 2.94% latency, 25.67 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.1 ms = 1.63% latency, 15.43 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 679.49 us = 0.36% latency, 17.42 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 221.25 us = 0.12% latency, 53.38 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.27 us = 0.05% latency, 125.01 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.27 us = 0.05% latency, 125.01 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.16 us = 0.04% latency, 137.05 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.16 us = 0.04% latency, 137.05 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 662.57 us = 0.35% latency, 17.86 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 217.2 us = 0.11% latency, 54.38 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.1 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.1 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.12 us = 0.05% latency, 127.99 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.12 us = 0.05% latency, 127.99 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.45 us = 0.04% latency, 138.22 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.45 us = 0.04% latency, 138.22 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 680.69 us = 0.36% latency, 17.39 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 216.96 us = 0.11% latency, 54.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.65 us = 0.05% latency, 128.67 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.65 us = 0.05% latency, 128.67 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 98.23 us = 0.05% latency, 117.42 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 98.23 us = 0.05% latency, 117.42 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 218.87 us = 0.12% latency, 53.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 67 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.89 ms = 0.99% latency, 50.51 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 574.11 us = 0.3% latency, 55.29 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 569.11 us = 0.3% latency, 55.78 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 478.98 us = 0.25% latency, 66.27 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 67.47 us = 0.04% latency, 57.43 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 197.89 us = 0.1% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 189.78 us = 0.1% latency, 0 FLOPS)
          )
          (29): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.67 ms = 2.99% latency, 25.22 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.18 ms = 1.67% latency, 15.05 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 694.75 us = 0.37% latency, 17.03 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 221.97 us = 0.12% latency, 53.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 75.82 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 75.82 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 93.22 us = 0.05% latency, 123.73 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 93.22 us = 0.05% latency, 123.73 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.21 us = 0.04% latency, 138.62 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.21 us = 0.04% latency, 138.62 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 729.32 us = 0.38% latency, 16.23 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 227.69 us = 0.12% latency, 51.87 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 82.97 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 82.97 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 108.48 us = 0.06% latency, 106.33 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 108.48 us = 0.06% latency, 106.33 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.73 us = 0.04% latency, 139.42 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.73 us = 0.04% latency, 139.42 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 661.85 us = 0.35% latency, 17.88 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 217.68 us = 0.11% latency, 54.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.33 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.33 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.88 us = 0.05% latency, 128.32 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.88 us = 0.05% latency, 128.32 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.49 us = 0.04% latency, 139.82 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.49 us = 0.04% latency, 139.82 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 219.82 us = 0.12% latency, 53.73 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 67.95 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.9 ms = 1% latency, 50 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 571.49 us = 0.3% latency, 55.54 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 585.32 us = 0.31% latency, 54.23 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 480.18 us = 0.25% latency, 66.11 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 66.76 us = 0.04% latency, 58.04 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 198.13 us = 0.1% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 190.97 us = 0.1% latency, 0 FLOPS)
          )
          (30): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.62 ms = 2.96% latency, 25.46 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.15 ms = 1.66% latency, 15.2 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 677.59 us = 0.36% latency, 17.47 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 220.78 us = 0.12% latency, 53.5 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.19 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.19 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.03 us = 0.05% latency, 125.33 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 92.03 us = 0.05% latency, 125.33 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.21 us = 0.04% latency, 138.62 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.21 us = 0.04% latency, 138.62 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 667.1 us = 0.35% latency, 17.74 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 216.72 us = 0.11% latency, 54.5 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.1 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.1 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.55 us = 0.05% latency, 125.99 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 91.55 us = 0.05% latency, 125.99 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.97 us = 0.04% latency, 139.02 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 82.97 us = 0.04% latency, 139.02 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 663.04 us = 0.35% latency, 17.85 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 216.25 us = 0.11% latency, 54.62 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.57 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.88 us = 0.05% latency, 128.32 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.88 us = 0.05% latency, 128.32 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.68 us = 0.04% latency, 137.83 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.68 us = 0.04% latency, 137.83 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 221.73 us = 0.12% latency, 53.27 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 67.47 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.88 ms = 0.99% latency, 50.61 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 575.3 us = 0.3% latency, 55.18 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 566.72 us = 0.3% latency, 56.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 476.6 us = 0.25% latency, 66.6 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 67.95 us = 0.04% latency, 57.03 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 198.36 us = 0.1% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 191.45 us = 0.1% latency, 0 FLOPS)
          )
          (31): LlamaDecoderLayer(
            202.48 M = 3% Params, 71.52 GMACs = 3.06% MACs, 5.58 ms = 2.94% latency, 25.65 TFLOPS
            (self_attn): LlamaAttention(
              67.21 M = 1% Params, 23.91 GMACs = 1.02% MACs, 3.1 ms = 1.63% latency, 15.44 TFLOPS
              (q_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 684.5 us = 0.36% latency, 17.29 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 220.3 us = 0.12% latency, 53.61 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 97.99 us = 0.05% latency, 117.71 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 97.99 us = 0.05% latency, 117.71 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.64 us = 0.04% latency, 136.28 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 84.64 us = 0.04% latency, 136.28 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 665.19 us = 0.35% latency, 17.79 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 217.2 us = 0.11% latency, 54.38 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 71.76 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 71.76 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.41 us = 0.05% latency, 129.01 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 89.41 us = 0.05% latency, 129.01 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.68 us = 0.04% latency, 137.83 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.68 us = 0.04% latency, 137.83 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                16.81 M = 0.25% Params, 5.92 GMACs = 0.25% MACs, 663.28 us = 0.35% latency, 17.84 TFLOPS
                (base_layer): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 216.25 us = 0.11% latency, 54.62 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 70.81 us = 0.04% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 70.81 us = 0.04% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.12 us = 0.05% latency, 127.99 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 90.12 us = 0.05% latency, 127.99 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.68 us = 0.04% latency, 137.83 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 5.77 MMACs = 0% MACs, 83.68 us = 0.04% latency, 137.83 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.25% Params, 5.91 GMACs = 0.25% MACs, 218.87 us = 0.12% latency, 53.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 67.95 us = 0.04% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              135.27 M = 2.01% Params, 47.61 GMACs = 2.04% MACs, 1.89 ms = 1% latency, 50.38 TFLOPS
              (gate_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 576.73 us = 0.3% latency, 55.04 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 570.54 us = 0.3% latency, 55.64 TFLOPS, in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(45.09 M = 0.67% Params, 15.87 GMACs = 0.68% MACs, 478.03 us = 0.25% latency, 66.4 TFLOPS, in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLUActivation(0 = 0% Params, 0 MACs = 0% MACs, 67.71 us = 0.04% latency, 57.23 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 197.41 us = 0.1% latency, 0 FLOPS)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 190.5 us = 0.1% latency, 0 FLOPS)
          )
        )
        (norm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 223.16 us = 0.12% latency, 0 FLOPS)
      )
      (lm_head): Linear(131.07 M = 1.94% Params, 46.14 GMACs = 1.98% MACs, 1.66 ms = 0.88% latency, 55.46 TFLOPS, in_features=4096, out_features=32000, bias=False)
    )
  )
)
------------------------------------------------------------------------------
