
-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 2:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

world size:                                                             2       
data parallel size:                                                     2       
model parallel size:                                                    1       
batch size per GPU:                                                     10      
params per GPU:                                                         8.03 B  
params of model = params per GPU * mp_size:                             8.03 B  
fwd MACs per GPU:                                                       15.94 TMACs
fwd flops per GPU:                                                      31.88 T 
fwd flops of model = fwd flops per GPU * mp_size:                       31.88 T 
fwd latency:                                                            393.26 ms
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    81.07 TFLOPS
bwd latency:                                                            1.87 s  
bwd FLOPS per GPU = 2 * fwd flops per GPU / bwd latency:                34.18 TFLOPS
fwd+bwd FLOPS per GPU = 3 * fwd flops per GPU / (fwd+bwd latency):      42.35 TFLOPS
step latency:                                                           45.35 ms
iter latency:                                                           2.3 s   
FLOPS per GPU = 3 * fwd flops per GPU / iter latency:                   41.51 TFLOPS
samples/second:                                                         8.68    

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'PeftModelForCausalLM': '8.03 B'}
    MACs        - {'PeftModelForCausalLM': '15.94 TMACs'}
    fwd latency - {'PeftModelForCausalLM': '381.88 ms'}
depth 1:
    params      - {'LoraModel': '8.03 B'}
    MACs        - {'LoraModel': '15.94 TMACs'}
    fwd latency - {'LoraModel': '381.88 ms'}
depth 2:
    params      - {'LlamaForCausalLM': '8.03 B'}
    MACs        - {'LlamaForCausalLM': '15.94 TMACs'}
    fwd latency - {'LlamaForCausalLM': '381.88 ms'}
depth 3:
    params      - {'LlamaModel': '7.51 B'}
    MACs        - {'LlamaModel': '14.89 TMACs'}
    fwd latency - {'LlamaModel': '365.86 ms'}
depth 4:
    params      - {'ModuleList': '6.98 B'}
    MACs        - {'ModuleList': '14.89 TMACs'}
    fwd latency - {'ModuleList': '359.24 ms'}
depth 5:
    params      - {'LlamaDecoderLayer': '6.98 B'}
    MACs        - {'LlamaDecoderLayer': '14.89 TMACs'}
    fwd latency - {'LlamaDecoderLayer': '359.24 ms'}
depth 6:
    params      - {'LlamaMLP': '5.64 B'}
    MACs        - {'LlamaMLP': '11.33 TMACs'}
    fwd latency - {'LlamaMLP': '207.41 ms'}
depth 7:
    params      - {'Linear': '6.98 B'}
    MACs        - {'Linear': '14.04 TMACs'}
    fwd latency - {'Linear': '281.26 ms'}
depth 8:
    params      - {'Linear': '805.31 M'}
    MACs        - {'Linear': '1.62 TMACs'}
    fwd latency - {'Linear': '33.67 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

PeftModelForCausalLM(
  8.03 B = 100% Params, 15.94 TMACs = 100% MACs, 381.88 ms = 100% latency, 83.49 TFLOPS
  (base_model): LoraModel(
    8.03 B = 100% Params, 15.94 TMACs = 100% MACs, 381.88 ms = 100% latency, 83.49 TFLOPS
    (model): LlamaForCausalLM(
      8.03 B = 100% Params, 15.94 TMACs = 100% MACs, 381.88 ms = 100% latency, 83.49 TFLOPS
      (model): LlamaModel(
        7.51 B = 93.46% Params, 14.89 TMACs = 93.38% MACs, 365.86 ms = 95.8% latency, 81.37 TFLOPS
        (embed_tokens): Embedding(525.34 M = 6.54% Params, 0 MACs = 0% MACs, 159.03 us = 0.04% latency, 0 FLOPS, 128256, 4096)
        (layers): ModuleList(
          (0): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 10.95 ms = 2.87% latency, 84.95 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.54 ms = 0.93% latency, 62.7 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.17 ms = 0.31% latency, 57.74 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 659.47 us = 0.17% latency, 102.27 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 107.05 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 107.05 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 94.41 us = 0.02% latency, 1.4 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 94.41 us = 0.02% latency, 1.4 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 74.86 us = 0.02% latency, 1.76 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 74.86 us = 0.02% latency, 1.76 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 540.26 us = 0.14% latency, 31.51 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 194.55 us = 0.05% latency, 86.67 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 94.41 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 94.41 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.53 us = 0.02% latency, 1.84 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.53 us = 0.02% latency, 1.84 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 54.36 us = 0.01% latency, 605.82 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 54.36 us = 0.01% latency, 605.82 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 528.81 us = 0.14% latency, 32.2 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 184.3 us = 0.05% latency, 91.49 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 94.18 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 94.18 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.76 us = 0.02% latency, 1.84 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.76 us = 0.02% latency, 1.84 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.92 us = 0.01% latency, 687.19 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.92 us = 0.01% latency, 687.19 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 615.36 us = 0.16% latency, 109.6 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.2 ms = 1.62% latency, 114.28 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.89 ms = 0.49% latency, 125 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.87 ms = 0.49% latency, 126.32 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.89 ms = 0.5% latency, 124.79 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 196.7 us = 0.05% latency, 146.5 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 482.56 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 476.36 us = 0.12% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (1): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 10.72 ms = 2.81% latency, 86.79 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.4 ms = 0.89% latency, 65.35 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.06 ms = 0.28% latency, 63.9 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 611.07 us = 0.16% latency, 110.37 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 95.37 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 95.37 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.72 us = 0.02% latency, 1.81 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.72 us = 0.02% latency, 1.81 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 57.46 us = 0.02% latency, 2.29 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 57.46 us = 0.02% latency, 2.29 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 518.56 us = 0.14% latency, 32.83 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 183.11 us = 0.05% latency, 92.08 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 93.94 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 93.94 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.05 us = 0.02% latency, 1.85 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.05 us = 0.02% latency, 1.85 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.68 us = 0.01% latency, 690.63 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.68 us = 0.01% latency, 690.63 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 514.51 us = 0.13% latency, 33.09 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 182.63 us = 0.05% latency, 92.32 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 94.18 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 94.18 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.1 us = 0.02% latency, 1.88 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.1 us = 0.02% latency, 1.88 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.97 us = 0.01% latency, 701.15 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.97 us = 0.01% latency, 701.15 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 619.89 us = 0.16% latency, 108.8 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.12 ms = 1.6% latency, 115.75 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.86 ms = 0.49% latency, 126.95 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.86 ms = 0.49% latency, 126.98 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.87 ms = 0.49% latency, 126.3 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 190.02 us = 0.05% latency, 151.64 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 480.18 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 476.12 us = 0.12% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (2): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11 ms = 2.88% latency, 84.61 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.4 ms = 0.89% latency, 65.34 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.07 ms = 0.28% latency, 63.25 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 613.45 us = 0.16% latency, 109.94 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 94.89 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 94.89 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.48 us = 0.02% latency, 1.82 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.48 us = 0.02% latency, 1.82 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 65.57 us = 0.02% latency, 2.01 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 65.57 us = 0.02% latency, 2.01 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 515.7 us = 0.14% latency, 33.01 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 184.06 us = 0.05% latency, 91.61 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 95.13 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 95.13 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 69.86 us = 0.02% latency, 1.89 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 69.86 us = 0.02% latency, 1.89 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.49 us = 0.01% latency, 708.34 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.49 us = 0.01% latency, 708.34 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 510.93 us = 0.13% latency, 33.32 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 183.11 us = 0.05% latency, 92.08 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 91.79 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 91.79 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 69.86 us = 0.02% latency, 1.89 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 69.86 us = 0.02% latency, 1.89 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.49 us = 0.01% latency, 708.34 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.49 us = 0.01% latency, 708.34 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 615.36 us = 0.16% latency, 109.6 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.38 ms = 1.67% latency, 111.07 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.86 ms = 0.49% latency, 126.92 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 119.23 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2 ms = 0.52% latency, 117.77 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 189.3 us = 0.05% latency, 152.22 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 478.03 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 483.51 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (3): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11.25 ms = 2.95% latency, 82.66 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.55 ms = 0.93% latency, 62.6 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.13 ms = 0.3% latency, 59.98 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 664.95 us = 0.17% latency, 101.43 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 105.38 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 105.38 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 75.34 us = 0.02% latency, 1.75 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 75.34 us = 0.02% latency, 1.75 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.41 us = 0.02% latency, 2.26 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.41 us = 0.02% latency, 2.26 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 530.96 us = 0.14% latency, 32.07 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 197.17 us = 0.05% latency, 85.51 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 93.46 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 93.46 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.81 us = 0.02% latency, 1.86 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.81 us = 0.02% latency, 1.86 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.68 us = 0.01% latency, 690.63 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.68 us = 0.01% latency, 690.63 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 527.14 us = 0.14% latency, 32.3 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 195.03 us = 0.05% latency, 86.46 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 93.94 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 93.94 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.53 us = 0.02% latency, 1.84 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.53 us = 0.02% latency, 1.84 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.21 us = 0.01% latency, 697.61 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.21 us = 0.01% latency, 697.61 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 661.37 us = 0.17% latency, 101.98 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.5 ms = 1.7% latency, 108.9 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.99 ms = 0.52% latency, 118.67 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 119.23 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2 ms = 0.52% latency, 118.04 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 192.4 us = 0.05% latency, 149.77 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 481.84 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 476.6 us = 0.12% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (4): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11.26 ms = 2.95% latency, 82.59 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.56 ms = 0.93% latency, 62.33 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.12 ms = 0.29% latency, 60.58 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 664.47 us = 0.17% latency, 101.5 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 97.04 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 97.04 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 73.67 us = 0.02% latency, 1.79 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 73.67 us = 0.02% latency, 1.79 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.17 us = 0.02% latency, 2.26 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.17 us = 0.02% latency, 2.26 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 546.22 us = 0.14% latency, 31.17 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 196.46 us = 0.05% latency, 85.83 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 104.67 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 104.67 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.72 us = 0.02% latency, 1.81 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.72 us = 0.02% latency, 1.81 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 48.16 us = 0.01% latency, 683.79 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 48.16 us = 0.01% latency, 683.79 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 531.91 us = 0.14% latency, 32.01 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 195.98 us = 0.05% latency, 86.03 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 95.37 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 95.37 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.53 us = 0.02% latency, 1.84 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.53 us = 0.02% latency, 1.84 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.68 us = 0.01% latency, 690.63 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.68 us = 0.01% latency, 690.63 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 666.62 us = 0.17% latency, 101.17 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.5 ms = 1.7% latency, 109.02 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 119.02 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 119.4 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2 ms = 0.52% latency, 117.87 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 190.73 us = 0.05% latency, 151.08 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 481.37 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 477.08 us = 0.12% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (5): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11.26 ms = 2.95% latency, 82.64 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.55 ms = 0.93% latency, 62.51 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.12 ms = 0.29% latency, 60.24 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 667.33 us = 0.17% latency, 101.07 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 97.99 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 97.99 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 73.91 us = 0.02% latency, 1.78 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 73.91 us = 0.02% latency, 1.78 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.65 us = 0.02% latency, 2.25 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.65 us = 0.02% latency, 2.25 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 532.63 us = 0.14% latency, 31.97 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 196.7 us = 0.05% latency, 85.72 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 94.18 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 94.18 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.81 us = 0.02% latency, 1.86 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.81 us = 0.02% latency, 1.86 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.97 us = 0.01% latency, 701.15 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.97 us = 0.01% latency, 701.15 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 532.15 us = 0.14% latency, 31.99 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 195.03 us = 0.05% latency, 86.46 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 94.18 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 94.18 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.96 us = 0.02% latency, 1.81 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.96 us = 0.02% latency, 1.81 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.45 us = 0.01% latency, 694.1 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.45 us = 0.01% latency, 694.1 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 665.19 us = 0.17% latency, 101.39 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.5 ms = 1.7% latency, 108.95 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 119.12 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.99 ms = 0.52% latency, 118.86 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2 ms = 0.52% latency, 118.04 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 191.69 us = 0.05% latency, 150.32 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 482.32 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 478.03 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (6): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11.22 ms = 2.94% latency, 82.95 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.54 ms = 0.93% latency, 62.79 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.12 ms = 0.29% latency, 60.53 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 664.95 us = 0.17% latency, 101.43 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 97.51 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 97.51 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 73.43 us = 0.02% latency, 1.79 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 73.43 us = 0.02% latency, 1.79 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.65 us = 0.02% latency, 2.25 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.65 us = 0.02% latency, 2.25 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 532.15 us = 0.14% latency, 31.99 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 196.22 us = 0.05% latency, 85.93 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 95.61 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 95.61 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.29 us = 0.02% latency, 1.85 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.29 us = 0.02% latency, 1.85 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.21 us = 0.01% latency, 697.61 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.21 us = 0.01% latency, 697.61 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 525.71 us = 0.14% latency, 32.39 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 194.55 us = 0.05% latency, 86.67 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 93.46 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 93.46 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.33 us = 0.02% latency, 1.87 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.33 us = 0.02% latency, 1.87 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.25 us = 0.01% latency, 711.99 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.25 us = 0.01% latency, 711.99 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 663.04 us = 0.17% latency, 101.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.48 ms = 1.7% latency, 109.36 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 119.37 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.97 ms = 0.52% latency, 119.81 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.99 ms = 0.52% latency, 118.35 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 190.5 us = 0.05% latency, 151.26 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 480.41 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 477.31 us = 0.12% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (7): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11.23 ms = 2.94% latency, 82.87 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.55 ms = 0.93% latency, 62.59 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.11 ms = 0.29% latency, 60.73 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 662.57 us = 0.17% latency, 101.79 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 96.32 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 96.32 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.72 us = 0.02% latency, 1.81 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.72 us = 0.02% latency, 1.81 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.17 us = 0.02% latency, 2.26 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.17 us = 0.02% latency, 2.26 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 528.1 us = 0.14% latency, 32.24 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 195.74 us = 0.05% latency, 86.14 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 94.18 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 94.18 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 69.86 us = 0.02% latency, 1.89 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 69.86 us = 0.02% latency, 1.89 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.73 us = 0.01% latency, 704.73 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.73 us = 0.01% latency, 704.73 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 525.95 us = 0.14% latency, 32.37 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 193.6 us = 0.05% latency, 87.09 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 94.18 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 94.18 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 69.86 us = 0.02% latency, 1.89 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 69.86 us = 0.02% latency, 1.89 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.68 us = 0.01% latency, 690.63 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.68 us = 0.01% latency, 690.63 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 680.69 us = 0.18% latency, 99.08 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.48 ms = 1.7% latency, 109.36 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 119.5 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.97 ms = 0.52% latency, 119.68 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2 ms = 0.52% latency, 118.29 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 190.97 us = 0.05% latency, 150.89 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 479.46 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 478.27 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (8): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11.21 ms = 2.94% latency, 83 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.53 ms = 0.93% latency, 62.88 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.12 ms = 0.29% latency, 60.38 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 668.76 us = 0.18% latency, 100.85 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 97.51 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 97.51 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.24 us = 0.02% latency, 1.82 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.24 us = 0.02% latency, 1.82 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.17 us = 0.02% latency, 2.26 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.17 us = 0.02% latency, 2.26 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 526.43 us = 0.14% latency, 32.34 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 194.79 us = 0.05% latency, 86.56 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 94.18 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 94.18 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.1 us = 0.02% latency, 1.88 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.1 us = 0.02% latency, 1.88 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.49 us = 0.01% latency, 708.34 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.49 us = 0.01% latency, 708.34 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 525.47 us = 0.14% latency, 32.4 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 193.6 us = 0.05% latency, 87.09 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 93.94 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 93.94 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.1 us = 0.02% latency, 1.88 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.1 us = 0.02% latency, 1.88 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.49 us = 0.01% latency, 708.34 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.49 us = 0.01% latency, 708.34 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 662.8 us = 0.17% latency, 101.76 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.47 ms = 1.69% latency, 109.43 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 119.49 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.97 ms = 0.52% latency, 119.71 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.99 ms = 0.52% latency, 118.4 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 190.5 us = 0.05% latency, 151.26 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 482.32 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 477.31 us = 0.12% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (9): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11.25 ms = 2.94% latency, 82.73 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.53 ms = 0.92% latency, 62.92 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.12 ms = 0.29% latency, 60.6 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 661.85 us = 0.17% latency, 101.9 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 97.75 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 97.75 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 73.43 us = 0.02% latency, 1.79 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 73.43 us = 0.02% latency, 1.79 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 57.22 us = 0.01% latency, 2.3 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 57.22 us = 0.01% latency, 2.3 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 526.67 us = 0.14% latency, 32.33 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 194.55 us = 0.05% latency, 86.67 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 94.65 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 94.65 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.57 us = 0.02% latency, 1.87 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.57 us = 0.02% latency, 1.87 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.73 us = 0.01% latency, 704.73 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.73 us = 0.01% latency, 704.73 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 524.76 us = 0.14% latency, 32.44 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 194.07 us = 0.05% latency, 86.88 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 93.46 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 93.46 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.33 us = 0.02% latency, 1.87 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.33 us = 0.02% latency, 1.87 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.97 us = 0.01% latency, 701.15 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.97 us = 0.01% latency, 701.15 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 664.71 us = 0.17% latency, 101.46 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.51 ms = 1.7% latency, 108.77 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 119.5 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.99 ms = 0.52% latency, 118.74 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2.01 ms = 0.53% latency, 117.21 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 191.93 us = 0.05% latency, 150.14 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 481.37 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 476.84 us = 0.12% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (10): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11.27 ms = 2.95% latency, 82.54 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.54 ms = 0.93% latency, 62.7 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.12 ms = 0.29% latency, 60.55 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 665.9 us = 0.17% latency, 101.28 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 97.51 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 97.51 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 73.19 us = 0.02% latency, 1.8 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 73.19 us = 0.02% latency, 1.8 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.65 us = 0.02% latency, 2.25 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.65 us = 0.02% latency, 2.25 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 530.48 us = 0.14% latency, 32.09 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 197.65 us = 0.05% latency, 85.31 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 94.65 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 94.65 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.57 us = 0.02% latency, 1.87 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.57 us = 0.02% latency, 1.87 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.49 us = 0.01% latency, 708.34 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.49 us = 0.01% latency, 708.34 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 525.71 us = 0.14% latency, 32.39 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 195.5 us = 0.05% latency, 86.24 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 93.94 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 93.94 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.57 us = 0.02% latency, 1.87 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.57 us = 0.02% latency, 1.87 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 45.78 us = 0.01% latency, 719.41 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 45.78 us = 0.01% latency, 719.41 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 667.33 us = 0.17% latency, 101.07 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.53 ms = 1.71% latency, 108.53 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.99 ms = 0.52% latency, 118.43 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.99 ms = 0.52% latency, 118.6 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2.01 ms = 0.53% latency, 117.48 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 190.97 us = 0.05% latency, 150.89 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 480.65 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 477.79 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (11): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11.37 ms = 2.98% latency, 81.84 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.59 ms = 0.94% latency, 61.94 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.12 ms = 0.29% latency, 60.24 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 668.29 us = 0.17% latency, 100.92 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 97.51 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 97.51 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 74.15 us = 0.02% latency, 1.78 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 74.15 us = 0.02% latency, 1.78 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.89 us = 0.02% latency, 2.24 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.89 us = 0.02% latency, 2.24 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 555.75 us = 0.15% latency, 30.64 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 199.08 us = 0.05% latency, 84.7 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 95.84 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 95.84 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 78.44 us = 0.02% latency, 1.68 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 78.44 us = 0.02% latency, 1.68 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 57.7 us = 0.02% latency, 570.77 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 57.7 us = 0.02% latency, 570.77 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 530 us = 0.14% latency, 32.12 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 196.93 us = 0.05% latency, 85.62 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 93.94 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 93.94 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.81 us = 0.02% latency, 1.86 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.81 us = 0.02% latency, 1.86 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.49 us = 0.01% latency, 708.34 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.49 us = 0.01% latency, 708.34 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 674.25 us = 0.18% latency, 100.03 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.58 ms = 1.72% latency, 107.68 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2.01 ms = 0.53% latency, 117.45 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2.01 ms = 0.53% latency, 117.66 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2.03 ms = 0.53% latency, 116.41 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 190.5 us = 0.05% latency, 151.26 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 483.75 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 477.31 us = 0.12% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (12): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11.37 ms = 2.98% latency, 81.82 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.59 ms = 0.94% latency, 61.9 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.12 ms = 0.29% latency, 60.24 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 669.48 us = 0.18% latency, 100.74 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 97.27 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 97.27 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 73.43 us = 0.02% latency, 1.79 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 73.43 us = 0.02% latency, 1.79 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 59.84 us = 0.02% latency, 2.2 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 59.84 us = 0.02% latency, 2.2 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 531.2 us = 0.14% latency, 32.05 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 198.13 us = 0.05% latency, 85.1 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 93.7 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 93.7 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.29 us = 0.02% latency, 1.85 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.29 us = 0.02% latency, 1.85 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.68 us = 0.01% latency, 690.63 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.68 us = 0.01% latency, 690.63 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 557.42 us = 0.15% latency, 30.54 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 196.46 us = 0.05% latency, 85.83 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 94.65 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 94.65 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 77.01 us = 0.02% latency, 1.71 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 77.01 us = 0.02% latency, 1.71 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 60.56 us = 0.02% latency, 543.8 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 60.56 us = 0.02% latency, 543.8 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 674.72 us = 0.18% latency, 99.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.58 ms = 1.72% latency, 107.67 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2.01 ms = 0.53% latency, 117.38 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2 ms = 0.52% latency, 117.74 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2.03 ms = 0.53% latency, 116.44 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 190.97 us = 0.05% latency, 150.89 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 483.27 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 477.31 us = 0.12% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (13): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11.25 ms = 2.95% latency, 82.7 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.54 ms = 0.93% latency, 62.67 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.12 ms = 0.29% latency, 60.41 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 669 us = 0.18% latency, 100.81 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 97.04 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 97.04 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.96 us = 0.02% latency, 1.81 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.96 us = 0.02% latency, 1.81 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.17 us = 0.02% latency, 2.26 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.17 us = 0.02% latency, 2.26 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 529.05 us = 0.14% latency, 32.18 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 195.98 us = 0.05% latency, 86.03 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 94.89 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 94.89 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.81 us = 0.02% latency, 1.86 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.81 us = 0.02% latency, 1.86 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.92 us = 0.01% latency, 687.19 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.92 us = 0.01% latency, 687.19 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 527.86 us = 0.14% latency, 32.25 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 195.5 us = 0.05% latency, 86.24 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 94.65 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 94.65 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.05 us = 0.02% latency, 1.85 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.05 us = 0.02% latency, 1.85 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.21 us = 0.01% latency, 697.61 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.21 us = 0.01% latency, 697.61 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 666.38 us = 0.17% latency, 101.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.5 ms = 1.7% latency, 108.93 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 119.06 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 119.1 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2 ms = 0.52% latency, 117.9 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 190.26 us = 0.05% latency, 151.45 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 483.51 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 477.79 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (14): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11.23 ms = 2.94% latency, 82.82 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.54 ms = 0.93% latency, 62.77 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.11 ms = 0.29% latency, 60.86 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 661.61 us = 0.17% latency, 101.94 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 97.27 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 97.27 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.48 us = 0.02% latency, 1.82 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.48 us = 0.02% latency, 1.82 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.89 us = 0.02% latency, 2.24 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.89 us = 0.02% latency, 2.24 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 531.44 us = 0.14% latency, 32.04 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 195.5 us = 0.05% latency, 86.24 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 96.56 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 96.56 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.81 us = 0.02% latency, 1.86 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.81 us = 0.02% latency, 1.86 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.21 us = 0.01% latency, 697.61 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.21 us = 0.01% latency, 697.61 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 526.19 us = 0.14% latency, 32.36 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 194.79 us = 0.05% latency, 86.56 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 93.22 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 93.22 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.1 us = 0.02% latency, 1.88 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.1 us = 0.02% latency, 1.88 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.01 us = 0.01% latency, 715.68 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.01 us = 0.01% latency, 715.68 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 667.1 us = 0.17% latency, 101.1 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.49 ms = 1.7% latency, 109.07 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 119.04 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 119.43 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2 ms = 0.52% latency, 117.94 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 190.73 us = 0.05% latency, 151.08 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 480.41 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 477.79 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (15): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11.24 ms = 2.94% latency, 82.78 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.54 ms = 0.93% latency, 62.71 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.12 ms = 0.29% latency, 60.32 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 668.53 us = 0.18% latency, 100.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 96.56 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 96.56 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 73.19 us = 0.02% latency, 1.8 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 73.19 us = 0.02% latency, 1.8 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.89 us = 0.02% latency, 2.24 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.89 us = 0.02% latency, 2.24 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 529.29 us = 0.14% latency, 32.17 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 196.7 us = 0.05% latency, 85.72 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 94.41 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 94.41 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.81 us = 0.02% latency, 1.86 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.81 us = 0.02% latency, 1.86 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.49 us = 0.01% latency, 708.34 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.49 us = 0.01% latency, 708.34 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 525.95 us = 0.14% latency, 32.37 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 195.26 us = 0.05% latency, 86.35 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 92.98 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 92.98 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.57 us = 0.02% latency, 1.87 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.57 us = 0.02% latency, 1.87 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.45 us = 0.01% latency, 694.1 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.45 us = 0.01% latency, 694.1 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 667.81 us = 0.17% latency, 100.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.5 ms = 1.7% latency, 109.03 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 119.03 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 119.19 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2 ms = 0.52% latency, 117.92 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 190.02 us = 0.05% latency, 151.64 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 480.18 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 476.84 us = 0.12% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (16): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11.26 ms = 2.95% latency, 82.66 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.54 ms = 0.93% latency, 62.71 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.12 ms = 0.29% latency, 60.29 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 669.24 us = 0.18% latency, 100.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 97.75 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 97.75 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 73.43 us = 0.02% latency, 1.79 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 73.43 us = 0.02% latency, 1.79 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.41 us = 0.02% latency, 2.26 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.41 us = 0.02% latency, 2.26 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 530 us = 0.14% latency, 32.12 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 195.98 us = 0.05% latency, 86.03 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 94.65 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 94.65 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.57 us = 0.02% latency, 1.87 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.57 us = 0.02% latency, 1.87 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.68 us = 0.01% latency, 690.63 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.68 us = 0.01% latency, 690.63 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 526.19 us = 0.14% latency, 32.36 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 194.79 us = 0.05% latency, 86.56 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 94.65 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 94.65 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 69.62 us = 0.02% latency, 1.89 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 69.62 us = 0.02% latency, 1.89 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.73 us = 0.01% latency, 704.73 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.73 us = 0.01% latency, 704.73 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 661.13 us = 0.17% latency, 102.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.51 ms = 1.7% latency, 108.81 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 119.47 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.99 ms = 0.52% latency, 118.77 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2.01 ms = 0.53% latency, 117.43 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 190.97 us = 0.05% latency, 150.89 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 482.32 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 477.31 us = 0.12% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (17): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11.34 ms = 2.97% latency, 82.06 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.57 ms = 0.93% latency, 62.3 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.13 ms = 0.29% latency, 60.17 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 666.14 us = 0.17% latency, 101.25 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 98.71 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 98.71 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 73.43 us = 0.02% latency, 1.79 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 73.43 us = 0.02% latency, 1.79 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 61.27 us = 0.02% latency, 2.15 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 61.27 us = 0.02% latency, 2.15 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 530.72 us = 0.14% latency, 32.08 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 198.13 us = 0.05% latency, 85.1 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 94.65 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 94.65 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.81 us = 0.02% latency, 1.86 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.81 us = 0.02% latency, 1.86 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.45 us = 0.01% latency, 694.1 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.45 us = 0.01% latency, 694.1 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 528.57 us = 0.14% latency, 32.21 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 197.89 us = 0.05% latency, 85.21 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 93.7 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 93.7 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.57 us = 0.02% latency, 1.87 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.57 us = 0.02% latency, 1.87 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.25 us = 0.01% latency, 711.99 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.25 us = 0.01% latency, 711.99 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 678.3 us = 0.18% latency, 99.43 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.57 ms = 1.72% latency, 107.83 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2.01 ms = 0.53% latency, 117.48 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2 ms = 0.52% latency, 117.81 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2.02 ms = 0.53% latency, 116.8 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 190.97 us = 0.05% latency, 150.89 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 481.61 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 477.08 us = 0.12% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (18): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11.35 ms = 2.97% latency, 81.99 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.57 ms = 0.93% latency, 62.28 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.13 ms = 0.3% latency, 60.08 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 670.91 us = 0.18% latency, 100.53 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 99.66 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 99.66 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 73.19 us = 0.02% latency, 1.8 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 73.19 us = 0.02% latency, 1.8 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 59.6 us = 0.02% latency, 2.21 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 59.6 us = 0.02% latency, 2.21 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 539.78 us = 0.14% latency, 31.54 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 196.7 us = 0.05% latency, 85.72 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 94.89 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 94.89 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.24 us = 0.02% latency, 1.82 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.24 us = 0.02% latency, 1.82 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.97 us = 0.01% latency, 701.15 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.97 us = 0.01% latency, 701.15 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 528.57 us = 0.14% latency, 32.21 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 197.41 us = 0.05% latency, 85.41 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 93.46 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 93.46 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.1 us = 0.02% latency, 1.88 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.1 us = 0.02% latency, 1.88 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.73 us = 0.01% latency, 704.73 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.73 us = 0.01% latency, 704.73 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 668.29 us = 0.17% latency, 100.92 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.57 ms = 1.72% latency, 107.72 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2.01 ms = 0.53% latency, 117.56 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2 ms = 0.52% latency, 117.9 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2.02 ms = 0.53% latency, 116.62 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 192.17 us = 0.05% latency, 149.95 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 482.56 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 478.51 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (19): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11.34 ms = 2.97% latency, 82.04 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.57 ms = 0.93% latency, 62.24 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.12 ms = 0.29% latency, 60.21 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 669.24 us = 0.18% latency, 100.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 96.08 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 96.08 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 73.91 us = 0.02% latency, 1.78 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 73.91 us = 0.02% latency, 1.78 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.41 us = 0.02% latency, 2.26 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.41 us = 0.02% latency, 2.26 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 534.77 us = 0.14% latency, 31.84 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 197.65 us = 0.05% latency, 85.31 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 94.65 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 94.65 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.29 us = 0.02% latency, 1.85 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.29 us = 0.02% latency, 1.85 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.21 us = 0.01% latency, 697.61 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.21 us = 0.01% latency, 697.61 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 532.39 us = 0.14% latency, 31.98 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 196.93 us = 0.05% latency, 85.62 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 95.37 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 95.37 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.29 us = 0.02% latency, 1.85 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.29 us = 0.02% latency, 1.85 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.97 us = 0.01% latency, 701.15 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.97 us = 0.01% latency, 701.15 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 671.86 us = 0.18% latency, 100.38 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.56 ms = 1.72% latency, 107.88 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2 ms = 0.52% latency, 117.84 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2 ms = 0.52% latency, 117.88 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2.02 ms = 0.53% latency, 116.58 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 191.93 us = 0.05% latency, 150.14 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 484.47 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 478.03 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (20): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11.24 ms = 2.94% latency, 82.78 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.54 ms = 0.93% latency, 62.79 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.12 ms = 0.29% latency, 60.29 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 666.14 us = 0.17% latency, 101.25 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 97.75 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 97.75 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.96 us = 0.02% latency, 1.81 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.96 us = 0.02% latency, 1.81 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.65 us = 0.02% latency, 2.25 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.65 us = 0.02% latency, 2.25 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 528.81 us = 0.14% latency, 32.2 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 195.03 us = 0.05% latency, 86.46 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 95.61 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 95.61 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.57 us = 0.02% latency, 1.87 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.57 us = 0.02% latency, 1.87 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.01 us = 0.01% latency, 715.68 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.01 us = 0.01% latency, 715.68 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 525.71 us = 0.14% latency, 32.39 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 193.83 us = 0.05% latency, 86.99 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 95.13 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 95.13 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.1 us = 0.02% latency, 1.88 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.1 us = 0.02% latency, 1.88 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 45.78 us = 0.01% latency, 719.41 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 45.78 us = 0.01% latency, 719.41 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 660.42 us = 0.17% latency, 102.12 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.5 ms = 1.7% latency, 108.96 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 119.2 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 118.96 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2 ms = 0.52% latency, 117.85 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 190.26 us = 0.05% latency, 151.45 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 481.13 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 476.36 us = 0.12% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (21): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11.24 ms = 2.94% latency, 82.8 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.54 ms = 0.93% latency, 62.74 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.12 ms = 0.29% latency, 60.69 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 663.76 us = 0.17% latency, 101.61 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 97.04 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 97.04 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 73.91 us = 0.02% latency, 1.78 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 73.91 us = 0.02% latency, 1.78 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 57.7 us = 0.02% latency, 2.28 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 57.7 us = 0.02% latency, 2.28 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 532.39 us = 0.14% latency, 31.98 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 196.7 us = 0.05% latency, 85.72 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 94.89 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 94.89 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.05 us = 0.02% latency, 1.85 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.05 us = 0.02% latency, 1.85 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 48.16 us = 0.01% latency, 683.79 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 48.16 us = 0.01% latency, 683.79 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 527.62 us = 0.14% latency, 32.27 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 195.26 us = 0.05% latency, 86.35 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 94.89 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 94.89 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.1 us = 0.02% latency, 1.88 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.1 us = 0.02% latency, 1.88 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.97 us = 0.01% latency, 701.15 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.97 us = 0.01% latency, 701.15 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 663.76 us = 0.17% latency, 101.61 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.49 ms = 1.7% latency, 109.07 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 118.96 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.97 ms = 0.52% latency, 119.55 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2 ms = 0.52% latency, 117.88 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 190.5 us = 0.05% latency, 151.26 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 480.89 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 476.36 us = 0.12% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (22): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11.24 ms = 2.94% latency, 82.78 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.54 ms = 0.93% latency, 62.79 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.12 ms = 0.29% latency, 60.58 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 666.14 us = 0.17% latency, 101.25 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 96.56 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 96.56 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 73.19 us = 0.02% latency, 1.8 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 73.19 us = 0.02% latency, 1.8 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 57.94 us = 0.02% latency, 2.27 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 57.94 us = 0.02% latency, 2.27 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 527.86 us = 0.14% latency, 32.25 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 196.46 us = 0.05% latency, 85.83 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 93.46 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 93.46 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.05 us = 0.02% latency, 1.85 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.05 us = 0.02% latency, 1.85 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.73 us = 0.01% latency, 704.73 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.73 us = 0.01% latency, 704.73 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 526.19 us = 0.14% latency, 32.36 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 195.03 us = 0.05% latency, 86.46 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 93.22 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 93.22 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.33 us = 0.02% latency, 1.87 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.33 us = 0.02% latency, 1.87 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.21 us = 0.01% latency, 697.61 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.21 us = 0.01% latency, 697.61 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 666.86 us = 0.17% latency, 101.14 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.49 ms = 1.7% latency, 109.08 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 119.1 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 119.27 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2 ms = 0.52% latency, 118.04 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 190.02 us = 0.05% latency, 151.64 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 481.61 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 478.98 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (23): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11.26 ms = 2.95% latency, 82.66 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.54 ms = 0.93% latency, 62.75 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.12 ms = 0.29% latency, 60.32 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 665.43 us = 0.17% latency, 101.36 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 98.94 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 98.94 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.96 us = 0.02% latency, 1.81 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.96 us = 0.02% latency, 1.81 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 57.46 us = 0.02% latency, 2.29 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 57.46 us = 0.02% latency, 2.29 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 527.14 us = 0.14% latency, 32.3 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 194.31 us = 0.05% latency, 86.77 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 94.18 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 94.18 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.57 us = 0.02% latency, 1.87 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.57 us = 0.02% latency, 1.87 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.92 us = 0.01% latency, 687.19 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.92 us = 0.01% latency, 687.19 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 528.1 us = 0.14% latency, 32.24 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 193.6 us = 0.05% latency, 87.09 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 95.37 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 95.37 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.81 us = 0.02% latency, 1.86 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.81 us = 0.02% latency, 1.86 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.97 us = 0.01% latency, 701.15 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.97 us = 0.01% latency, 701.15 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 660.9 us = 0.17% latency, 102.05 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.51 ms = 1.71% latency, 108.75 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 119.39 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.99 ms = 0.52% latency, 118.55 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2.01 ms = 0.53% latency, 117.32 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 190.73 us = 0.05% latency, 151.08 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 483.99 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 477.31 us = 0.12% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (24): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11.28 ms = 2.95% latency, 82.49 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.57 ms = 0.93% latency, 62.28 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.14 ms = 0.3% latency, 59.51 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 672.82 us = 0.18% latency, 100.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 104.43 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 104.43 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 75.1 us = 0.02% latency, 1.75 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 75.1 us = 0.02% latency, 1.75 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.89 us = 0.02% latency, 2.24 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.89 us = 0.02% latency, 2.24 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 534.06 us = 0.14% latency, 31.88 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 197.17 us = 0.05% latency, 85.51 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 96.56 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 96.56 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.05 us = 0.02% latency, 1.85 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.05 us = 0.02% latency, 1.85 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.73 us = 0.01% latency, 704.73 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.73 us = 0.01% latency, 704.73 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 525.71 us = 0.14% latency, 32.39 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 195.03 us = 0.05% latency, 86.46 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 95.61 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 95.61 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.1 us = 0.02% latency, 1.88 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.1 us = 0.02% latency, 1.88 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.25 us = 0.01% latency, 711.99 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.25 us = 0.01% latency, 711.99 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 666.86 us = 0.17% latency, 101.14 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.51 ms = 1.7% latency, 108.82 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.99 ms = 0.52% latency, 118.82 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 118.93 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2.01 ms = 0.53% latency, 117.73 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 190.97 us = 0.05% latency, 150.89 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 481.13 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 478.27 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (25): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11.25 ms = 2.95% latency, 82.67 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.55 ms = 0.93% latency, 62.56 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.12 ms = 0.29% latency, 60.56 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 665.66 us = 0.17% latency, 101.32 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 97.04 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 97.04 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.96 us = 0.02% latency, 1.81 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.96 us = 0.02% latency, 1.81 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.17 us = 0.02% latency, 2.26 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.17 us = 0.02% latency, 2.26 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 535.96 us = 0.14% latency, 31.77 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 197.41 us = 0.05% latency, 85.41 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 95.13 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 95.13 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72 us = 0.02% latency, 1.83 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72 us = 0.02% latency, 1.83 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.97 us = 0.01% latency, 701.15 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.97 us = 0.01% latency, 701.15 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 529.77 us = 0.14% latency, 32.14 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 195.5 us = 0.05% latency, 86.24 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 96.8 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 96.8 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.29 us = 0.02% latency, 1.85 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.29 us = 0.02% latency, 1.85 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.25 us = 0.01% latency, 711.99 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.25 us = 0.01% latency, 711.99 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 668.05 us = 0.17% latency, 100.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.5 ms = 1.7% latency, 108.94 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 118.93 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 119.17 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2 ms = 0.52% latency, 117.92 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 191.69 us = 0.05% latency, 150.32 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 482.08 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 476.36 us = 0.12% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (26): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11.26 ms = 2.95% latency, 82.62 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.55 ms = 0.93% latency, 62.53 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.12 ms = 0.29% latency, 60.46 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 665.9 us = 0.17% latency, 101.28 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 97.04 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 97.04 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.96 us = 0.02% latency, 1.81 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.96 us = 0.02% latency, 1.81 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.89 us = 0.02% latency, 2.24 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.89 us = 0.02% latency, 2.24 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 531.44 us = 0.14% latency, 32.04 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 195.98 us = 0.05% latency, 86.03 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 94.41 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 94.41 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.29 us = 0.02% latency, 1.85 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.29 us = 0.02% latency, 1.85 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.92 us = 0.01% latency, 687.19 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.92 us = 0.01% latency, 687.19 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 535.01 us = 0.14% latency, 31.82 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 195.74 us = 0.05% latency, 86.14 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 97.51 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 97.51 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.53 us = 0.02% latency, 1.84 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.53 us = 0.02% latency, 1.84 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.68 us = 0.01% latency, 690.63 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.68 us = 0.01% latency, 690.63 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 664 us = 0.17% latency, 101.57 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.5 ms = 1.7% latency, 108.88 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 118.96 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 119.22 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2 ms = 0.52% latency, 117.8 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 192.17 us = 0.05% latency, 149.95 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 481.61 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 478.03 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (27): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11.2 ms = 2.93% latency, 83.05 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.53 ms = 0.93% latency, 62.87 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.12 ms = 0.29% latency, 60.51 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 666.38 us = 0.17% latency, 101.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 96.08 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 96.08 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.96 us = 0.02% latency, 1.81 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.96 us = 0.02% latency, 1.81 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.89 us = 0.02% latency, 2.24 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.89 us = 0.02% latency, 2.24 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 528.34 us = 0.14% latency, 32.23 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 194.79 us = 0.05% latency, 86.56 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 94.41 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 94.41 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.05 us = 0.02% latency, 1.85 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.05 us = 0.02% latency, 1.85 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.73 us = 0.01% latency, 704.73 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.73 us = 0.01% latency, 704.73 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 525.71 us = 0.14% latency, 32.39 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 192.64 us = 0.05% latency, 87.53 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 95.84 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 95.84 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.57 us = 0.02% latency, 1.87 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.57 us = 0.02% latency, 1.87 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.68 us = 0.01% latency, 690.63 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.68 us = 0.01% latency, 690.63 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 663.04 us = 0.17% latency, 101.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.47 ms = 1.69% latency, 109.52 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 119.35 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.97 ms = 0.52% latency, 119.73 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.99 ms = 0.52% latency, 118.86 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 189.3 us = 0.05% latency, 152.22 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 482.56 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 476.6 us = 0.12% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (28): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11.21 ms = 2.94% latency, 82.96 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.55 ms = 0.93% latency, 62.64 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.12 ms = 0.29% latency, 60.71 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 661.61 us = 0.17% latency, 101.94 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 97.51 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 97.51 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.96 us = 0.02% latency, 1.81 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.96 us = 0.02% latency, 1.81 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 59.37 us = 0.02% latency, 2.22 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 59.37 us = 0.02% latency, 2.22 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 529.77 us = 0.14% latency, 32.14 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 195.26 us = 0.05% latency, 86.35 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 93.7 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 93.7 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 69.86 us = 0.02% latency, 1.89 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 69.86 us = 0.02% latency, 1.89 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 48.16 us = 0.01% latency, 683.79 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 48.16 us = 0.01% latency, 683.79 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 527.62 us = 0.14% latency, 32.27 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 193.83 us = 0.05% latency, 86.99 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 92.74 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 92.74 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.81 us = 0.02% latency, 1.86 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.81 us = 0.02% latency, 1.86 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.68 us = 0.01% latency, 690.63 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.68 us = 0.01% latency, 690.63 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 672.82 us = 0.18% latency, 100.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.46 ms = 1.69% latency, 109.55 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.97 ms = 0.51% latency, 120.07 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 119.33 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.99 ms = 0.52% latency, 118.72 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 190.5 us = 0.05% latency, 151.26 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 480.89 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 477.79 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (29): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11.21 ms = 2.93% latency, 83.01 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.54 ms = 0.93% latency, 62.74 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.11 ms = 0.29% latency, 61.01 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 657.56 us = 0.17% latency, 102.57 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 96.32 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 96.32 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.96 us = 0.02% latency, 1.81 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 72.96 us = 0.02% latency, 1.81 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 57.94 us = 0.02% latency, 2.27 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 57.94 us = 0.02% latency, 2.27 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 530 us = 0.14% latency, 32.12 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 193.83 us = 0.05% latency, 86.99 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 94.65 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 94.65 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.05 us = 0.02% latency, 1.85 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.05 us = 0.02% latency, 1.85 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.45 us = 0.01% latency, 694.1 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.45 us = 0.01% latency, 694.1 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 530.72 us = 0.14% latency, 32.08 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 194.79 us = 0.05% latency, 86.56 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 94.65 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 94.65 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.57 us = 0.02% latency, 1.87 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.57 us = 0.02% latency, 1.87 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.68 us = 0.01% latency, 690.63 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.68 us = 0.01% latency, 690.63 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 669.72 us = 0.18% latency, 100.71 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.46 ms = 1.69% latency, 109.55 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.97 ms = 0.52% latency, 119.65 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.97 ms = 0.52% latency, 119.71 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.99 ms = 0.52% latency, 118.72 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 189.78 us = 0.05% latency, 151.83 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 482.56 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 477.08 us = 0.12% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (30): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11.23 ms = 2.94% latency, 82.87 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.54 ms = 0.93% latency, 62.77 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.12 ms = 0.29% latency, 60.32 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 663.52 us = 0.17% latency, 101.65 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 97.27 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 97.27 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 73.91 us = 0.02% latency, 1.78 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 73.91 us = 0.02% latency, 1.78 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.65 us = 0.02% latency, 2.25 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 58.65 us = 0.02% latency, 2.25 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 530.24 us = 0.14% latency, 32.11 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 196.7 us = 0.05% latency, 85.72 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 93.46 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 93.46 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.33 us = 0.02% latency, 1.87 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.33 us = 0.02% latency, 1.87 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.68 us = 0.01% latency, 690.63 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.68 us = 0.01% latency, 690.63 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 526.91 us = 0.14% latency, 32.31 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 193.6 us = 0.05% latency, 87.09 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 96.08 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 96.08 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 69.62 us = 0.02% latency, 1.89 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 69.62 us = 0.02% latency, 1.89 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.73 us = 0.01% latency, 704.73 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 46.73 us = 0.01% latency, 704.73 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 663.28 us = 0.17% latency, 101.68 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.48 ms = 1.7% latency, 109.21 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.97 ms = 0.52% latency, 119.87 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.97 ms = 0.52% latency, 119.62 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2.01 ms = 0.53% latency, 117.5 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 192.4 us = 0.05% latency, 149.77 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 482.08 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 476.84 us = 0.12% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (31): LlamaDecoderLayer(
            218.26 M = 2.72% Params, 465.16 GMACs = 2.92% MACs, 11.27 ms = 2.95% latency, 82.53 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.09 M = 0.52% Params, 111.08 GMACs = 0.7% MACs, 3.56 ms = 0.93% latency, 62.42 TFLOPS
              (q_proj): lora.Linear(
                16.84 M = 0.21% Params, 33.85 GMACs = 0.21% MACs, 1.13 ms = 0.3% latency, 59.96 TFLOPS
                (base_layer): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 671.39 us = 0.18% latency, 100.46 TFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 96.08 us = 0.03% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 96.08 us = 0.03% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 74.39 us = 0.02% latency, 1.77 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 74.39 us = 0.02% latency, 1.77 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 60.32 us = 0.02% latency, 2.18 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 60.32 us = 0.02% latency, 2.18 TFLOPS, in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 530.96 us = 0.14% latency, 32.07 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 196.7 us = 0.05% latency, 85.72 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 93.7 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 93.7 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.53 us = 0.02% latency, 1.84 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 71.53 us = 0.02% latency, 1.84 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.21 us = 0.01% latency, 697.61 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.21 us = 0.01% latency, 697.61 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                4.24 M = 0.05% Params, 8.51 GMACs = 0.05% MACs, 525.71 us = 0.14% latency, 32.39 TFLOPS
                (base_layer): Linear(4.19 M = 0.05% Params, 8.43 GMACs = 0.05% MACs, 195.5 us = 0.05% latency, 86.24 TFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 93.7 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 93.7 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.57 us = 0.02% latency, 1.87 TFLOPS
                  (default): Linear(32.77 K = 0% Params, 65.86 MMACs = 0% MACs, 70.57 us = 0.02% latency, 1.87 TFLOPS, in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.21 us = 0.01% latency, 697.61 GFLOPS
                  (default): Linear(8.19 K = 0% Params, 16.47 MMACs = 0% MACs, 47.21 us = 0.01% latency, 697.61 GFLOPS, in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear(16.78 M = 0.21% Params, 33.72 GMACs = 0.21% MACs, 672.58 us = 0.18% latency, 100.28 TFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 354.08 GMACs = 2.22% MACs, 6.51 ms = 1.7% latency, 108.81 TFLOPS
              (gate_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.99 ms = 0.52% latency, 118.9 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 1.98 ms = 0.52% latency, 119 TFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(58.72 M = 0.73% Params, 118.03 GMACs = 0.74% MACs, 2 ms = 0.52% latency, 117.74 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 191.21 us = 0.05% latency, 150.7 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 482.08 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 477.79 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 481.61 us = 0.13% latency, 0 FLOPS, (4096,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 102.91 KMACs = 0% MACs, 540.73 us = 0.14% latency, 380.64 MFLOPS)
      )
      (lm_head): Linear(525.34 M = 6.54% Params, 1.06 TMACs = 6.62% MACs, 16.02 ms = 4.2% latency, 131.81 TFLOPS, in_features=4096, out_features=128256, bias=False)
    )
  )
)
------------------------------------------------------------------------------
