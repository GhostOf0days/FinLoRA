
-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 2:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

world size:                                                             4       
data parallel size:                                                     4       
model parallel size:                                                    1       
batch size per GPU:                                                     8       
params per GPU:                                                         8.03 B  
params of model = params per GPU * mp_size:                             8.03 B  
fwd MACs per GPU:                                                       1.12 TMACs
fwd flops per GPU:                                                      2.23 T  
fwd flops of model = fwd flops per GPU * mp_size:                       2.23 T  
fwd latency:                                                            380.93 ms
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    5.86 TFLOPS
bwd latency:                                                            765.78 ms
bwd FLOPS per GPU = 2 * fwd flops per GPU / bwd latency:                5.83 TFLOPS
fwd+bwd FLOPS per GPU = 3 * fwd flops per GPU / (fwd+bwd latency):      5.84 TFLOPS
step latency:                                                           853.63 us
iter latency:                                                           1.15 s  
FLOPS per GPU = 3 * fwd flops per GPU / iter latency:                   5.84 TFLOPS
samples/second:                                                         27.89   

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'PeftModelForCausalLM': '8.03 B'}
    MACs        - {'PeftModelForCausalLM': '1.12 TMACs'}
    fwd latency - {'PeftModelForCausalLM': '373.56 ms'}
depth 1:
    params      - {'LoraModel': '8.03 B'}
    MACs        - {'LoraModel': '1.12 TMACs'}
    fwd latency - {'LoraModel': '373.56 ms'}
depth 2:
    params      - {'LlamaForCausalLM': '8.03 B'}
    MACs        - {'LlamaForCausalLM': '1.12 TMACs'}
    fwd latency - {'LlamaForCausalLM': '373.56 ms'}
depth 3:
    params      - {'LlamaModel': '7.51 B'}
    MACs        - {'Linear': '672.43 GMACs'}
    fwd latency - {'LlamaModel': '363.51 ms'}
depth 4:
    params      - {'ModuleList': '6.98 B'}
    MACs        - {'ModuleList': '444.34 GMACs'}
    fwd latency - {'ModuleList': '356.51 ms'}
depth 5:
    params      - {'LlamaDecoderLayer': '6.98 B'}
    MACs        - {'LlamaDecoderLayer': '444.34 GMACs'}
    fwd latency - {'LlamaDecoderLayer': '356.51 ms'}
depth 6:
    params      - {'LlamaMLP': '5.64 B'}
    MACs        - {'LlamaSdpaAttention': '438.24 GMACs'}
    fwd latency - {'LlamaMLP': '172.95 ms'}
depth 7:
    params      - {'Linear8bitLt': '6.98 B'}
    MACs        - {'Linear8bitLt': '14.84 GMACs'}
    fwd latency - {'Linear8bitLt': '298.58 ms'}
depth 8:
    params      - {'Linear8bitLt': '805.31 M'}
    MACs        - {'Linear8bitLt': '5.71 GMACs'}
    fwd latency - {'Linear8bitLt': '87.4 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

PeftModelForCausalLM(
  8.03 B = 100% Params, 1.12 TMACs = 100% MACs, 373.56 ms = 100% latency, 5.98 TFLOPS
  (base_model): LoraModel(
    8.03 B = 100% Params, 1.12 TMACs = 100% MACs, 373.56 ms = 100% latency, 5.98 TFLOPS
    (model): LlamaForCausalLM(
      8.03 B = 100% Params, 1.12 TMACs = 100% MACs, 373.56 ms = 100% latency, 5.98 TFLOPS
      (model): LlamaModel(
        7.51 B = 93.46% Params, 444.34 GMACs = 39.79% MACs, 363.51 ms = 97.31% latency, 2.45 TFLOPS
        (embed_tokens): Embedding(525.34 M = 6.54% Params, 0 MACs = 0% MACs, 153.06 us = 0.04% latency, 0 FLOPS, 128256, 4096)
        (layers): ModuleList(
          (0): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.54 GMACs = 1.21% MACs, 10.29 ms = 2.75% latency, 2.63 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.53 GMACs = 1.21% MACs, 5.01 ms = 1.34% latency, 5.41 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 52.43 MMACs = 0% MACs, 1.58 ms = 0.42% latency, 66.56 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 10.49 MMACs = 0% MACs, 1.16 ms = 0.31% latency, 18.04 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 83.68 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 83.68 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 75.82 us = 0.02% latency, 553.21 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 75.82 us = 0.02% latency, 553.21 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 58.89 us = 0.02% latency, 712.23 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 58.89 us = 0.02% latency, 712.23 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 28.84 MMACs = 0% MACs, 1.18 ms = 0.32% latency, 48.68 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 2.62 MMACs = 0% MACs, 857.83 us = 0.23% latency, 6.11 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.48 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.48 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.09 us = 0.02% latency, 644.4 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.09 us = 0.02% latency, 644.4 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 56.74 us = 0.02% latency, 184.79 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 56.74 us = 0.02% latency, 184.79 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 28.84 MMACs = 0% MACs, 1.16 ms = 0.31% latency, 49.52 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 2.62 MMACs = 0% MACs, 843.29 us = 0.23% latency, 6.22 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.24 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.24 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.09 us = 0.02% latency, 644.4 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.09 us = 0.02% latency, 644.4 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.26 us = 0.01% latency, 204.56 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.26 us = 0.01% latency, 204.56 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 557.66 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 5.24 MMACs = 0% MACs, 4.4 ms = 1.18% latency, 6.56 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 0 MACs = 0% MACs, 1.23 ms = 0.33% latency, 0 FLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 0 MACs = 0% MACs, 1.22 ms = 0.33% latency, 0 FLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 5.24 MMACs = 0% MACs, 1.53 ms = 0.41% latency, 6.85 GFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 138.28 us = 0.04% latency, 132.7 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 330.21 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 328.3 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (1): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.53 GMACs = 1.21% MACs, 8.75 ms = 2.34% latency, 3.09 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.52 GMACs = 1.21% MACs, 3.46 ms = 0.92% latency, 7.82 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 41.94 MMACs = 0% MACs, 948.67 us = 0.25% latency, 88.43 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 553.37 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 68.66 us = 0.02% latency, 610.84 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 68.66 us = 0.02% latency, 610.84 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 58.17 us = 0.02% latency, 720.99 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 58.17 us = 0.02% latency, 720.99 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 26.21 MMACs = 0% MACs, 718.36 us = 0.19% latency, 72.98 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 0 MACs = 0% MACs, 396.01 us = 0.11% latency, 0 FLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.85 us = 0.02% latency, 646.77 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.85 us = 0.02% latency, 646.77 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 53.17 us = 0.01% latency, 197.22 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 53.17 us = 0.01% latency, 197.22 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 26.21 MMACs = 0% MACs, 715.97 us = 0.19% latency, 73.23 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 0 MACs = 0% MACs, 391.72 us = 0.1% latency, 0 FLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.48 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.48 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 66.28 us = 0.02% latency, 632.81 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 66.28 us = 0.02% latency, 632.81 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.45 us = 0.01% latency, 199.91 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.45 us = 0.01% latency, 199.91 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 552.42 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 10.49 MMACs = 0% MACs, 4.42 ms = 1.18% latency, 8.9 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 0 MACs = 0% MACs, 1.22 ms = 0.33% latency, 0 FLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 0 MACs = 0% MACs, 1.22 ms = 0.33% latency, 0 FLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 10.49 MMACs = 0% MACs, 1.57 ms = 0.42% latency, 13.4 GFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 133.51 us = 0.04% latency, 137.44 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 329.26 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 327.59 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (2): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.6 GMACs = 1.22% MACs, 9.74 ms = 2.61% latency, 2.79 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.6 GMACs = 1.22% MACs, 4.85 ms = 1.3% latency, 5.6 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 99.61 MMACs = 0.01% MACs, 1.43 ms = 0.38% latency, 139.74 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 57.67 MMACs = 0.01% MACs, 1.04 ms = 0.28% latency, 110.93 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 74.39 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 74.39 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 67.47 us = 0.02% latency, 621.63 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 67.47 us = 0.02% latency, 621.63 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 54.12 us = 0.01% latency, 774.99 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 54.12 us = 0.01% latency, 774.99 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 40.63 MMACs = 0% MACs, 1.18 ms = 0.32% latency, 68.76 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 14.42 MMACs = 0% MACs, 853.78 us = 0.23% latency, 33.77 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 66.52 us = 0.02% latency, 630.54 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 66.52 us = 0.02% latency, 630.54 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 53.17 us = 0.01% latency, 197.22 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 53.17 us = 0.01% latency, 197.22 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 40.63 MMACs = 0% MACs, 1.17 ms = 0.31% latency, 69.56 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 14.42 MMACs = 0% MACs, 841.38 us = 0.23% latency, 34.27 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.19 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.19 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.85 us = 0.02% latency, 646.77 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.85 us = 0.02% latency, 646.77 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.21 us = 0.01% latency, 200.82 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.21 us = 0.01% latency, 200.82 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 558.14 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 0 MACs = 0% MACs, 4.01 ms = 1.07% latency, 4.58 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 0 MACs = 0% MACs, 1.23 ms = 0.33% latency, 0 FLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 0 MACs = 0% MACs, 1.23 ms = 0.33% latency, 0 FLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 0 MACs = 0% MACs, 1.14 ms = 0.31% latency, 0 FLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 132.56 us = 0.04% latency, 138.43 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 328.78 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 328.54 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (3): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.66 GMACs = 1.22% MACs, 11.03 ms = 2.95% latency, 2.48 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.55 GMACs = 1.21% MACs, 4.86 ms = 1.3% latency, 5.58 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 62.91 MMACs = 0.01% MACs, 1.42 ms = 0.38% latency, 88.82 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 20.97 MMACs = 0% MACs, 1.03 ms = 0.28% latency, 40.56 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.43 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.43 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 67 us = 0.02% latency, 626.06 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 67 us = 0.02% latency, 626.06 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 53.41 us = 0.01% latency, 785.37 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 53.41 us = 0.01% latency, 785.37 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 31.46 MMACs = 0% MACs, 1.2 ms = 0.32% latency, 52.39 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 5.24 MMACs = 0% MACs, 873.09 us = 0.23% latency, 12.01 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.57 us = 0.02% latency, 639.72 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.57 us = 0.02% latency, 639.72 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.45 us = 0.01% latency, 199.91 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.45 us = 0.01% latency, 199.91 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 31.46 MMACs = 0% MACs, 1.16 ms = 0.31% latency, 54.01 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 5.24 MMACs = 0% MACs, 842.81 us = 0.23% latency, 12.44 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.37 us = 0.02% latency, 651.56 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.37 us = 0.02% latency, 651.56 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.74 us = 0.01% latency, 202.67 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.74 us = 0.01% latency, 202.67 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 556.23 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 110.1 MMACs = 0.01% MACs, 5.29 ms = 1.42% latency, 45.09 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.88 ms = 0.5% latency, 58.51 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.86 ms = 0.5% latency, 59.29 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 0 MACs = 0% MACs, 1.15 ms = 0.31% latency, 0 FLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 132.56 us = 0.04% latency, 138.43 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 327.11 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 328.54 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (4): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.65 GMACs = 1.22% MACs, 10.99 ms = 2.94% latency, 2.48 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.54 GMACs = 1.21% MACs, 4.85 ms = 1.3% latency, 5.58 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 57.67 MMACs = 0.01% MACs, 1.41 ms = 0.38% latency, 81.71 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 15.73 MMACs = 0% MACs, 1.03 ms = 0.28% latency, 30.56 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 66.76 us = 0.02% latency, 628.29 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 66.76 us = 0.02% latency, 628.29 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 53.64 us = 0.01% latency, 781.87 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 53.64 us = 0.01% latency, 781.87 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 30.15 MMACs = 0% MACs, 1.17 ms = 0.31% latency, 51.33 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 3.93 MMACs = 0% MACs, 849.01 us = 0.23% latency, 9.26 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.19 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.19 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.85 us = 0.02% latency, 646.77 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.85 us = 0.02% latency, 646.77 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.93 us = 0.01% latency, 198.11 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.93 us = 0.01% latency, 198.11 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 30.15 MMACs = 0% MACs, 1.18 ms = 0.32% latency, 51.08 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 3.93 MMACs = 0% MACs, 847.34 us = 0.23% latency, 9.28 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.33 us = 0.02% latency, 642.05 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.33 us = 0.02% latency, 642.05 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 59.13 us = 0.02% latency, 177.34 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 59.13 us = 0.02% latency, 177.34 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 562.91 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 110.1 MMACs = 0.01% MACs, 5.26 ms = 1.41% latency, 45.35 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.85 ms = 0.5% latency, 59.38 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.85 ms = 0.5% latency, 59.46 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 0 MACs = 0% MACs, 1.14 ms = 0.31% latency, 0 FLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 134.71 us = 0.04% latency, 136.22 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 328.54 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 329.26 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (5): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.66 GMACs = 1.22% MACs, 11.03 ms = 2.95% latency, 2.48 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.55 GMACs = 1.21% MACs, 4.83 ms = 1.29% latency, 5.6 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 62.91 MMACs = 0.01% MACs, 1.41 ms = 0.38% latency, 88.94 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 20.97 MMACs = 0% MACs, 1.03 ms = 0.28% latency, 40.55 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.19 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.19 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 66.04 us = 0.02% latency, 635.1 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 66.04 us = 0.02% latency, 635.1 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 53.17 us = 0.01% latency, 788.89 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 53.17 us = 0.01% latency, 788.89 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 31.46 MMACs = 0% MACs, 1.17 ms = 0.31% latency, 53.63 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 5.24 MMACs = 0% MACs, 847.34 us = 0.23% latency, 12.37 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.48 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.48 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.09 us = 0.02% latency, 644.4 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.09 us = 0.02% latency, 644.4 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.45 us = 0.01% latency, 199.91 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.45 us = 0.01% latency, 199.91 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 31.46 MMACs = 0% MACs, 1.17 ms = 0.31% latency, 53.9 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 5.24 MMACs = 0% MACs, 839.71 us = 0.22% latency, 12.49 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.24 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.24 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.57 us = 0.02% latency, 639.72 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.57 us = 0.02% latency, 639.72 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.21 us = 0.01% latency, 200.82 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.21 us = 0.01% latency, 200.82 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 556.95 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 110.1 MMACs = 0.01% MACs, 5.31 ms = 1.42% latency, 44.91 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.85 ms = 0.5% latency, 59.37 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.91 ms = 0.51% latency, 57.79 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 0 MACs = 0% MACs, 1.15 ms = 0.31% latency, 0 FLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 133.28 us = 0.04% latency, 137.68 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 329.73 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 329.73 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (6): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.67 GMACs = 1.22% MACs, 10.97 ms = 2.94% latency, 2.49 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.56 GMACs = 1.21% MACs, 4.84 ms = 1.29% latency, 5.61 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 73.4 MMACs = 0.01% MACs, 1.42 ms = 0.38% latency, 103.47 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 31.46 MMACs = 0% MACs, 1.04 ms = 0.28% latency, 60.52 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.43 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.43 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.8 us = 0.02% latency, 637.4 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.8 us = 0.02% latency, 637.4 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 53.64 us = 0.01% latency, 781.87 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 53.64 us = 0.01% latency, 781.87 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 34.08 MMACs = 0% MACs, 1.18 ms = 0.32% latency, 57.92 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 7.86 MMACs = 0% MACs, 852.82 us = 0.23% latency, 18.44 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.61 us = 0.02% latency, 649.16 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.61 us = 0.02% latency, 649.16 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.98 us = 0.01% latency, 201.75 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.98 us = 0.01% latency, 201.75 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 34.08 MMACs = 0% MACs, 1.16 ms = 0.31% latency, 58.85 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 7.86 MMACs = 0% MACs, 837.09 us = 0.22% latency, 18.79 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.13 us = 0.02% latency, 653.98 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.13 us = 0.02% latency, 653.98 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 50.78 us = 0.01% latency, 206.48 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 50.78 us = 0.01% latency, 206.48 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 563.86 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 110.1 MMACs = 0.01% MACs, 5.26 ms = 1.41% latency, 45.34 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.85 ms = 0.5% latency, 59.46 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.86 ms = 0.5% latency, 59.28 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 0 MACs = 0% MACs, 1.15 ms = 0.31% latency, 0 FLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 133.04 us = 0.04% latency, 137.93 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 330.92 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 328.3 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (7): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.71 GMACs = 1.23% MACs, 10.98 ms = 2.94% latency, 2.5 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.6 GMACs = 1.22% MACs, 4.83 ms = 1.29% latency, 5.63 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 99.61 MMACs = 0.01% MACs, 1.43 ms = 0.38% latency, 139.41 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 57.67 MMACs = 0.01% MACs, 1.04 ms = 0.28% latency, 111.32 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.43 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.43 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 76.06 us = 0.02% latency, 551.48 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 76.06 us = 0.02% latency, 551.48 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 55.31 us = 0.01% latency, 758.28 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 55.31 us = 0.01% latency, 758.28 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 40.63 MMACs = 0% MACs, 1.17 ms = 0.31% latency, 69.45 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 14.42 MMACs = 0% MACs, 845.67 us = 0.23% latency, 34.1 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.24 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.24 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.85 us = 0.02% latency, 646.77 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.85 us = 0.02% latency, 646.77 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 53.41 us = 0.01% latency, 196.34 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 53.41 us = 0.01% latency, 196.34 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 40.63 MMACs = 0% MACs, 1.16 ms = 0.31% latency, 70.19 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 14.42 MMACs = 0% MACs, 836.37 us = 0.22% latency, 34.48 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.13 us = 0.02% latency, 653.98 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.13 us = 0.02% latency, 653.98 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.02 us = 0.01% latency, 205.52 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.02 us = 0.01% latency, 205.52 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 552.42 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 110.1 MMACs = 0.01% MACs, 5.26 ms = 1.41% latency, 45.34 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.85 ms = 0.5% latency, 59.45 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.85 ms = 0.5% latency, 59.5 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 0 MACs = 0% MACs, 1.15 ms = 0.31% latency, 0 FLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 132.56 us = 0.04% latency, 138.43 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 329.73 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 329.02 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (8): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.81 GMACs = 1.24% MACs, 10.98 ms = 2.94% latency, 2.52 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.7 GMACs = 1.23% MACs, 4.85 ms = 1.3% latency, 5.65 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 162.53 MMACs = 0.01% MACs, 1.43 ms = 0.38% latency, 227.38 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 120.59 MMACs = 0.01% MACs, 1.05 ms = 0.28% latency, 230 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.43 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.43 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 66.52 us = 0.02% latency, 630.54 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 66.52 us = 0.02% latency, 630.54 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 52.45 us = 0.01% latency, 799.64 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 52.45 us = 0.01% latency, 799.64 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 56.36 MMACs = 0.01% MACs, 1.19 ms = 0.32% latency, 95.07 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 30.15 MMACs = 0% MACs, 858.31 us = 0.23% latency, 70.25 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.67 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.67 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.33 us = 0.02% latency, 642.05 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.33 us = 0.02% latency, 642.05 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.74 us = 0.01% latency, 202.67 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.74 us = 0.01% latency, 202.67 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 56.36 MMACs = 0.01% MACs, 1.16 ms = 0.31% latency, 96.78 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 30.15 MMACs = 0% MACs, 842.57 us = 0.23% latency, 71.56 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 71.53 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 71.53 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.13 us = 0.02% latency, 653.98 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.13 us = 0.02% latency, 653.98 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.21 us = 0.01% latency, 200.82 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.21 us = 0.01% latency, 200.82 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 554.56 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 110.1 MMACs = 0.01% MACs, 5.25 ms = 1.41% latency, 45.43 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.85 ms = 0.49% latency, 59.64 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.85 ms = 0.5% latency, 59.43 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 0 MACs = 0% MACs, 1.14 ms = 0.31% latency, 0 FLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 134.23 us = 0.04% latency, 136.71 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 329.02 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 329.26 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (9): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.78 GMACs = 1.23% MACs, 10.97 ms = 2.94% latency, 2.51 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.67 GMACs = 1.22% MACs, 4.84 ms = 1.3% latency, 5.65 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 146.8 MMACs = 0.01% MACs, 1.41 ms = 0.38% latency, 207.63 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 104.86 MMACs = 0.01% MACs, 1.04 ms = 0.28% latency, 202.49 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.8 us = 0.02% latency, 637.4 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.8 us = 0.02% latency, 637.4 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 52.45 us = 0.01% latency, 799.64 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 52.45 us = 0.01% latency, 799.64 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 52.43 MMACs = 0% MACs, 1.18 ms = 0.32% latency, 89.03 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 26.21 MMACs = 0% MACs, 855.45 us = 0.23% latency, 61.29 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.24 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.24 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.13 us = 0.02% latency, 653.98 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.13 us = 0.02% latency, 653.98 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.74 us = 0.01% latency, 202.67 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.74 us = 0.01% latency, 202.67 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 52.43 MMACs = 0% MACs, 1.18 ms = 0.31% latency, 89.19 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 26.21 MMACs = 0% MACs, 854.49 us = 0.23% latency, 61.36 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.24 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.24 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.9 us = 0.02% latency, 656.42 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.9 us = 0.02% latency, 656.42 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.45 us = 0.01% latency, 199.91 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.45 us = 0.01% latency, 199.91 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 553.85 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 110.1 MMACs = 0.01% MACs, 5.25 ms = 1.41% latency, 45.4 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.85 ms = 0.5% latency, 59.39 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.85 ms = 0.5% latency, 59.49 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 0 MACs = 0% MACs, 1.14 ms = 0.31% latency, 0 FLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 133.04 us = 0.04% latency, 137.93 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 329.26 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 329.26 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (10): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.85 GMACs = 1.24% MACs, 11.38 ms = 3.05% latency, 2.44 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.74 GMACs = 1.23% MACs, 4.83 ms = 1.29% latency, 5.69 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 188.74 MMACs = 0.02% MACs, 1.42 ms = 0.38% latency, 266.23 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 146.8 MMACs = 0.01% MACs, 1.04 ms = 0.28% latency, 281.93 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.8 us = 0.02% latency, 637.4 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.8 us = 0.02% latency, 637.4 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 52.69 us = 0.01% latency, 796.03 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 52.69 us = 0.01% latency, 796.03 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 62.91 MMACs = 0.01% MACs, 1.16 ms = 0.31% latency, 108.53 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 36.7 MMACs = 0% MACs, 836.61 us = 0.22% latency, 87.74 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.24 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.24 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.18 us = 0.02% latency, 663.86 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.18 us = 0.02% latency, 663.86 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.74 us = 0.01% latency, 202.67 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.74 us = 0.01% latency, 202.67 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 62.91 MMACs = 0.01% MACs, 1.17 ms = 0.31% latency, 107.91 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 36.7 MMACs = 0% MACs, 841.86 us = 0.23% latency, 87.19 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 71.76 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 71.76 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.13 us = 0.02% latency, 653.98 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.13 us = 0.02% latency, 653.98 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.74 us = 0.01% latency, 202.67 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.74 us = 0.01% latency, 202.67 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 555.28 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 115.34 MMACs = 0.01% MACs, 5.67 ms = 1.52% latency, 43.89 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.85 ms = 0.5% latency, 59.52 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.87 ms = 0.5% latency, 58.92 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 5.24 MMACs = 0% MACs, 1.55 ms = 0.41% latency, 6.77 GFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 132.32 us = 0.04% latency, 138.68 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 329.49 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 329.97 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (11): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.82 GMACs = 1.24% MACs, 11.03 ms = 2.95% latency, 2.51 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.71 GMACs = 1.23% MACs, 4.89 ms = 1.31% latency, 5.61 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 173.02 MMACs = 0.02% MACs, 1.44 ms = 0.38% latency, 240.85 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 131.07 MMACs = 0.01% MACs, 1.06 ms = 0.28% latency, 248.36 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.43 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.43 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 66.28 us = 0.02% latency, 632.81 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 66.28 us = 0.02% latency, 632.81 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 54.12 us = 0.01% latency, 774.99 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 54.12 us = 0.01% latency, 774.99 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 58.98 MMACs = 0.01% MACs, 1.18 ms = 0.32% latency, 99.87 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 32.77 MMACs = 0% MACs, 858.78 us = 0.23% latency, 76.31 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 71.76 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 71.76 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 62.94 us = 0.02% latency, 666.37 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 62.94 us = 0.02% latency, 666.37 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.21 us = 0.01% latency, 200.82 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.21 us = 0.01% latency, 200.82 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 58.98 MMACs = 0.01% MACs, 1.16 ms = 0.31% latency, 101.39 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 32.77 MMACs = 0% MACs, 842.09 us = 0.23% latency, 77.83 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.24 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.24 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.18 us = 0.02% latency, 663.86 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.18 us = 0.02% latency, 663.86 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.21 us = 0.01% latency, 200.82 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.21 us = 0.01% latency, 200.82 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 564.34 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 110.1 MMACs = 0.01% MACs, 5.26 ms = 1.41% latency, 45.34 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.85 ms = 0.5% latency, 59.47 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.85 ms = 0.49% latency, 59.55 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 0 MACs = 0% MACs, 1.15 ms = 0.31% latency, 0 FLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 132.56 us = 0.04% latency, 138.43 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 328.06 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 327.83 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (12): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.79 GMACs = 1.23% MACs, 11 ms = 2.95% latency, 2.51 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.68 GMACs = 1.23% MACs, 4.84 ms = 1.3% latency, 5.65 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 152.04 MMACs = 0.01% MACs, 1.43 ms = 0.38% latency, 212.29 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 110.1 MMACs = 0.01% MACs, 1.05 ms = 0.28% latency, 209 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.91 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.91 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.33 us = 0.02% latency, 642.05 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.33 us = 0.02% latency, 642.05 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 52.21 us = 0.01% latency, 803.3 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 52.21 us = 0.01% latency, 803.3 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 53.74 MMACs = 0% MACs, 1.18 ms = 0.32% latency, 90.74 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 27.53 MMACs = 0% MACs, 859.26 us = 0.23% latency, 64.07 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.61 us = 0.02% latency, 649.16 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.61 us = 0.02% latency, 649.16 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.5 us = 0.01% latency, 203.61 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.5 us = 0.01% latency, 203.61 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 53.74 MMACs = 0% MACs, 1.16 ms = 0.31% latency, 92.83 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 27.53 MMACs = 0% MACs, 837.56 us = 0.22% latency, 65.73 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 71.53 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 71.53 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.18 us = 0.02% latency, 663.86 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.18 us = 0.02% latency, 663.86 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.98 us = 0.01% latency, 201.75 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.98 us = 0.01% latency, 201.75 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 551.22 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 110.1 MMACs = 0.01% MACs, 5.28 ms = 1.41% latency, 45.17 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.88 ms = 0.5% latency, 58.66 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.86 ms = 0.5% latency, 59.3 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 0 MACs = 0% MACs, 1.14 ms = 0.31% latency, 0 FLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 132.8 us = 0.04% latency, 138.18 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 329.97 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 328.3 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (13): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.89 GMACs = 1.24% MACs, 11.08 ms = 2.97% latency, 2.51 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.78 GMACs = 1.23% MACs, 4.93 ms = 1.32% latency, 5.59 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 214.96 MMACs = 0.02% MACs, 1.43 ms = 0.38% latency, 301.44 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 173.02 MMACs = 0.02% MACs, 1.05 ms = 0.28% latency, 330.68 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.19 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.19 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.85 us = 0.02% latency, 646.77 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.85 us = 0.02% latency, 646.77 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 56.74 us = 0.02% latency, 739.17 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 56.74 us = 0.02% latency, 739.17 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 69.47 MMACs = 0.01% MACs, 1.26 ms = 0.34% latency, 110.16 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 43.25 MMACs = 0% MACs, 936.27 us = 0.25% latency, 92.4 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.33 us = 0.02% latency, 642.05 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.33 us = 0.02% latency, 642.05 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.21 us = 0.01% latency, 200.82 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.21 us = 0.01% latency, 200.82 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 69.47 MMACs = 0.01% MACs, 1.17 ms = 0.31% latency, 119.12 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 43.25 MMACs = 0% MACs, 845.19 us = 0.23% latency, 102.35 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.48 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.48 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 62.47 us = 0.02% latency, 671.46 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 62.47 us = 0.02% latency, 671.46 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.98 us = 0.01% latency, 201.75 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.98 us = 0.01% latency, 201.75 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 553.85 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 110.1 MMACs = 0.01% MACs, 5.27 ms = 1.41% latency, 45.23 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.87 ms = 0.5% latency, 58.92 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.85 ms = 0.5% latency, 59.51 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 0 MACs = 0% MACs, 1.14 ms = 0.31% latency, 0 FLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 132.56 us = 0.04% latency, 138.43 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 326.87 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 328.06 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (14): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.9 GMACs = 1.24% MACs, 11.06 ms = 2.96% latency, 2.51 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.79 GMACs = 1.23% MACs, 4.93 ms = 1.32% latency, 5.59 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 225.44 MMACs = 0.02% MACs, 1.46 ms = 0.39% latency, 308.96 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 183.5 MMACs = 0.02% MACs, 1.08 ms = 0.29% latency, 339.28 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.43 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.43 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.57 us = 0.02% latency, 639.72 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.57 us = 0.02% latency, 639.72 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 52.69 us = 0.01% latency, 796.03 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 52.69 us = 0.01% latency, 796.03 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 72.09 MMACs = 0.01% MACs, 1.21 ms = 0.32% latency, 119.3 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 45.88 MMACs = 0% MACs, 875.71 us = 0.23% latency, 104.77 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.61 us = 0.02% latency, 649.16 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.61 us = 0.02% latency, 649.16 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 59.13 us = 0.02% latency, 177.34 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 59.13 us = 0.02% latency, 177.34 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 72.09 MMACs = 0.01% MACs, 1.19 ms = 0.32% latency, 121.04 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 45.88 MMACs = 0% MACs, 870.7 us = 0.23% latency, 105.37 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.48 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.48 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.42 us = 0.02% latency, 661.36 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.42 us = 0.02% latency, 661.36 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.98 us = 0.01% latency, 201.75 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.98 us = 0.01% latency, 201.75 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 555.04 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 110.1 MMACs = 0.01% MACs, 5.25 ms = 1.41% latency, 45.44 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.85 ms = 0.49% latency, 59.67 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.85 ms = 0.5% latency, 59.45 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 0 MACs = 0% MACs, 1.14 ms = 0.31% latency, 0 FLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 133.99 us = 0.04% latency, 136.95 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 329.49 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 328.3 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (15): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.87 GMACs = 1.24% MACs, 10.94 ms = 2.93% latency, 2.54 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.76 GMACs = 1.23% MACs, 4.82 ms = 1.29% latency, 5.71 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 204.47 MMACs = 0.02% MACs, 1.42 ms = 0.38% latency, 288.52 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 162.53 MMACs = 0.01% MACs, 1.04 ms = 0.28% latency, 312.63 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.91 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.91 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.33 us = 0.02% latency, 642.05 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.33 us = 0.02% latency, 642.05 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 53.41 us = 0.01% latency, 785.37 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 53.41 us = 0.01% latency, 785.37 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 66.85 MMACs = 0.01% MACs, 1.16 ms = 0.31% latency, 115.03 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 40.63 MMACs = 0% MACs, 841.62 us = 0.23% latency, 96.56 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.66 us = 0.02% latency, 658.88 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.66 us = 0.02% latency, 658.88 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.26 us = 0.01% latency, 204.56 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.26 us = 0.01% latency, 204.56 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 66.85 MMACs = 0.01% MACs, 1.17 ms = 0.31% latency, 114.16 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 40.63 MMACs = 0% MACs, 849.25 us = 0.23% latency, 95.69 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.48 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.48 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.13 us = 0.02% latency, 653.98 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.13 us = 0.02% latency, 653.98 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.26 us = 0.01% latency, 204.56 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.26 us = 0.01% latency, 204.56 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 549.79 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 110.1 MMACs = 0.01% MACs, 5.25 ms = 1.4% latency, 45.48 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.85 ms = 0.49% latency, 59.62 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.85 ms = 0.5% latency, 59.38 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 0 MACs = 0% MACs, 1.14 ms = 0.31% latency, 0 FLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 131.61 us = 0.04% latency, 139.43 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 327.59 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 329.26 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (16): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.9 GMACs = 1.25% MACs, 11.48 ms = 3.07% latency, 2.42 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.78 GMACs = 1.23% MACs, 4.91 ms = 1.31% latency, 5.61 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 220.2 MMACs = 0.02% MACs, 1.45 ms = 0.39% latency, 304.46 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 178.26 MMACs = 0.02% MACs, 1.07 ms = 0.29% latency, 332.37 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.24 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.24 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.09 us = 0.02% latency, 644.4 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.09 us = 0.02% latency, 644.4 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 51.98 us = 0.01% latency, 806.98 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 51.98 us = 0.01% latency, 806.98 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 70.78 MMACs = 0.01% MACs, 1.2 ms = 0.32% latency, 118.23 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 44.56 MMACs = 0% MACs, 878.57 us = 0.24% latency, 101.45 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.48 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.48 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 62.47 us = 0.02% latency, 671.46 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 62.47 us = 0.02% latency, 671.46 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.02 us = 0.01% latency, 205.52 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.02 us = 0.01% latency, 205.52 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 70.78 MMACs = 0.01% MACs, 1.19 ms = 0.32% latency, 119.3 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 44.56 MMACs = 0% MACs, 864.27 us = 0.23% latency, 103.13 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.48 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.48 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.42 us = 0.02% latency, 661.36 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.42 us = 0.02% latency, 661.36 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.74 us = 0.01% latency, 202.67 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.74 us = 0.01% latency, 202.67 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 556.95 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 120.59 MMACs = 0.01% MACs, 5.69 ms = 1.52% latency, 45.65 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.86 ms = 0.5% latency, 59.29 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.85 ms = 0.5% latency, 59.4 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 10.49 MMACs = 0% MACs, 1.57 ms = 0.42% latency, 13.4 GFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 132.56 us = 0.04% latency, 138.43 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 331.16 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 328.3 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (17): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.85 GMACs = 1.24% MACs, 11.41 ms = 3.05% latency, 2.43 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.75 GMACs = 1.23% MACs, 4.87 ms = 1.3% latency, 5.65 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 199.23 MMACs = 0.02% MACs, 1.43 ms = 0.38% latency, 279.43 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 157.29 MMACs = 0.01% MACs, 1.05 ms = 0.28% latency, 300.21 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.09 us = 0.02% latency, 644.4 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.09 us = 0.02% latency, 644.4 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 52.45 us = 0.01% latency, 799.64 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 52.45 us = 0.01% latency, 799.64 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 65.54 MMACs = 0.01% MACs, 1.17 ms = 0.31% latency, 112.13 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 39.32 MMACs = 0% MACs, 847.1 us = 0.23% latency, 92.84 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 71.76 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 71.76 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.37 us = 0.02% latency, 651.56 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.37 us = 0.02% latency, 651.56 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.02 us = 0.01% latency, 205.52 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.02 us = 0.01% latency, 205.52 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 65.54 MMACs = 0.01% MACs, 1.19 ms = 0.32% latency, 110.44 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 39.32 MMACs = 0% MACs, 842.09 us = 0.23% latency, 93.39 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.24 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.24 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.37 us = 0.02% latency, 651.56 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.37 us = 0.02% latency, 651.56 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 56.03 us = 0.01% latency, 187.15 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 56.03 us = 0.01% latency, 187.15 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 552.89 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 94.37 MMACs = 0.01% MACs, 5.66 ms = 1.52% latency, 36.58 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 36.7 MMACs = 0% MACs, 1.85 ms = 0.49% latency, 39.74 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 36.7 MMACs = 0% MACs, 1.85 ms = 0.49% latency, 39.77 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 20.97 MMACs = 0% MACs, 1.56 ms = 0.42% latency, 26.81 GFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 132.32 us = 0.04% latency, 138.68 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 329.97 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 329.02 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (18): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.85 GMACs = 1.24% MACs, 11.33 ms = 3.03% latency, 2.45 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.77 GMACs = 1.23% MACs, 4.81 ms = 1.29% latency, 5.73 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 209.72 MMACs = 0.02% MACs, 1.42 ms = 0.38% latency, 296.21 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 167.77 MMACs = 0.02% MACs, 1.03 ms = 0.28% latency, 325.78 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 74.15 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 74.15 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 66.04 us = 0.02% latency, 635.1 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 66.04 us = 0.02% latency, 635.1 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 52.21 us = 0.01% latency, 803.3 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 52.21 us = 0.01% latency, 803.3 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 68.16 MMACs = 0.01% MACs, 1.16 ms = 0.31% latency, 117.21 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 41.94 MMACs = 0% MACs, 838.76 us = 0.22% latency, 100.01 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.66 us = 0.02% latency, 658.88 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.66 us = 0.02% latency, 658.88 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.21 us = 0.01% latency, 200.82 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.21 us = 0.01% latency, 200.82 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 68.16 MMACs = 0.01% MACs, 1.15 ms = 0.31% latency, 118.23 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 41.94 MMACs = 0% MACs, 831.6 us = 0.22% latency, 100.87 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 62.94 us = 0.02% latency, 666.37 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 62.94 us = 0.02% latency, 666.37 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.5 us = 0.01% latency, 203.61 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.5 us = 0.01% latency, 203.61 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 556.47 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 78.64 MMACs = 0.01% MACs, 5.64 ms = 1.51% latency, 31.14 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 36.7 MMACs = 0% MACs, 1.85 ms = 0.49% latency, 39.74 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 36.7 MMACs = 0% MACs, 1.85 ms = 0.49% latency, 39.75 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 5.24 MMACs = 0% MACs, 1.54 ms = 0.41% latency, 6.82 GFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 133.75 us = 0.04% latency, 137.19 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 327.59 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 329.97 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (19): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.86 GMACs = 1.24% MACs, 11.37 ms = 3.04% latency, 2.44 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.77 GMACs = 1.23% MACs, 4.81 ms = 1.29% latency, 5.73 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 209.72 MMACs = 0.02% MACs, 1.41 ms = 0.38% latency, 298.53 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 167.77 MMACs = 0.02% MACs, 1.03 ms = 0.27% latency, 327.07 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.43 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.43 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.85 us = 0.02% latency, 646.77 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.85 us = 0.02% latency, 646.77 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 52.21 us = 0.01% latency, 803.3 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 52.21 us = 0.01% latency, 803.3 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 68.16 MMACs = 0.01% MACs, 1.17 ms = 0.31% latency, 116.16 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 41.94 MMACs = 0% MACs, 849.96 us = 0.23% latency, 98.69 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.85 us = 0.02% latency, 646.77 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.85 us = 0.02% latency, 646.77 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.74 us = 0.01% latency, 202.67 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.74 us = 0.01% latency, 202.67 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 68.16 MMACs = 0.01% MACs, 1.16 ms = 0.31% latency, 117.69 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 41.94 MMACs = 0% MACs, 836.85 us = 0.22% latency, 100.24 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.24 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.24 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.66 us = 0.02% latency, 658.88 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.66 us = 0.02% latency, 658.88 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.26 us = 0.01% latency, 204.56 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.26 us = 0.01% latency, 204.56 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 554.8 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 89.13 MMACs = 0.01% MACs, 5.68 ms = 1.52% latency, 34.6 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 36.7 MMACs = 0% MACs, 1.85 ms = 0.5% latency, 39.66 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 36.7 MMACs = 0% MACs, 1.85 ms = 0.49% latency, 39.74 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 15.73 MMACs = 0% MACs, 1.58 ms = 0.42% latency, 19.94 GFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 132.32 us = 0.04% latency, 138.68 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 328.78 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 329.02 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (20): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.87 GMACs = 1.24% MACs, 11.49 ms = 3.08% latency, 2.42 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.78 GMACs = 1.23% MACs, 4.91 ms = 1.31% latency, 5.61 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 214.96 MMACs = 0.02% MACs, 1.45 ms = 0.39% latency, 297.41 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 173.02 MMACs = 0.02% MACs, 1.07 ms = 0.29% latency, 323.75 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.91 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.91 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.33 us = 0.02% latency, 642.05 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.33 us = 0.02% latency, 642.05 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 52.45 us = 0.01% latency, 799.64 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 52.45 us = 0.01% latency, 799.64 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 69.47 MMACs = 0.01% MACs, 1.21 ms = 0.32% latency, 115.23 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 43.25 MMACs = 0% MACs, 879.29 us = 0.24% latency, 98.38 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.13 us = 0.02% latency, 653.98 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.13 us = 0.02% latency, 653.98 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.74 us = 0.01% latency, 202.67 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.74 us = 0.01% latency, 202.67 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 69.47 MMACs = 0.01% MACs, 1.19 ms = 0.32% latency, 116.88 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 43.25 MMACs = 0% MACs, 864.03 us = 0.23% latency, 100.12 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.91 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.91 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.42 us = 0.02% latency, 661.36 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.42 us = 0.02% latency, 661.36 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.74 us = 0.01% latency, 202.67 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.74 us = 0.01% latency, 202.67 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 552.42 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 94.37 MMACs = 0.01% MACs, 5.7 ms = 1.52% latency, 36.36 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 36.7 MMACs = 0% MACs, 1.87 ms = 0.5% latency, 39.28 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 36.7 MMACs = 0% MACs, 1.85 ms = 0.5% latency, 39.62 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 20.97 MMACs = 0% MACs, 1.57 ms = 0.42% latency, 26.74 GFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 132.32 us = 0.04% latency, 138.68 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 328.78 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 328.78 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (21): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.92 GMACs = 1.25% MACs, 11.51 ms = 3.08% latency, 2.42 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.79 GMACs = 1.23% MACs, 4.96 ms = 1.33% latency, 5.56 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 225.44 MMACs = 0.02% MACs, 1.45 ms = 0.39% latency, 310.03 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 183.5 MMACs = 0.02% MACs, 1.07 ms = 0.29% latency, 341.77 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.19 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.19 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 66.28 us = 0.02% latency, 632.81 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 66.28 us = 0.02% latency, 632.81 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 53.64 us = 0.01% latency, 781.87 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 53.64 us = 0.01% latency, 781.87 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 72.09 MMACs = 0.01% MACs, 1.21 ms = 0.32% latency, 119.3 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 45.88 MMACs = 0% MACs, 879.29 us = 0.24% latency, 104.35 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.48 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.48 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.85 us = 0.02% latency, 646.77 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.85 us = 0.02% latency, 646.77 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.69 us = 0.01% latency, 199.01 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.69 us = 0.01% latency, 199.01 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 72.09 MMACs = 0.01% MACs, 1.23 ms = 0.33% latency, 117.65 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 45.88 MMACs = 0% MACs, 887.87 us = 0.24% latency, 103.34 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.48 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.48 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 69.38 us = 0.02% latency, 604.54 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 69.38 us = 0.02% latency, 604.54 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 59.84 us = 0.02% latency, 175.22 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 59.84 us = 0.02% latency, 175.22 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 553.61 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 125.83 MMACs = 0.01% MACs, 5.67 ms = 1.52% latency, 47.61 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.85 ms = 0.5% latency, 59.52 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.85 ms = 0.5% latency, 59.5 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 15.73 MMACs = 0% MACs, 1.56 ms = 0.42% latency, 20.14 GFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 133.04 us = 0.04% latency, 137.93 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 328.54 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 327.35 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (22): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.89 GMACs = 1.24% MACs, 11.63 ms = 3.11% latency, 2.39 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.8 GMACs = 1.24% MACs, 5.08 ms = 1.36% latency, 5.43 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 230.69 MMACs = 0.02% MACs, 1.51 ms = 0.4% latency, 306.39 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 188.74 MMACs = 0.02% MACs, 1.12 ms = 0.3% latency, 338.17 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 77.49 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 77.49 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 66.76 us = 0.02% latency, 628.29 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 66.76 us = 0.02% latency, 628.29 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 53.17 us = 0.01% latency, 788.89 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 53.17 us = 0.01% latency, 788.89 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 73.4 MMACs = 0.01% MACs, 1.26 ms = 0.34% latency, 116.07 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 47.19 MMACs = 0% MACs, 933.17 us = 0.25% latency, 101.13 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 71.76 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 71.76 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.37 us = 0.02% latency, 651.56 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.37 us = 0.02% latency, 651.56 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 60.32 us = 0.02% latency, 173.84 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 60.32 us = 0.02% latency, 173.84 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 73.4 MMACs = 0.01% MACs, 1.23 ms = 0.33% latency, 119.07 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 47.19 MMACs = 0% MACs, 911.24 us = 0.24% latency, 103.56 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 62.7 us = 0.02% latency, 668.9 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 62.7 us = 0.02% latency, 668.9 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.5 us = 0.01% latency, 203.61 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.5 us = 0.01% latency, 203.61 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 552.18 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 89.13 MMACs = 0.01% MACs, 5.68 ms = 1.52% latency, 34.64 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 36.7 MMACs = 0% MACs, 1.84 ms = 0.49% latency, 39.8 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 36.7 MMACs = 0% MACs, 1.85 ms = 0.49% latency, 39.72 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 15.73 MMACs = 0% MACs, 1.57 ms = 0.42% latency, 20 GFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 133.51 us = 0.04% latency, 137.44 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 327.35 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 327.35 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (23): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.87 GMACs = 1.24% MACs, 11.43 ms = 3.06% latency, 2.43 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.78 GMACs = 1.23% MACs, 4.85 ms = 1.3% latency, 5.68 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 220.2 MMACs = 0.02% MACs, 1.43 ms = 0.38% latency, 308.89 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 178.26 MMACs = 0.02% MACs, 1.05 ms = 0.28% latency, 340.62 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.91 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.91 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.85 us = 0.02% latency, 646.77 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.85 us = 0.02% latency, 646.77 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 53.17 us = 0.01% latency, 788.89 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 53.17 us = 0.01% latency, 788.89 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 70.78 MMACs = 0.01% MACs, 1.18 ms = 0.32% latency, 120.29 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 44.56 MMACs = 0% MACs, 851.63 us = 0.23% latency, 104.66 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.19 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.19 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.9 us = 0.02% latency, 656.42 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.9 us = 0.02% latency, 656.42 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.74 us = 0.01% latency, 202.67 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.74 us = 0.01% latency, 202.67 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 70.78 MMACs = 0.01% MACs, 1.17 ms = 0.31% latency, 120.87 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 44.56 MMACs = 0% MACs, 851.87 us = 0.23% latency, 104.63 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.18 us = 0.02% latency, 663.86 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.18 us = 0.02% latency, 663.86 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.26 us = 0.01% latency, 204.56 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.26 us = 0.01% latency, 204.56 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 556.47 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 89.13 MMACs = 0.01% MACs, 5.7 ms = 1.53% latency, 34.51 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 36.7 MMACs = 0% MACs, 1.85 ms = 0.5% latency, 39.57 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 36.7 MMACs = 0% MACs, 1.85 ms = 0.5% latency, 39.59 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 15.73 MMACs = 0% MACs, 1.58 ms = 0.42% latency, 19.95 GFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 133.28 us = 0.04% latency, 137.68 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 329.97 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 329.49 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (24): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.93 GMACs = 1.25% MACs, 11.51 ms = 3.08% latency, 2.42 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.8 GMACs = 1.24% MACs, 4.94 ms = 1.32% latency, 5.59 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 230.69 MMACs = 0.02% MACs, 1.45 ms = 0.39% latency, 317.55 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 188.74 MMACs = 0.02% MACs, 1.07 ms = 0.29% latency, 351.3 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.19 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.19 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.85 us = 0.02% latency, 646.77 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.85 us = 0.02% latency, 646.77 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 52.21 us = 0.01% latency, 803.3 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 52.21 us = 0.01% latency, 803.3 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 73.4 MMACs = 0.01% MACs, 1.21 ms = 0.32% latency, 121.78 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 47.19 MMACs = 0% MACs, 882.86 us = 0.24% latency, 106.89 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.66 us = 0.02% latency, 658.88 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.66 us = 0.02% latency, 658.88 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.69 us = 0.01% latency, 199.01 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.69 us = 0.01% latency, 199.01 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 73.4 MMACs = 0.01% MACs, 1.2 ms = 0.32% latency, 121.95 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 47.19 MMACs = 0% MACs, 882.15 us = 0.24% latency, 106.98 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.37 us = 0.02% latency, 651.56 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.37 us = 0.02% latency, 651.56 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.98 us = 0.01% latency, 201.75 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.98 us = 0.01% latency, 201.75 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 556.71 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 131.07 MMACs = 0.01% MACs, 5.7 ms = 1.53% latency, 49.23 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.86 ms = 0.5% latency, 59.26 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.86 ms = 0.5% latency, 59.23 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 20.97 MMACs = 0% MACs, 1.57 ms = 0.42% latency, 26.72 GFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 133.75 us = 0.04% latency, 137.19 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 329.02 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 328.78 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (25): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.89 GMACs = 1.24% MACs, 11.42 ms = 3.06% latency, 2.43 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.75 GMACs = 1.23% MACs, 4.84 ms = 1.3% latency, 5.68 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 199.23 MMACs = 0.02% MACs, 1.42 ms = 0.38% latency, 280.13 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 157.29 MMACs = 0.01% MACs, 1.05 ms = 0.28% latency, 300.89 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.43 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.43 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.09 us = 0.02% latency, 644.4 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.09 us = 0.02% latency, 644.4 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 52.69 us = 0.01% latency, 796.03 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 52.69 us = 0.01% latency, 796.03 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 65.54 MMACs = 0.01% MACs, 1.17 ms = 0.31% latency, 111.99 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 39.32 MMACs = 0% MACs, 850.68 us = 0.23% latency, 92.45 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.42 us = 0.02% latency, 661.36 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.42 us = 0.02% latency, 661.36 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.02 us = 0.01% latency, 205.52 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.02 us = 0.01% latency, 205.52 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 65.54 MMACs = 0.01% MACs, 1.18 ms = 0.31% latency, 111.53 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 39.32 MMACs = 0% MACs, 841.38 us = 0.23% latency, 93.47 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 73.67 us = 0.02% latency, 569.33 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 73.67 us = 0.02% latency, 569.33 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.69 us = 0.01% latency, 199.01 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.69 us = 0.01% latency, 199.01 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 554.8 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 141.56 MMACs = 0.01% MACs, 5.69 ms = 1.52% latency, 52.97 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.85 ms = 0.5% latency, 59.38 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.87 ms = 0.5% latency, 59.03 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 31.46 MMACs = 0% MACs, 1.56 ms = 0.42% latency, 40.23 GFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 133.99 us = 0.04% latency, 136.95 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 341.18 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 329.26 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (26): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.94 GMACs = 1.25% MACs, 11.7 ms = 3.13% latency, 2.38 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.76 GMACs = 1.23% MACs, 5.12 ms = 1.37% latency, 5.37 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 204.47 MMACs = 0.02% MACs, 1.52 ms = 0.41% latency, 268.8 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 162.53 MMACs = 0.01% MACs, 1.14 ms = 0.3% latency, 285.59 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 74.63 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 74.63 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 67.23 us = 0.02% latency, 623.84 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 67.23 us = 0.02% latency, 623.84 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 53.88 us = 0.01% latency, 778.42 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 53.88 us = 0.01% latency, 778.42 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 66.85 MMACs = 0.01% MACs, 1.26 ms = 0.34% latency, 106.24 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 40.63 MMACs = 0% MACs, 929.83 us = 0.25% latency, 87.4 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.67 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.67 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.09 us = 0.02% latency, 644.4 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.09 us = 0.02% latency, 644.4 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 53.17 us = 0.01% latency, 197.22 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 53.17 us = 0.01% latency, 197.22 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 66.85 MMACs = 0.01% MACs, 1.26 ms = 0.34% latency, 105.8 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 40.63 MMACs = 0% MACs, 927.21 us = 0.25% latency, 87.64 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.19 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.19 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.09 us = 0.02% latency, 644.4 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.09 us = 0.02% latency, 644.4 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.98 us = 0.01% latency, 201.75 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.98 us = 0.01% latency, 201.75 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 557.18 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 183.5 MMACs = 0.02% MACs, 5.7 ms = 1.53% latency, 67.56 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.85 ms = 0.5% latency, 59.43 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.87 ms = 0.5% latency, 58.81 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 73.4 MMACs = 0.01% MACs, 1.57 ms = 0.42% latency, 93.36 GFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 132.8 us = 0.04% latency, 138.18 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 327.83 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 327.11 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (27): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.93 GMACs = 1.25% MACs, 11.42 ms = 3.06% latency, 2.44 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.74 GMACs = 1.23% MACs, 4.86 ms = 1.3% latency, 5.66 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 188.74 MMACs = 0.02% MACs, 1.43 ms = 0.38% latency, 264.28 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 146.8 MMACs = 0.01% MACs, 1.05 ms = 0.28% latency, 279.3 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.24 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.24 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 66.28 us = 0.02% latency, 632.81 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 66.28 us = 0.02% latency, 632.81 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 52.93 us = 0.01% latency, 792.44 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 52.93 us = 0.01% latency, 792.44 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 62.91 MMACs = 0.01% MACs, 1.18 ms = 0.32% latency, 106.71 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 36.7 MMACs = 0% MACs, 854.25 us = 0.23% latency, 85.92 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.09 us = 0.02% latency, 644.4 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.09 us = 0.02% latency, 644.4 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 50.78 us = 0.01% latency, 206.48 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 50.78 us = 0.01% latency, 206.48 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 62.91 MMACs = 0.01% MACs, 1.17 ms = 0.31% latency, 107.71 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 36.7 MMACs = 0% MACs, 842.57 us = 0.23% latency, 87.11 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.91 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.91 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.66 us = 0.02% latency, 658.88 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.66 us = 0.02% latency, 658.88 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.45 us = 0.01% latency, 199.91 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.45 us = 0.01% latency, 199.91 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 559.33 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 193.99 MMACs = 0.02% MACs, 5.69 ms = 1.52% latency, 71.45 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.85 ms = 0.5% latency, 59.39 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.86 ms = 0.5% latency, 59.26 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 83.89 MMACs = 0.01% MACs, 1.56 ms = 0.42% latency, 107.24 GFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 133.51 us = 0.04% latency, 137.44 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 329.02 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 327.11 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (28): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.98 GMACs = 1.25% MACs, 11.52 ms = 3.08% latency, 2.43 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.74 GMACs = 1.23% MACs, 4.94 ms = 1.32% latency, 5.56 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 188.74 MMACs = 0.02% MACs, 1.46 ms = 0.39% latency, 259.43 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 146.8 MMACs = 0.01% MACs, 1.08 ms = 0.29% latency, 272.45 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.43 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.43 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.85 us = 0.02% latency, 646.77 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.85 us = 0.02% latency, 646.77 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 52.45 us = 0.01% latency, 799.64 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 52.45 us = 0.01% latency, 799.64 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 62.91 MMACs = 0.01% MACs, 1.21 ms = 0.32% latency, 104.24 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 36.7 MMACs = 0% MACs, 880.96 us = 0.24% latency, 83.32 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.67 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.67 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.61 us = 0.02% latency, 649.16 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.61 us = 0.02% latency, 649.16 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.45 us = 0.01% latency, 199.91 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 52.45 us = 0.01% latency, 199.91 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 62.91 MMACs = 0.01% MACs, 1.2 ms = 0.32% latency, 105.07 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 36.7 MMACs = 0% MACs, 873.57 us = 0.23% latency, 84.02 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.48 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.48 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.9 us = 0.02% latency, 656.42 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.9 us = 0.02% latency, 656.42 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.74 us = 0.01% latency, 202.67 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.74 us = 0.01% latency, 202.67 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 557.66 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 241.17 MMACs = 0.02% MACs, 5.7 ms = 1.53% latency, 87.78 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.86 ms = 0.5% latency, 59.31 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.86 ms = 0.5% latency, 59.07 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 131.07 MMACs = 0.01% MACs, 1.57 ms = 0.42% latency, 166.67 GFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 132.8 us = 0.04% latency, 138.18 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 328.78 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 328.3 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (29): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 13.93 GMACs = 1.25% MACs, 11.48 ms = 3.07% latency, 2.43 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.68 GMACs = 1.23% MACs, 4.88 ms = 1.31% latency, 5.61 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 152.04 MMACs = 0.01% MACs, 1.47 ms = 0.39% latency, 207.42 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 110.1 MMACs = 0.01% MACs, 1.06 ms = 0.28% latency, 206.85 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 75.58 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 75.58 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 66.76 us = 0.02% latency, 628.29 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 66.76 us = 0.02% latency, 628.29 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.66 us = 0.02% latency, 658.88 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.66 us = 0.02% latency, 658.88 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 53.74 MMACs = 0% MACs, 1.17 ms = 0.31% latency, 91.5 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 27.53 MMACs = 0% MACs, 846.86 us = 0.23% latency, 65 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.91 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.91 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.33 us = 0.02% latency, 642.05 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 65.33 us = 0.02% latency, 642.05 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 53.41 us = 0.01% latency, 196.34 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 53.41 us = 0.01% latency, 196.34 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 53.74 MMACs = 0% MACs, 1.16 ms = 0.31% latency, 92.47 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 27.53 MMACs = 0% MACs, 840.9 us = 0.23% latency, 65.47 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.91 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.91 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.37 us = 0.02% latency, 651.56 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.37 us = 0.02% latency, 651.56 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 50.07 us = 0.01% latency, 209.43 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 50.07 us = 0.01% latency, 209.43 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 557.9 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 246.42 MMACs = 0.02% MACs, 5.72 ms = 1.53% latency, 89.32 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.85 ms = 0.5% latency, 59.39 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.86 ms = 0.5% latency, 59.05 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 136.31 MMACs = 0.01% MACs, 1.59 ms = 0.43% latency, 171.08 GFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 133.51 us = 0.04% latency, 137.44 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 328.78 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 329.49 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (30): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 14.1 GMACs = 1.26% MACs, 11.49 ms = 3.07% latency, 2.46 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.72 GMACs = 1.23% MACs, 4.85 ms = 1.3% latency, 5.66 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 178.26 MMACs = 0.02% MACs, 1.44 ms = 0.38% latency, 248.35 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 136.31 MMACs = 0.01% MACs, 1.04 ms = 0.28% latency, 261.91 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 77.25 us = 0.02% latency, 542.97 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 77.25 us = 0.02% latency, 542.97 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 55.79 us = 0.01% latency, 751.8 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 55.79 us = 0.01% latency, 751.8 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 60.29 MMACs = 0.01% MACs, 1.19 ms = 0.32% latency, 101.72 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 34.08 MMACs = 0% MACs, 860.93 us = 0.23% latency, 79.17 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.61 us = 0.02% latency, 649.16 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.61 us = 0.02% latency, 649.16 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.26 us = 0.01% latency, 204.56 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.26 us = 0.01% latency, 204.56 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 60.29 MMACs = 0.01% MACs, 1.16 ms = 0.31% latency, 103.88 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 34.08 MMACs = 0% MACs, 839.23 us = 0.22% latency, 81.21 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.48 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.48 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.9 us = 0.02% latency, 656.42 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 63.9 us = 0.02% latency, 656.42 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.5 us = 0.01% latency, 203.61 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.5 us = 0.01% latency, 203.61 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 0 MACs = 0% MACs, 554.56 us = 0.15% latency, 0 FLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 382.73 MMACs = 0.03% MACs, 5.75 ms = 1.54% latency, 136.24 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.86 ms = 0.5% latency, 59.05 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.88 ms = 0.5% latency, 58.48 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 272.63 MMACs = 0.02% MACs, 1.59 ms = 0.43% latency, 343.18 GFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 133.04 us = 0.04% latency, 137.93 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 328.3 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 329.26 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
          (31): LlamaDecoderLayer(
            218.19 M = 2.72% Params, 15.9 GMACs = 1.42% MACs, 12.06 ms = 3.23% latency, 2.64 TFLOPS
            (self_attn): LlamaSdpaAttention(
              42.02 M = 0.52% Params, 13.55 GMACs = 1.21% MACs, 5.3 ms = 1.42% latency, 5.11 TFLOPS
              (q_proj): lora.Linear8bitLt(
                16.81 M = 0.21% Params, 57.67 MMACs = 0.01% MACs, 1.41 ms = 0.38% latency, 81.57 GFLOPS
                (base_layer): Linear8bitLt(16.78 M = 0.21% Params, 15.73 MMACs = 0% MACs, 1.03 ms = 0.28% latency, 30.41 GFLOPS, in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 73.43 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 73.43 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 66.52 us = 0.02% latency, 630.54 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 66.52 us = 0.02% latency, 630.54 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 52.45 us = 0.01% latency, 799.64 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 52.45 us = 0.01% latency, 799.64 GFLOPS, in_features=4, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 30.15 MMACs = 0% MACs, 1.18 ms = 0.32% latency, 51.09 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 3.93 MMACs = 0% MACs, 857.59 us = 0.23% latency, 9.17 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.96 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.37 us = 0.02% latency, 651.56 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.37 us = 0.02% latency, 651.56 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.5 us = 0.01% latency, 203.61 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.5 us = 0.01% latency, 203.61 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear8bitLt(
                4.21 M = 0.05% Params, 30.15 MMACs = 0% MACs, 1.16 ms = 0.31% latency, 51.98 GFLOPS
                (base_layer): Linear8bitLt(4.19 M = 0.05% Params, 3.93 MMACs = 0% MACs, 838.76 us = 0.22% latency, 9.38 GFLOPS, in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.02% latency, 0 FLOPS
                  (default): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 72.72 us = 0.02% latency, 0 FLOPS, p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.13 us = 0.02% latency, 653.98 GFLOPS
                  (default): Linear(16.38 K = 0% Params, 20.97 MMACs = 0% MACs, 64.13 us = 0.02% latency, 653.98 GFLOPS, in_features=4096, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.5 us = 0.01% latency, 203.61 GFLOPS
                  (default): Linear(4.1 K = 0% Params, 5.24 MMACs = 0% MACs, 51.5 us = 0.01% latency, 203.61 GFLOPS, in_features=4, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): Linear8bitLt(16.78 M = 0.21% Params, 10.49 MMACs = 0% MACs, 1.03 ms = 0.28% latency, 20.4 GFLOPS, in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): LlamaMLP(
              176.16 M = 2.19% Params, 2.35 GMACs = 0.21% MACs, 5.88 ms = 1.57% latency, 801.86 GFLOPS
              (gate_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.85 ms = 0.5% latency, 59.48 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear8bitLt(58.72 M = 0.73% Params, 55.05 MMACs = 0% MACs, 1.86 ms = 0.5% latency, 59.19 GFLOPS, in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear8bitLt(58.72 M = 0.73% Params, 2.24 GMACs = 0.2% MACs, 1.76 ms = 0.47% latency, 2.54 TFLOPS, in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 132.56 us = 0.04% latency, 138.43 GFLOPS)
            )
            (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 328.78 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 329.73 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 331.16 us = 0.09% latency, 0 FLOPS, (4096,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 81.92 KMACs = 0% MACs, 752.93 us = 0.2% latency, 217.6 MFLOPS)
      )
      (lm_head): Linear(525.34 M = 6.54% Params, 672.43 GMACs = 60.21% MACs, 10.06 ms = 2.69% latency, 133.75 TFLOPS, in_features=4096, out_features=128256, bias=False)
    )
  )
)
------------------------------------------------------------------------------
