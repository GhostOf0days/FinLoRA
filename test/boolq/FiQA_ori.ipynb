{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/xfs/home/tensor_zy/guoxuan/Task3/ChatGLM/inferencing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score,f1_score,roc_auc_score,recall_score,precision_score, auc\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json, torch\n",
    "\n",
    "from cover_alpaca2jsonl import format_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-26 19:27:43,557] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /xfs/home/tensor_zy/anaconda3/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda110.so\n",
      "CUDA SETUP: CUDA runtime path found: /xfs/home/tensor_zy/anaconda3/envs/isaacgym/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 110\n",
      "CUDA SETUP: Loading binary /xfs/home/tensor_zy/anaconda3/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda110.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e84f5e0d004e0db6149b6f19c41d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"THUDM/chatglm2-6b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "# model = AutoModel.from_pretrained(model_path, trust_remote_code=True, load_in_8bit=True, device = \"cuda\")\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True, load_in_8bit=True, device_map = \"auto\")\n",
    "# model = AutoModel.from_pretrained(model_path, trust_remote_code=True, device_map = \"auto\")\n",
    "model = torch.compile(model)\n",
    "model = model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset boolq (/xfs/home/tensor_zy/.cache/huggingface/datasets/boolq/default/0.1.0/bf0dd57da941c50de94ae3ce3cef7fea48c08f337a4b7aac484e9dddc5aa24e5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908b0038a85c4a56841c4aa3b9d77046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import datasets\n",
    "\n",
    "dataset = load_dataset(\"boolq\")\n",
    "dataset = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>passage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>does ethanol take more energy make that produces</td>\n",
       "      <td>False</td>\n",
       "      <td>All biomass goes through at least some of thes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is house tax and property tax are same</td>\n",
       "      <td>True</td>\n",
       "      <td>Property tax or 'house tax' is a local tax on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question answer  \\\n",
       "0  does ethanol take more energy make that produces  False   \n",
       "1            is house tax and property tax are same   True   \n",
       "\n",
       "                                             passage  \n",
       "0  All biomass goes through at least some of thes...  \n",
       "1  Property tax or 'house tax' is a local tax on ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.to_pandas()\n",
    "dataset['answer'] = dataset['answer'].astype('str')\n",
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns = ['instruction', 'output', 'input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>output</th>\n",
       "      <th>input</th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>does ethanol take more energy make that produces</td>\n",
       "      <td>False</td>\n",
       "      <td>All biomass goes through at least some of thes...</td>\n",
       "      <td>Instruction: does ethanol take more energy mak...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is house tax and property tax are same</td>\n",
       "      <td>True</td>\n",
       "      <td>Property tax or 'house tax' is a local tax on ...</td>\n",
       "      <td>Instruction: is house tax and property tax are...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        instruction output  \\\n",
       "0  does ethanol take more energy make that produces  False   \n",
       "1            is house tax and property tax are same   True   \n",
       "\n",
       "                                               input  \\\n",
       "0  All biomass goes through at least some of thes...   \n",
       "1  Property tax or 'house tax' is a local tax on ...   \n",
       "\n",
       "                                             context target  \n",
       "0  Instruction: does ethanol take more energy mak...  False  \n",
       "1  Instruction: is house tax and property tax are...   True  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[[\"context\",\"target\"]] = dataset.apply(format_example, axis = 1, result_type=\"expand\")\n",
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3270"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = dataset['context'].tolist()\n",
    "len(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "total_steps = dataset.shape[0]//batch_size\n",
    "total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/103 [00:10<17:12, 10.12s/it]Input length of input_ids is 791, but `max_length` is set to 512. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  8%|▊         | 8/103 [00:50<08:38,  5.46s/it]Input length of input_ids is 563, but `max_length` is set to 512. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      " 17%|█▋        | 17/103 [01:34<07:00,  4.89s/it]Input length of input_ids is 672, but `max_length` is set to 512. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      " 28%|██▊       | 29/103 [03:06<14:21, 11.64s/it]Input length of input_ids is 615, but `max_length` is set to 512. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      " 37%|███▋      | 38/103 [04:50<08:16,  7.64s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 566.00 MiB (GPU 0; 39.59 GiB total capacity; 11.56 GiB already allocated; 155.19 MiB free; 14.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1e65190b9fcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mres_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mout_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres_sentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m             \u001b[0;31m# 11. run greedy search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m             return self.greedy_search(\n\u001b[0m\u001b[1;32m   1523\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgreedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2338\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2339\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2340\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2341\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b14463edd62620ce9f/modeling_chatglm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, return_last_logit)\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m    933\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b14463edd62620ce9f/modeling_chatglm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, position_ids, attention_mask, full_attention_mask, past_key_values, inputs_embeds, use_cache, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0;31m# Run encoder.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m         hidden_states, presents, all_hidden_states, all_self_attentions = self.encoder(\n\u001b[0m\u001b[1;32m    829\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotary_pos_emb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrotary_pos_emb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m             \u001b[0mkv_caches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b14463edd62620ce9f/modeling_chatglm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, rotary_pos_emb, kv_caches, use_cache, output_hidden_states)\u001b[0m\n\u001b[1;32m    636\u001b[0m                 )\n\u001b[1;32m    637\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m                 layer_ret = layer(\n\u001b[0m\u001b[1;32m    639\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b14463edd62620ce9f/modeling_chatglm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, rotary_pos_emb, kv_cache, use_cache)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;31m# MLP.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m         \u001b[0mmlp_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayernorm_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;31m# Second residual connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b14463edd62620ce9f/modeling_chatglm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;31m# [s, b, 4hp]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         \u001b[0mintermediate_parallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_h_to_4h\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m         \u001b[0mintermediate_parallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_parallel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;31m# [s, b, h]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_fp16_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(A, B, out, state, threshold, bias)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mMatMul8bitLt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_context\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_SingleLevelFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_context\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, A, B, out, bias, state)\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;31m# 4. Mixed-precision decomposition matmul\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcoo_tensorA\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msubA\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;31m# 5. Save state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 566.00 MiB (GPU 0; 39.59 GiB total capacity; 11.56 GiB already allocated; 155.19 MiB free; 14.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "res_list = []\n",
    "res_sentences_list = []\n",
    "out_text_list = []\n",
    "\n",
    "for i in tqdm(range(total_steps+1)):\n",
    "    tmp_context = context[i* batch_size:(i+1)* batch_size]\n",
    "    tokens = tokenizer(tmp_context, return_tensors='pt', padding=True, max_length=512)\n",
    "    for k in tokens.keys():\n",
    "        tokens[k] = tokens[k].cuda()\n",
    "    \n",
    "    res = model.generate(**tokens, max_length=512)\n",
    "    res_sentences = [tokenizer.decode(i) for i in res]\n",
    "    out_text = [o.split(\"Answer: \")[1] for o in res_sentences]\n",
    "    res_list += res\n",
    "    res_sentences_list += res_sentences\n",
    "    out_text_list += out_text\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_list = [i.cpu() for i in res_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"res\"] = res_list\n",
    "dataset[\"res_sentences\"] = res_sentences_list\n",
    "dataset[\"out_text\"] = out_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.34 units of fuel energy for each unit of energy expended',\n",
       " ' yes',\n",
       " '1. Yes.',\n",
       " ' yes',\n",
       " '1',\n",
       " ' no',\n",
       " '1',\n",
       " '71',\n",
       " '1.',\n",
       " '1.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_text_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.34 units of fuel energy for each unit of energy expended',\n",
       " ' yes',\n",
       " '1. Yes.',\n",
       " ' yes',\n",
       " '1',\n",
       " ' no',\n",
       " '1',\n",
       " '71',\n",
       " '1.',\n",
       " '1.',\n",
       " '1. Yes.',\n",
       " '2',\n",
       " '0',\n",
       " ' no',\n",
       " '2016 fantasy film directed by David Yates. A joint British and American production, it is a spin-off and prequel to the Harry Potter film series, and it was produced and written by J.K. Rowling in her screenwriting debut, and inspired by her 2001 book of the same name.',\n",
       " '1. yes',\n",
       " '2.',\n",
       " ' yes',\n",
       " '2018',\n",
       " '1',\n",
       " '1598 Main Street Unit 9975',\n",
       " ' yes',\n",
       " ' no',\n",
       " ' yes',\n",
       " '7th generation Fire 7 is not the same as the Kindle.',\n",
       " '18 - 20 year olds can drink alcohol with their parents in Wisconsin if they are with a parent, legal guardian, or spouse who is of legal drinking age.',\n",
       " '1. yes',\n",
       " '2.',\n",
       " ' yes',\n",
       " '14 holes outside the track , along the backstretch, and four holes in the infield.',\n",
       " '1',\n",
       " ' yes',\n",
       " ' no',\n",
       " ' no',\n",
       " ' yes',\n",
       " '2',\n",
       " '2',\n",
       " ' yes',\n",
       " '1',\n",
       " '0',\n",
       " ' no',\n",
       " ' yes',\n",
       " '1',\n",
       " ' no',\n",
       " '1',\n",
       " '1',\n",
       " ' yes',\n",
       " '4',\n",
       " '2',\n",
       " '2',\n",
       " ' it',\n",
       " ' yes',\n",
       " ' Yes',\n",
       " '1',\n",
       " '2',\n",
       " ' yes',\n",
       " ' yes',\n",
       " ' yes',\n",
       " '1',\n",
       " ' yes',\n",
       " '1',\n",
       " '1',\n",
       " '5',\n",
       " ' yes',\n",
       " '13',\n",
       " ' yes',\n",
       " ' yes',\n",
       " '1',\n",
       " '1950',\n",
       " ' no',\n",
       " ' yes',\n",
       " '1.',\n",
       " '1.',\n",
       " '3',\n",
       " '1. Yes, there is such a thing as a floating island.',\n",
       " ' yes',\n",
       " ' yes',\n",
       " '0',\n",
       " '1000000.',\n",
       " '1',\n",
       " '1. It is not known if there will be a season 2 of 11.22.63. 2. King stated that he would like to revisit Jake and Sadie and the rabbit hole, but did not indicate if a season 2 will be made. 3. The answer is 2.',\n",
       " '99',\n",
       " '6--6',\n",
       " '2 World Trade Center',\n",
       " '1992',\n",
       " ' no',\n",
       " ' no',\n",
       " '1.',\n",
       " '1',\n",
       " '5-speed manual transmission',\n",
       " ' no',\n",
       " '3',\n",
       " '1.',\n",
       " '1. Yes.',\n",
       " '2',\n",
       " ' no',\n",
       " ' yes',\n",
       " '2,423,912.',\n",
       " '2006',\n",
       " ' no']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_text_list[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_target(x):\n",
    "    if 'yes' in x or \"1\" in x:\n",
    "        return \"True\"\n",
    "    elif 'no' in x or '0' in x:\n",
    "        return \"False\"\n",
    "    else:\n",
    "        return 'missing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataset[\"new_target\"] = dataset[\"target\"].apply(change_target)\n",
    "dataset[\"new_target\"] = dataset[\"target\"]\n",
    "dataset[\"new_target\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset[\"new_out\"] = dataset[\"out_text\"].apply(change_target)\n",
    "dataset[\"new_out\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5892966360856269"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = accuracy_score(dataset[\"new_target\"], dataset[\"new_out\"])\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39939477273190566"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = f1_score(dataset[\"new_target\"], dataset[\"new_out\"], average = \"macro\")\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5892966360856269"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = f1_score(dataset[\"new_target\"], dataset[\"new_out\"], average = \"micro\")\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6295356076439681"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = f1_score(dataset[\"new_target\"], dataset[\"new_out\"], average = \"weighted\")\n",
    "f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8327272727272728"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = accuracy_score(dataset[\"new_target\"], dataset[\"new_out\"])\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.569713523470749"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = f1_score(dataset[\"new_target\"], dataset[\"new_out\"], average = \"macro\")\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8327272727272728"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = f1_score(dataset[\"new_target\"], dataset[\"new_out\"], average = \"micro\")\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8010500482176782"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = f1_score(dataset[\"new_target\"], dataset[\"new_out\"], average = \"weighted\")\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dataset.to_csv(\"fiqa_ori.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
