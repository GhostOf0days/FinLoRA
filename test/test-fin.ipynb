{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e89015ab-3b42-4ad4-a272-29fdfce8c49b",
   "metadata": {},
   "source": [
    "# Testing flare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006b4476-915b-4cee-97d5-57bd782460cf",
   "metadata": {},
   "source": [
    "### Load Model\n",
    "Load the base model and finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97f8843f-289b-41f3-85d0-cbd8169457ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf8aae07e57462db5aeb60a554ca785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaForCausalLM\n",
    ")\n",
    "\n",
    "base_model = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "peft_model = \"../l-8b-r8-out\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True, padding_side='left')\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    # load_in_8bit=True,\n",
    "    # load_in_4bit=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map={\n",
    "    \"\": 0\n",
    "    },\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, peft_model)\n",
    "model = model.eval()\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0175a08c-f923-4494-b2f7-200c37bbb639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt example:\n",
      "Instruction: What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}.\n",
      "Input: L&T has also made a commitment to redeem the remaining shares by the end of 2011 .\n",
      "Answer: \n",
      "\n",
      "\n",
      "Total len: 1212. Batchsize: 8. Total steps: 152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                  | 0/152 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 152/152 [00:27<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.8597359735973598. F1 macro: 0.8521758625664301. F1 micro: 0.8597359735973598. F1 weighted (BloombergGPT): 0.8582934906818401. \n",
      "\n",
      "\n",
      "Prompt example:\n",
      "Instruction: What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}.\n",
      "Input: This $BBBY stock options trade would have more than doubled your money https://t.co/Oa0loiRIJL via @TheStreet\n",
      "Answer: \n",
      "\n",
      "\n",
      "Total len: 275. Batchsize: 8. Total steps: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:05<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.8654545454545455. F1 macro: 0.7658162488735493. F1 micro: 0.8654545454545455. F1 weighted (BloombergGPT): 0.879037623309444. \n",
      "\n",
      "\n",
      "Prompt example:\n",
      "Instruction: What is the sentiment of this tweet? Please choose an answer from {negative/neutral/positive}.\n",
      "Input: $ALLY - Ally Financial pulls outlook https://t.co/G9Zdi1boy5\n",
      "Answer: \n",
      "\n",
      "\n",
      "Total len: 2388. Batchsize: 8. Total steps: 299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 299/299 [00:49<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.8965661641541038. F1 macro: 0.873412785752509. F1 micro: 0.8965661641541038. F1 weighted (BloombergGPT): 0.8974006335116431. \n",
      "\n",
      "\n",
      "Prompt example:\n",
      "Instruction: What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}.\n",
      "Input: In the latest trading session, Adobe Systems (ADBE) closed at $535.98, marking a +0.31% move from the previous day.\n",
      "Answer: \n",
      "\n",
      "\n",
      "Total len: 4047. Batchsize: 8. Total steps: 506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 506/506 [01:40<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.6330615270570793. F1 macro: 0.6430639552603411. F1 micro: 0.6330615270570793. F1 weighted (BloombergGPT): 0.6311732986569045. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>instruction</th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>out_text</th>\n",
       "      <th>new_target</th>\n",
       "      <th>new_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the latest trading session, Adobe Systems (...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>What is the sentiment of this news? Please cho...</td>\n",
       "      <td>Instruction: What is the sentiment of this new...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive&lt;|end_of_text|&gt;</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tech stocks are down today after an antitrust ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>What is the sentiment of this news? Please cho...</td>\n",
       "      <td>Instruction: What is the sentiment of this new...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative&lt;|end_of_text|&gt;</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Intel Corp is committing $20 billion to build ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>What is the sentiment of this news? Please cho...</td>\n",
       "      <td>Instruction: What is the sentiment of this new...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive&lt;|end_of_text|&gt;</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High costs and supply chain disruptions are li...</td>\n",
       "      <td>negative</td>\n",
       "      <td>What is the sentiment of this news? Please cho...</td>\n",
       "      <td>Instruction: What is the sentiment of this new...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative&lt;|end_of_text|&gt;</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMD still seems set to generate significant gr...</td>\n",
       "      <td>positive</td>\n",
       "      <td>What is the sentiment of this news? Please cho...</td>\n",
       "      <td>Instruction: What is the sentiment of this new...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive&lt;|end_of_text|&gt;</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4042</th>\n",
       "      <td>Amazon.com Inc. AMZN, +1.08% has proposed on T...</td>\n",
       "      <td>negative</td>\n",
       "      <td>What is the sentiment of this news? Please cho...</td>\n",
       "      <td>Instruction: What is the sentiment of this new...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative&lt;|end_of_text|&gt;</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4043</th>\n",
       "      <td>Not everyone has thousands of dollars on hand ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>What is the sentiment of this news? Please cho...</td>\n",
       "      <td>Instruction: What is the sentiment of this new...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral&lt;|end_of_text|&gt;</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4044</th>\n",
       "      <td>Amazon has delivered strong advertising growth...</td>\n",
       "      <td>positive</td>\n",
       "      <td>What is the sentiment of this news? Please cho...</td>\n",
       "      <td>Instruction: What is the sentiment of this new...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive&lt;|end_of_text|&gt;</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4045</th>\n",
       "      <td>U.S. chip manufacturer SkyWater Technology Inc...</td>\n",
       "      <td>positive</td>\n",
       "      <td>What is the sentiment of this news? Please cho...</td>\n",
       "      <td>Instruction: What is the sentiment of this new...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive&lt;|end_of_text|&gt;</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4046</th>\n",
       "      <td>NVIDIA Corporation (NASDAQ: NVDA) shares are t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>What is the sentiment of this news? Please cho...</td>\n",
       "      <td>Instruction: What is the sentiment of this new...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative&lt;|end_of_text|&gt;</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4047 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  input    output  \\\n",
       "0     In the latest trading session, Adobe Systems (...   neutral   \n",
       "1     Tech stocks are down today after an antitrust ...  negative   \n",
       "2     Intel Corp is committing $20 billion to build ...  positive   \n",
       "3     High costs and supply chain disruptions are li...  negative   \n",
       "4     AMD still seems set to generate significant gr...  positive   \n",
       "...                                                 ...       ...   \n",
       "4042  Amazon.com Inc. AMZN, +1.08% has proposed on T...  negative   \n",
       "4043  Not everyone has thousands of dollars on hand ...   neutral   \n",
       "4044  Amazon has delivered strong advertising growth...  positive   \n",
       "4045  U.S. chip manufacturer SkyWater Technology Inc...  positive   \n",
       "4046  NVIDIA Corporation (NASDAQ: NVDA) shares are t...  negative   \n",
       "\n",
       "                                            instruction  \\\n",
       "0     What is the sentiment of this news? Please cho...   \n",
       "1     What is the sentiment of this news? Please cho...   \n",
       "2     What is the sentiment of this news? Please cho...   \n",
       "3     What is the sentiment of this news? Please cho...   \n",
       "4     What is the sentiment of this news? Please cho...   \n",
       "...                                                 ...   \n",
       "4042  What is the sentiment of this news? Please cho...   \n",
       "4043  What is the sentiment of this news? Please cho...   \n",
       "4044  What is the sentiment of this news? Please cho...   \n",
       "4045  What is the sentiment of this news? Please cho...   \n",
       "4046  What is the sentiment of this news? Please cho...   \n",
       "\n",
       "                                                context    target  \\\n",
       "0     Instruction: What is the sentiment of this new...   neutral   \n",
       "1     Instruction: What is the sentiment of this new...  negative   \n",
       "2     Instruction: What is the sentiment of this new...  positive   \n",
       "3     Instruction: What is the sentiment of this new...  negative   \n",
       "4     Instruction: What is the sentiment of this new...  positive   \n",
       "...                                                 ...       ...   \n",
       "4042  Instruction: What is the sentiment of this new...  negative   \n",
       "4043  Instruction: What is the sentiment of this new...   neutral   \n",
       "4044  Instruction: What is the sentiment of this new...  positive   \n",
       "4045  Instruction: What is the sentiment of this new...  positive   \n",
       "4046  Instruction: What is the sentiment of this new...  negative   \n",
       "\n",
       "                     out_text new_target   new_out  \n",
       "0     positive<|end_of_text|>    neutral  positive  \n",
       "1     negative<|end_of_text|>   negative  negative  \n",
       "2     positive<|end_of_text|>   positive  positive  \n",
       "3     negative<|end_of_text|>   negative  negative  \n",
       "4     positive<|end_of_text|>   positive  positive  \n",
       "...                       ...        ...       ...  \n",
       "4042  negative<|end_of_text|>   negative  negative  \n",
       "4043   neutral<|end_of_text|>    neutral   neutral  \n",
       "4044  positive<|end_of_text|>   positive  positive  \n",
       "4045  positive<|end_of_text|>   positive  positive  \n",
       "4046  negative<|end_of_text|>   negative  negative  \n",
       "\n",
       "[4047 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "from FinNLP.finnlp.benchmarks.fpb import test_fpb\n",
    "from FinNLP.finnlp.benchmarks.fiqa import test_fiqa , add_instructions\n",
    "from FinNLP.finnlp.benchmarks.tfns import test_tfns\n",
    "from FinNLP.finnlp.benchmarks.nwgi import test_nwgi\n",
    "batch_size = 8\n",
    "\n",
    "test_fpb(model, tokenizer, batch_size=batch_size)\n",
    "test_fiqa(model, tokenizer, batch_size=batch_size)\n",
    "test_tfns(model, tokenizer, batch_size=batch_size)\n",
    "test_nwgi(model, tokenizer, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61058dbc-2884-4cfb-ab50-6be4ebe244ac",
   "metadata": {},
   "source": [
    "### Testing code\n",
    "\n",
    "You can follow one of the files here: https://github.com/AI4Finance-Foundation/FinNLP/tree/main/finnlp/benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afce0dfa-2c5c-4f8f-ac5c-1c5dee850b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt example:\n",
      "Q:The nominal risk-free rate is best described as the sum of the real risk-free rate and a premium for:,CHOICES: A: maturity.,B: liquidity.,C: expected inflation.\n",
      "\n",
      "\n",
      "Total len: 1032. Batchsize: 14. Total steps: 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 74/74 [00:41<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.4903100775193798. F1 macro: 0.10296047869200162. F1 micro: 0.4903100775193798. F1 weighted (BloombergGPT): 0.4507126041643837. \n",
      "Acc: 0.4903100775193798. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "def parse_answer(text):\n",
    "  \"\"\"\n",
    "  Parses the answer from the model's output text without using regex.\n",
    "\n",
    "  Args:\n",
    "    text: The output text from the language model.\n",
    "\n",
    "  Returns:\n",
    "    The first letter of the answer, or None if no answer is found.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    # Find the position of \"Answer:\" in the text\n",
    "    answer_pos = text.index(\"Answer:\")\n",
    "\n",
    "    # Extract the first letter after \"Answer:\"\n",
    "    first_letter = text[answer_pos + len(\"Answer:\"):].strip()[0]\n",
    "\n",
    "    # Convert the letter to uppercase and return it\n",
    "    return first_letter\n",
    "\n",
    "  except ValueError:\n",
    "    # \"Answer:\" is not found in the text\n",
    "    return \"\"\n",
    "\n",
    "  except IndexError:\n",
    "    # No letter found after \"Answer:\"\n",
    "    return \"\"\n",
    "    \n",
    "    \n",
    "def test(model, tokenizer, batch_size = 14, prompt_fun = None ):\n",
    "    dataset = load_dataset('TheFinAI/flare-cfa')\n",
    "    dataset = dataset['test']\n",
    "    dataset = dataset.to_pandas()\n",
    "    # dataset = dataset.head(100)\n",
    "\n",
    "    # print example\n",
    "    print(f\"\\n\\nPrompt example:\\n{dataset['text'][0]}\\n\\n\")\n",
    "\n",
    "    context = dataset['text'].tolist()\n",
    "    context = [\"Instruction: What is the correct answer to this question? Please choose an answer from  {A/B/C}. \\nInput: \" + x + \"\\nAnswer:\"\n",
    "               for x in context]\n",
    "    total_steps = dataset.shape[0] // batch_size + 1\n",
    "    print(f\"Total len: {len(context)}. Batchsize: {batch_size}. Total steps: {total_steps}\")\n",
    "\n",
    "\n",
    "    out_text_list = []\n",
    "    for i in tqdm(range(total_steps)):\n",
    "        tmp_context = context[i* batch_size: min((i+1)* batch_size, len(context))]\n",
    "        tokens = tokenizer(tmp_context, return_tensors='pt', padding=True, padding_side=\"left\")\n",
    "        for k in tokens.keys():\n",
    "            tokens[k] = tokens[k].cuda()\n",
    "        res = model.generate(**tokens, max_length=300)\n",
    "        res_sentences = [tokenizer.decode(i, skip_special_tokens=True) for i in res]\n",
    "        # print(res_sentences[0])\n",
    "        out_text = [parse_answer(o) for o in res_sentences]\n",
    "        # print(out_text)\n",
    "        out_text_list += out_text\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    dataset[\"out_text\"] = out_text_list\n",
    "    # dataset[\"new_target\"] = dataset[\"target\"].apply(change_target)\n",
    "    # dataset[\"new_out\"] = dataset[\"out_text\"].apply(change_target)\n",
    "    \n",
    "\n",
    "    acc = accuracy_score(dataset[\"answer\"], dataset[\"out_text\"])\n",
    "    f1_macro = f1_score(dataset[\"answer\"], dataset[\"out_text\"], average = \"macro\")\n",
    "    f1_micro = f1_score(dataset[\"answer\"], dataset[\"out_text\"], average = \"micro\")\n",
    "    f1_weighted = f1_score(dataset[\"answer\"], dataset[\"out_text\"], average = \"weighted\")\n",
    "\n",
    "    print(f\"Acc: {acc}. F1 macro: {f1_macro}. F1 micro: {f1_micro}. F1 weighted (BloombergGPT): {f1_weighted}. \")\n",
    "    print(f\"Acc: {acc}. \")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "dataset = test(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367857bb-d69d-476c-b992-19efd03a62d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
