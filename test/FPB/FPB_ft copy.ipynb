{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from transformers import AutoModel, AutoTokenizer, LlamaForCausalLM, LlamaTokenizerFast\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a6083deb8c4c839b45c7bb5b707880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Models\n",
    "base_model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# peft_model = \"oliverwang15/FinGPT_ChatGLM2_Sentiment_Instruction_LoRA_FT\"\n",
    "# peft_model = \"../../finetuned_model\"\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = LlamaForCausalLM.from_pretrained(base_model, trust_remote_code=True, device_map = \"auto\")\n",
    "from peft import PeftModel\n",
    "peft_path = \"../../finetuned_model/\"\n",
    "model = PeftModel.from_pretrained(model, peft_path)\n",
    "model = torch.compile(model)\n",
    "# model = PeftModel.from_pretrained(model, peft_model)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {\n",
    "        0:\"negative\",\n",
    "        1:'neutral',\n",
    "        2:'positive',\n",
    "    }\n",
    "\n",
    "def format_example(example: dict) -> dict:\n",
    "    context = f\"Instruction: {example['instruction']}\\n\"\n",
    "    if example.get(\"input\"):\n",
    "        context += f\"Input: {example['input']}\\n\"\n",
    "    context += \"Answer: \"\n",
    "    target = example[\"output\"]\n",
    "    return {\"context\": context, \"target\": target}\n",
    "\n",
    "def change_target(x):\n",
    "    if 'positive' in x or 'Positive' in x:\n",
    "        return 'positive'\n",
    "    elif 'negative' in x or 'Negative' in x:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "def test_fpb(model, tokenizer, batch_size = 8, prompt_fun = None ):\n",
    "    instructions = load_dataset(\"financial_phrasebank\", \"sentences_50agree\")\n",
    "    instructions = instructions[\"train\"]\n",
    "    instructions = instructions.train_test_split(seed = 42)['test']\n",
    "    instructions = instructions.to_pandas()\n",
    "    instructions.columns = [\"input\", \"output\"]\n",
    "    instructions[\"output\"] = instructions[\"output\"].apply(lambda x:dic[x])\n",
    "\n",
    "    if prompt_fun is None:\n",
    "        instructions[\"instruction\"] = \"What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}.\"\n",
    "    else:\n",
    "        instructions[\"instruction\"] = instructions.apply(prompt_fun, axis = 1)\n",
    "    \n",
    "    instructions[[\"context\",\"target\"]] = instructions.apply(format_example, axis = 1, result_type=\"expand\")\n",
    "\n",
    "    # print example\n",
    "    print(f\"\\n\\nPrompt example:\\n{instructions['context'][0]}\\n\\n\")\n",
    "\n",
    "\n",
    "    context = instructions['context'].tolist()\n",
    "    \n",
    "    total_steps = instructions.shape[0]//batch_size + 1\n",
    "    print(f\"Total len: {len(context)}. Batchsize: {batch_size}. Total steps: {total_steps}\")\n",
    "\n",
    "\n",
    "    out_text_list = []\n",
    "    for i in tqdm(range(total_steps)):\n",
    "        tmp_context = context[i* batch_size:(i+1)* batch_size]\n",
    "        tokens = tokenizer(tmp_context, return_tensors='pt', padding=True, max_length=512)\n",
    "        for k in tokens.keys():\n",
    "            tokens[k] = tokens[k].cuda()\n",
    "        res = model.generate(**tokens, max_length=512)\n",
    "        res_sentences = [tokenizer.decode(i) for i in res]\n",
    "        out_text = [o.split(\"Answer: \")[1] for o in res_sentences]\n",
    "        out_text_list += out_text\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    instructions[\"out_text\"] = out_text_list\n",
    "    instructions[\"new_target\"] = instructions[\"target\"].apply(change_target)\n",
    "    instructions[\"new_out\"] = instructions[\"out_text\"].apply(change_target)\n",
    "\n",
    "    acc = accuracy_score(instructions[\"new_target\"], instructions[\"new_out\"])\n",
    "    f1_macro = f1_score(instructions[\"new_target\"], instructions[\"new_out\"], average = \"macro\")\n",
    "    f1_micro = f1_score(instructions[\"new_target\"], instructions[\"new_out\"], average = \"micro\")\n",
    "    f1_weighted = f1_score(instructions[\"new_target\"], instructions[\"new_out\"], average = \"weighted\")\n",
    "\n",
    "    print(f\"Acc: {acc}. F1 macro: {f1_macro}. F1 micro: {f1_micro}. F1 weighted (BloombergGPT): {f1_weighted}. \")\n",
    "\n",
    "    return instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset financial_phrasebank (/xfs/home/tensor_zy/.cache/huggingface/datasets/financial_phrasebank/sentences_50agree/1.0.0/550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e3e129877b46b581185732ea7c3f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /xfs/home/tensor_zy/.cache/huggingface/datasets/financial_phrasebank/sentences_50agree/1.0.0/550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141/cache-5054f1bbfc7f1863.arrow and /xfs/home/tensor_zy/.cache/huggingface/datasets/financial_phrasebank/sentences_50agree/1.0.0/550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141/cache-c9d8592c85fcadbe.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt example:\n",
      "Instruction: What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}.\n",
      "Input: L&T has also made a commitment to redeem the remaining shares by the end of 2011 .\n",
      "Answer: \n",
      "\n",
      "\n",
      "Total len: 1212. Batchsize: 8. Total steps: 152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 152/152 [01:56<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.8506600660066007. F1 macro: 0.8396686677078228. F1 micro: 0.8506600660066006. F1 weighted (BloombergGPT): 0.8500004920027702. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "instructions = test_fpb(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
