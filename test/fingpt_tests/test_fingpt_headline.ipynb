{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzXEp7L7_mxK"
      },
      "source": [
        "# FinGPT Test: Financial Headline Analysis\n",
        "\n",
        "This notebook demonstrates how to test FinGPT on the Financial Headline Analysis dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuQCI9X0_mxL"
      },
      "source": [
        "## 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlxuSeEl_mxL"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.32.0 peft==0.5.0 datasets accelerate bitsandbytes sentencepiece tqdm scikit-learn pandas matplotlib seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmCujtNe_mxM"
      },
      "source": [
        "## 2. Clone the FinGPT Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBePNiVP_mxM"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/AI4Finance-Foundation/FinGPT.git\n",
        "%cd FinGPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNBwYi0n_mxM"
      },
      "source": [
        "## 3. Download the Financial Headline Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvcoJS50_mxN"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "from pathlib import Path\n",
        "\n",
        "data_dir = Path('./fingpt/FinGPT_Benchmark/data')\n",
        "data_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Downloading Financial Headline dataset...\")\n",
        "try:\n",
        "    dataset = datasets.load_dataset(\"FinGPT/fingpt-headline\")\n",
        "\n",
        "    # Save the dataset to disk\n",
        "    save_path = str(data_dir / \"fingpt-headline-instruct\")\n",
        "    print(f\"Saving dataset to {save_path}\")\n",
        "    dataset.save_to_disk(save_path)\n",
        "    print(\"Dataset download complete!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "    print(\"You may need to manually download the Headline dataset and place it in the fingpt/FinGPT_Benchmark/data directory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjkYtGkQ_mxN"
      },
      "source": [
        "## 4. Testing Module for Headline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zr0Klkp8_mxN"
      },
      "outputs": [],
      "source": [
        "%%writefile fingpt/FinGPT_Benchmark/benchmarks/headline.py\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from datasets import load_dataset, load_from_disk\n",
        "from tqdm import tqdm\n",
        "import datasets\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "\n",
        "import sys\n",
        "sys.path.append('../')\n",
        "\n",
        "def binary2multi(dataset):\n",
        "    pred, label = [], []\n",
        "    tmp_pred, tmp_label = [], []\n",
        "    for i, row in dataset.iterrows():\n",
        "        tmp_pred.append(row['pred'])\n",
        "        tmp_label.append(row['label'])\n",
        "        if (i + 1) % 9 == 0:\n",
        "            pred.append(tmp_pred)\n",
        "            label.append(tmp_label)\n",
        "            tmp_pred, tmp_label = [], []\n",
        "    return pred, label\n",
        "\n",
        "\n",
        "def map_output(feature):\n",
        "    pred = 1 if 'yes' in feature['out_text'].lower() else 0\n",
        "    label = 1 if 'yes' in feature['output'].lower() else 0\n",
        "    return {'label': label, 'pred': pred}\n",
        "\n",
        "\n",
        "def test_mapping(args, example):\n",
        "    prompt = f\"Instruction: {example['instruction']}\\nInput: {example['input']}\\nAnswer: \"\n",
        "    return {\"prompt\": prompt}\n",
        "\n",
        "\n",
        "def test_headline(args, model, tokenizer):\n",
        "    print(\"Loading Financial Headline dataset...\")\n",
        "    # dataset = load_from_disk('../data/fingpt-headline')['test']\n",
        "    dataset = load_from_disk(Path(__file__).parent.parent / 'data/fingpt-headline-instruct')['test']\n",
        "    dataset = dataset.map(partial(test_mapping, args), load_from_cache_file=False)\n",
        "\n",
        "    def collate_fn(batch):\n",
        "        inputs = tokenizer(\n",
        "            [f[\"prompt\"] for f in batch], return_tensors='pt',\n",
        "            padding=True, max_length=args.max_length,\n",
        "            return_token_type_ids=False\n",
        "        )\n",
        "        return inputs\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=args.batch_size, collate_fn=collate_fn, shuffle=False)\n",
        "\n",
        "    print(f\"Running inference on {len(dataset)} examples with batch size {args.batch_size}...\")\n",
        "    out_text_list = []\n",
        "    log_interval = max(1, len(dataloader) // 5)\n",
        "\n",
        "    for idx, inputs in enumerate(tqdm(dataloader)):\n",
        "        inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
        "        res = model.generate(**inputs, max_length=args.max_length, eos_token_id=tokenizer.eos_token_id)\n",
        "        res_sentences = [tokenizer.decode(i, skip_special_tokens=True) for i in res]\n",
        "        if (idx + 1) % log_interval == 0:\n",
        "            tqdm.write(f'Example {idx}: {res_sentences[0]}')\n",
        "        out_text = [o.split(\"Answer: \")[1] if \"Answer: \" in o else o for o in res_sentences]\n",
        "        out_text_list += out_text\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"Processing results...\")\n",
        "    dataset = dataset.add_column(\"out_text\", out_text_list)\n",
        "    dataset = dataset.map(map_output, load_from_cache_file=False)\n",
        "    dataset = dataset.to_pandas()\n",
        "\n",
        "    dataset.to_csv('headline_results.csv', index=False)\n",
        "    print(\"Results saved to headline_results.csv\")\n",
        "\n",
        "    acc = accuracy_score(dataset[\"label\"], dataset[\"pred\"])\n",
        "    f1 = f1_score(dataset[\"label\"], dataset[\"pred\"], average=\"binary\")\n",
        "\n",
        "    pred, label = binary2multi(dataset)\n",
        "\n",
        "    print(f\"\\n|| Accuracy: {acc:.4f} || F1 (binary): {f1:.4f} ||\\n\")\n",
        "\n",
        "    category_names = [\n",
        "        'price or not', 'price up', 'price stable',\n",
        "        'price down', 'price past', 'price future',\n",
        "        'event past', 'event future', 'asset comp'\n",
        "    ]\n",
        "    print(classification_report(label, pred, digits=4, target_names=category_names))\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5DzncH3_mxN"
      },
      "source": [
        "## 5. Update Benchmarking Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49pCYdPa_mxO"
      },
      "outputs": [],
      "source": [
        "%%writefile fingpt/FinGPT_Benchmark/benchmarks/benchmarks.py\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel, get_peft_model, LoraConfig, TaskType\n",
        "import torch\n",
        "import argparse\n",
        "\n",
        "from headline import test_headline\n",
        "\n",
        "import sys\n",
        "sys.path.append('../')\n",
        "from utils import *\n",
        "\n",
        "def main(args):\n",
        "    if args.from_remote:\n",
        "        model_name = parse_model_name(args.base_model, args.from_remote)\n",
        "    else:\n",
        "        model_name = '../' + parse_model_name(args.base_model)\n",
        "\n",
        "    print(f\"Loading base model: {model_name}\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name, trust_remote_code=True,\n",
        "        # load_in_8bit=True\n",
        "        device_map=\"auto\",\n",
        "        # fp16=True\n",
        "    )\n",
        "    model.model_parallel = True\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "    # tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    tokenizer.padding_side = \"left\"\n",
        "    if args.base_model == 'qwen':\n",
        "        tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "        tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids('<|extra_0|>')\n",
        "    if not tokenizer.pad_token or tokenizer.pad_token_id == tokenizer.eos_token_id:\n",
        "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    print(f'pad: {tokenizer.pad_token_id}, eos: {tokenizer.eos_token_id}')\n",
        "\n",
        "    print(f\"Loading FinGPT adapter: {args.peft_model}\")\n",
        "    model = PeftModel.from_pretrained(model, args.peft_model)\n",
        "    model = model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in args.dataset.split(','):\n",
        "            if data == 'headline':\n",
        "                test_headline(args, model, tokenizer)\n",
        "            else:\n",
        "                raise ValueError(f'Undefined dataset: {data}')\n",
        "\n",
        "    print('Evaluation Ends.')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--dataset\", required=True, type=str)\n",
        "    parser.add_argument(\"--base_model\", required=True, type=str, choices=['chatglm2', 'llama2', 'llama2-13b', 'llama2-13b-nr', 'baichuan', 'falcon', 'internlm', 'qwen', 'mpt', 'bloom'])\n",
        "    parser.add_argument(\"--peft_model\", required=True, type=str)\n",
        "    parser.add_argument(\"--max_length\", default=512, type=int)\n",
        "    parser.add_argument(\"--batch_size\", default=4, type=int, help=\"The train batch size per device\")\n",
        "    parser.add_argument(\"--instruct_template\", default='default')\n",
        "    parser.add_argument(\"--from_remote\", default=False, type=bool)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(args.base_model)\n",
        "    print(args.peft_model)\n",
        "\n",
        "    main(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne8oJBxV_mxO"
      },
      "source": [
        "## 6. Create Utils Module\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9-3XebT_mxO"
      },
      "outputs": [],
      "source": [
        "%%writefile fingpt/FinGPT_Benchmark/utils.py\n",
        "def parse_model_name(base_model, from_remote=False):\n",
        "    model_map = {\n",
        "        'chatglm2': 'THUDM/chatglm2-6b',\n",
        "        'llama2': 'meta-llama/Llama-2-7b-hf',\n",
        "        'llama2-13b': 'meta-llama/Llama-2-13b-hf',\n",
        "        'llama2-13b-nr': 'NousResearch/Llama-2-13b-hf',\n",
        "        'baichuan': 'baichuan-inc/Baichuan-7B',\n",
        "        'falcon': 'tiiuae/falcon-7b',\n",
        "        'internlm': 'internlm/internlm-7b',\n",
        "        'qwen': 'Qwen/Qwen-7B',\n",
        "        'mpt': 'mosaicml/mpt-7b',\n",
        "        'bloom': 'bigscience/bloom-7b1',\n",
        "    }\n",
        "    if base_model not in model_map:\n",
        "        raise ValueError(f\"Unknown base model: {base_model}\")\n",
        "    return model_map[base_model]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKGku6zYBHS1"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"token\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDRhXEXZ_mxO"
      },
      "source": [
        "## 7. Run the Financial Headline Benchmark Test\n",
        "\n",
        "Now that we have set up all the necessary files, let's run the benchmark test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3l-4zlmr_mxO",
        "outputId": "33d370e8-1155-41f6-f622-c861b4736cf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example 5134: Instruction: Does the news headline talk about a general event (apart from prices) in the future? Please choose an answer from {Yes/No}.\n",
            "Input: august gold up $7.60 at $878.80 an ounce on nymex\n",
            "Answer:  No\n",
            "100% 5137/5137 [22:43<00:00,  3.77it/s]\n",
            "Processing results...\n",
            "Map: 100% 20547/20547 [00:01<00:00, 16280.23 examples/s]\n",
            "Results saved to headline_results.csv\n",
            "\n",
            "|| Accuracy: 0.9701 || F1 (binary): 0.9344 ||\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "price or not     0.8765    0.6893    0.7717       103\n",
            "    price up     0.9411    0.9360    0.9385       938\n",
            "price stable     0.9036    0.7282    0.8065       103\n",
            "  price down     0.9386    0.9231    0.9308       845\n",
            "  price past     0.9712    0.9628    0.9670      1857\n",
            "price future     0.9219    0.6705    0.7763        88\n",
            "  event past     0.7821    0.8696    0.8235       322\n",
            "event future     0.0000    0.0000    0.0000        16\n",
            "  asset comp     0.9866    0.9866    0.9866       448\n",
            "\n",
            "   micro avg     0.9425    0.9265    0.9344      4720\n",
            "   macro avg     0.8135    0.7518    0.7779      4720\n",
            "weighted avg     0.9402    0.9265    0.9323      4720\n",
            " samples avg     0.9249    0.9197    0.9166      4720\n",
            "\n",
            "Evaluation Ends.\n"
          ]
        }
      ],
      "source": [
        "%cd /content/FinGPT/fingpt/FinGPT_Benchmark/benchmarks\n",
        "\n",
        "base_model = 'llama2'\n",
        "# The FinGPT adapter model\n",
        "peft_model = 'FinGPT/fingpt-mt_llama2-7b_lora'\n",
        "batch_size = 4\n",
        "max_length = 512\n",
        "\n",
        "!python benchmarks.py --dataset headline --base_model {base_model} --peft_model {peft_model} --batch_size {batch_size} --max_length {max_length} --from_remote True"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
